### **Â¿QuÃ© es Machine Learning?**  
**Machine Learning (ML)** o *Aprendizaje AutomÃ¡tico* es una rama de la **Inteligencia Artificial (IA)** que se centra en desarrollar algoritmos y modelos capaces de aprender patrones a partir de datos, **sin ser programados explÃ­citamente**. En lugar de seguir reglas predefinidas, los sistemas de ML **mejoran su desempeÃ±o con la experiencia** (datos).  

Su objetivo principal es **generalizar** a partir de ejemplos para realizar predicciones o tomar decisiones en situaciones nuevas.

---

### **Particularidades y CaracterÃ­sticas Principales**  

1. **Aprendizaje basado en datos**  
   - ML requiere **datos histÃ³ricos o de entrenamiento** para identificar patrones y relaciones.  
   - A diferencia de la programaciÃ³n tradicional (donde las reglas son fijas), en ML **el modelo "aprende" de los datos**.  

2. **Capacidad predictiva y adaptativa**  
   - Los modelos de ML pueden **predecir resultados futuros** (ej.: ventas, fallos en equipos) o **clasificar informaciÃ³n** (ej.: spam/no spam).  
   - Algunos sistemas se adaptan a cambios en los datos (ej.: recomendaciones en tiempo real en Netflix o Amazon).  

3. **Tipos principales de aprendizaje**  
   - **Supervisado**: Usa datos etiquetados (ej.: predecir precios de casas basado en ejemplos pasados).  
   - **No supervisado**: Encuentra patrones en datos sin etiquetas (ej.: agrupaciÃ³n de clientes por comportamiento).  
   - **Por refuerzo**: Aprende mediante prueba/error y recompensas (ej.: robots que aprenden a caminar).  

4. **AutomatizaciÃ³n y escalabilidad**  
   - ML permite automatizar tareas complejas (ej.: diagnÃ³stico mÃ©dico, detecciÃ³n de fraudes).  
   - Escala bien con grandes volÃºmenes de datos (*Big Data*).  

5. **Ã‰nfasis en la evaluaciÃ³n**  
   - Los modelos se validan con mÃ©tricas como **precisiÃ³n, recall o error cuadrÃ¡tico medio** para garantizar su fiabilidad.  

6. **Dependencia de la calidad de los datos**  
   - El rendimiento del ML estÃ¡ ligado a la **calidad, cantidad y representatividad** de los datos.  
   - Problemas como *sesgos* o *datos incompletos* afectan los resultados.  

7. **Uso de algoritmos diversos**  
   - Desde mÃ©todos clÃ¡sicos (*regresiÃ³n lineal, Ã¡rboles de decisiÃ³n*) hasta tÃ©cnicas avanzadas (*redes neuronales, deep learning*).  

---

### **Diferencia clave con Data Science**  
Mientras **Data Science** abarca todo el ciclo de anÃ¡lisis de datos (limpieza, visualizaciÃ³n, estadÃ­stica, etc.), **ML es una herramienta dentro de DS** enfocada especÃ­ficamente en **automatizar el aprendizaje** para predicciÃ³n o toma de decisiones.  

**Ejemplo prÃ¡ctico**:  
- Un modelo de ML podrÃ­a predecir el *churn* (abandono) de clientes en una empresa, mientras que un data scientist tambiÃ©n analizarÃ­a *por quÃ©* ocurre y cÃ³mo comunicarlo.  

En resumen, **Machine Learning es la tecnologÃ­a que permite a las mÃ¡quinas "aprender" de los datos para resolver problemas complejos de manera autÃ³noma o semiautÃ³noma**. 

---


### _Si quisiÃ©ramos resolver un problema donde tenemos informaciÃ³n georeferenciada de clientes Â¿cÃ³mo podrÃ­amos utilizar el ML para incrementar las ventas de un producto?_

Para incrementar las ventas de un producto utilizando **Machine Learning (ML)** con datos georreferenciados de clientes, puedes aplicar diversas estrategias basadas en anÃ¡lisis espacial y modelos predictivos. AquÃ­ te detallo un enfoque estructurado:

---

### **1. AnÃ¡lisis Exploratorio de Datos (EDA) Geoespacial**  
- **VisualizaciÃ³n de datos**:  
  - Usar mapas de calor (*heatmaps*) para identificar zonas con alta concentraciÃ³n de clientes o ventas.  
  - Segmentar por zonas geogrÃ¡ficas (barrios, ciudades, cÃ³digos postales).  
- **DetecciÃ³n de patrones**:  
  - Correlacionar ubicaciÃ³n con variables como ingresos, edad, clima o proximidad a puntos de interÃ©s (ej.: centros comerciales).  

**Herramientas**: Python (`geopandas`, `folium`), Power BI (integrado con Mapas).  

---

### **2. SegmentaciÃ³n de Clientes por UbicaciÃ³n**  
- **Clustering no supervisado** (ej.: *K-means* o *DBSCAN*) para agrupar clientes con caracterÃ­sticas similares:  
  - Crear clusters basados en:  
    - UbicaciÃ³n (coordenadas).  
    - Comportamiento de compra + datos demogrÃ¡ficos locales.  
  - Ejemplo: Identificar "zonas de alto potencial" con clientes similares a los que ya compran el producto.  

---

### **3. Modelos Predictivos para Ventas**  
- **Aprendizaje supervisado** (ej.: *Random Forest, XGBoost*) para predecir:  
  - **PropensiÃ³n de compra**: QuÃ© clientes (o zonas) tienen mayor probabilidad de comprar el producto.  
  - **Demanda geogrÃ¡fica**: DÃ³nde habrÃ¡ mayor demanda en funciÃ³n de variables temporales (ej.: festividades locales).  
- **Variables de entrada**:  
  - Datos geogrÃ¡ficos (latitud, longitud, distancia a tiendas).  
  - Datos socioeconÃ³micos de la zona (nivel de ingresos, densidad poblacional).  
  - Historial de ventas en la regiÃ³n.  

---

### **4. Recomendaciones Geo-Personalizadas**  
- **Sistemas de recomendaciÃ³n** con filtrado colaborativo o basado en contenido:  
  - Sugerir productos populares en la zona (ej.: "En tu barrio, otros clientes compran X").  
  - Adaptar promociones segÃºn el perfil geogrÃ¡fico (ej.: descuentos en zonas con menor penetraciÃ³n).  

---

### **5. OptimizaciÃ³n LogÃ­stica y UbicaciÃ³n de Puntos de Venta**  
- **Modelos de ubicaciÃ³n Ã³ptima**:  
  - Usar *algoritmos de optimizaciÃ³n* (ej.: *p-median*) para decidir dÃ³nde abrir nuevas tiendas o colocar anuncios.  
  - Ejemplo: "Las zonas con radio de 5 km sin cobertura tienen un 20% de clientes potenciales sin atender".  

---

### **6. CampaÃ±as de Marketing Dirigido**  
- **Geo-targeting publicitario**:  
  - Entrenar modelos para identificar zonas donde campaÃ±as especÃ­ficas (ej.: SMS, redes sociales) tendrÃ¡n mayor ROI.  
  - Ejemplo: Anuncios en Facebook Ads para un radio de 10 km alrededor de tiendas con stock alto.  

---

### **7. Ejemplo PrÃ¡ctico**  
**Problema**: Una cadena de cafeterÃ­as quiere aumentar ventas en Ciudad de MÃ©xico.  
**SoluciÃ³n con ML**:  
1. Agrupa clientes por colonia usando *DBSCAN*.  
2. Entrena un modelo para predecir ventas segÃºn:  
   - Proximidad a estaciones de metro.  
   - Nivel socioeconÃ³mico (datos del INEGI).  
3. Descubre que las colonias *Roma* y *Condesa* tienen alta demanda los fines de semana.  
4. Lanza promociones "2x1" los sÃ¡bados en esas zonas via WhatsApp.  

---

### **Beneficios**  
- **ReducciÃ³n de costos**: Enfoque en zonas de alto impacto.  
- **PersonalizaciÃ³n**: Ofertas relevantes por ubicaciÃ³n.  
- **Escalabilidad**: Aplicable a mÃºltiples regiones o productos.  

---
# ML

*  ## [Aprendizaje Supervizado](clase_7/aprendizaje-supervisado.md)
* ## [Aprendizaje no Supervisado](clase_7/aprendizaje-no-supervisado.md)

* ## [Paso a paso para un ML funcional](clase_7/paso-a-pas.md)


---
### Apartado especial para lo que es Feature Selection

### ğŸ§¹ 1. MÃ©todos de Filtro (*Filter Methods*)

Piensa en estos como un primer filtro rÃ¡pido.

* **CÃ³mo funcionan:** EvalÃºan cada caracterÃ­stica de forma individual usando pruebas estadÃ­sticas que miden su relaciÃ³n con la variable objetivo (la etiqueta).
* **Velocidad:** Muy rÃ¡pidos, ya que no requieren entrenar modelos.
* **Ejemplos:**

  * **Prueba de Chi-cuadrado** (para variables categÃ³ricas)
  * **Prueba ANOVA**
  * **InformaciÃ³n mutua**
  * **Coeficiente de correlaciÃ³n**

ğŸ“Œ **Ventajas:** RÃ¡pidos y no dependen del modelo
ğŸ“Œ **Desventajas:** No consideran las interacciones entre caracterÃ­sticas

---

### ğŸ§ª 2. MÃ©todos Envolventes (*Wrapper Methods*)

Estos son como probar diferentes combinaciones de ropa para ver cuÃ¡l te queda mejor.

* **CÃ³mo funcionan:** Prueban mÃºltiples subconjuntos de caracterÃ­sticas entrenando modelos, y eligen el subconjunto que da el mejor rendimiento.
* **Velocidad:** Lentos, porque entrenan muchos modelos.
* **Ejemplos:**

  * **EliminaciÃ³n recursiva de caracterÃ­sticas (RFE)**
  * **SelecciÃ³n hacia adelante**
  * **EliminaciÃ³n hacia atrÃ¡s**

ğŸ“Œ **Ventajas:** Tienen en cuenta interacciones entre caracterÃ­sticas
ğŸ“Œ **Desventajas:** Muy costosos en tiempo y recursos computacionales

---

### âš™ï¸ 3. MÃ©todos Embebidos (*Embedded Methods*)

Estos seleccionan caracterÃ­sticas **mientras entrenan** el modelo.

* **CÃ³mo funcionan:** La selecciÃ³n ocurre como parte del proceso de entrenamiento.
* **Ejemplos:**

  * **Lasso (regularizaciÃ³n L1)** â€“ reduce coeficientes a cero
  * **Ãrboles de decisiÃ³n / Bosques aleatorios** â€“ calculan importancia de cada variable
  * **Elastic Net** â€“ combina L1 y L2

ğŸ“Œ **Ventajas:** MÃ¡s eficientes y consideran interacciones
ğŸ“Œ **Desventajas:** Dependientes del modelo utilizado

---

### Tabla Resumen:

| Tipo de mÃ©todo | Â¿Usa modelo? | Â¿Considera interacciones? | Velocidad | Ejemplos                      |
| -------------- | ------------ | ------------------------- | --------- | ----------------------------- |
| Filtro         | âŒ No         | âŒ No                      | ğŸš€ RÃ¡pido | Chi-cuadrado, correlaciÃ³n     |
| Envolvente     | âœ… SÃ­         | âœ… SÃ­                      | ğŸ¢ Lento  | RFE, selecciÃ³n hacia adelante |
| Embebido       | âœ… SÃ­         | âœ… SÃ­                      | âš¡ Medio   | Lasso, Ã¡rboles de decisiÃ³n    |


## Recomendaciones para aprender:
Â¡Claro! Vamos a ver cada uno de estos mÃ©todos de selecciÃ³n de caracterÃ­sticas supervisadas con **ejemplos, ventajas y desventajas**, relacionÃ¡ndolos con los tipos que ya vimos: filtro, envolvente o embebido.

---

### ğŸ“‰ 1. **Variance Threshold**

ğŸ”¹ **Tipo:** Filtro
ğŸ”¹ **QuÃ© hace:** Elimina caracterÃ­sticas con **baja varianza**, es decir, que no cambian mucho entre ejemplos (por ejemplo, una columna donde casi todos los valores son iguales).

#### âœ… Ventajas:

* Muy simple y rÃ¡pido de aplicar
* No necesita etiquetas (tambiÃ©n se puede usar en problemas no supervisados)

#### âŒ Desventajas:

* No considera la relaciÃ³n con la variable objetivo
* Puede eliminar caracterÃ­sticas Ãºtiles si tienen poca varianza pero alta relevancia

#### ğŸ§ª Ejemplo en Python:

```python
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)
X_reduced = selector.fit_transform(X)
```

---

### â­ 2. **SelectKBest**

ğŸ”¹ **Tipo:** Filtro
ğŸ”¹ **QuÃ© hace:** Selecciona las **k mejores caracterÃ­sticas** segÃºn una mÃ©trica estadÃ­stica que mide la relaciÃ³n con la variable objetivo (como ANOVA, chi-cuadrado, etc.).

#### âœ… Ventajas:

* FÃ¡cil de interpretar y aplicar
* RÃ¡pido y eficiente
* Permite usar distintas mÃ©tricas

#### âŒ Desventajas:

* No considera interacciones entre variables
* Necesita que tÃº elijas el valor de *k* (el nÃºmero de caracterÃ­sticas a conservar)

#### ğŸ§ª Ejemplo en Python:

```python
from sklearn.feature_selection import SelectKBest, f_classif
selector = SelectKBest(score_func=f_classif, k=10)
X_kbest = selector.fit_transform(X, y)
```

---

### ğŸ”„ 3. **RFE (Recursive Feature Elimination)**

ğŸ”¹ **Tipo:** Envolvente
ğŸ”¹ **QuÃ© hace:** Usa un modelo (como una regresiÃ³n o un SVM) para eliminar **recursivamente** las caracterÃ­sticas menos importantes hasta quedarse con las mÃ¡s relevantes.

#### âœ… Ventajas:

* Considera interacciones entre variables
* Da muy buenos resultados si se elige bien el modelo base

#### âŒ Desventajas:

* Lento, especialmente con muchos datos o caracterÃ­sticas
* Depende fuertemente del modelo usado

#### ğŸ§ª Ejemplo en Python:

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
rfe = RFE(estimator=model, n_features_to_select=5)
X_rfe = rfe.fit_transform(X, y)
```

---

### ğŸŒ³ 4. **Boruta**

ğŸ”¹ **Tipo:** Envolvente (basado en Ã¡rboles, como Random Forest)
ğŸ”¹ **QuÃ© hace:** Crea versiones "aleatorias" de las caracterÃ­sticas y las compara con las reales. Solo conserva las que son mejores que las versiones aleatorias (shadow features).

#### âœ… Ventajas:

* Muy robusto y preciso
* Tiene en cuenta interacciones y relaciones no lineales
* Funciona bien con datos complejos

#### âŒ Desventajas:

* Bastante lento (usa muchos modelos de Random Forest)
* No estÃ¡ en `scikit-learn` directamente (hay que usar una librerÃ­a aparte como `boruta_py`)

#### ğŸ§ª Ejemplo con `boruta_py`:

```python
from boruta import BorutaPy
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)
boruta_selector = BorutaPy(estimator=model, n_estimators='auto', random_state=42)
boruta_selector.fit(X.values, y.values)
```

---

### ğŸ” Resumen Comparativo

| MÃ©todo             | Tipo       | InteracciÃ³n | Velocidad    | Modelo requerido | Ventaja principal                    |
| ------------------ | ---------- | ----------- | ------------ | ---------------- | ------------------------------------ |
| Variance Threshold | Filtro     | âŒ No        | ğŸš€ RÃ¡pido    | âŒ No             | Muy simple y rÃ¡pido                  |
| SelectKBest        | Filtro     | âŒ No        | ğŸš€ RÃ¡pido    | âŒ No             | MÃ©tricas estadÃ­sticas supervisadas   |
| RFE                | Envolvente | âœ… SÃ­        | ğŸ¢ Lento     | âœ… SÃ­             | Preciso si el modelo es adecuado     |
| Boruta             | Envolvente | âœ… SÃ­        | ğŸŒ Muy lento | âœ… SÃ­             | Robusto frente a ruido y redundancia |

