[Material Diapositivas](https://docs.google.com/presentation/d/14g5eAuGgTasx_D4dJ7w7txt2qOZEY7EqvrTVTMddh74/edit#slide=id.p44)

# üìö Clase 8: Aprendizaje No Supervisado

---

## üîÑ **Repaso Clase 7: Aprendizaje Supervisado**

### **üìö ¬øQu√© vimos en la Clase 7?**

En la **Clase 7** exploramos el **Aprendizaje Supervisado**, donde tenemos datos con etiquetas conocidas (target variables) para entrenar nuestros modelos.

#### **üéØ Conceptos Clave de la Clase 7:**

1. **Clasificaci√≥n**: Predecir categor√≠as (ej: spam/no spam)
   - **√Årboles de Decisi√≥n**: Reglas if-else interpretables
   - **Random Forest**: M√∫ltiples √°rboles para mayor precisi√≥n
   - **M√©tricas**: Accuracy, Precision, Recall, F1-Score

2. **Regresi√≥n**: Predecir valores num√©ricos (ej: precio de casa)
   - **Regresi√≥n Lineal**: Relaci√≥n lineal entre variables
   - **Regresi√≥n M√∫ltiple**: M√∫ltiples variables predictoras
   - **M√©tricas**: MSE, RMSE, R¬≤

#### **üîó Conexi√≥n con Clase 8:**

```python
# REPASO: En Clase 7 ten√≠amos datos ETIQUETADOS
import pandas as pd
import numpy as np

# Ejemplo de Clase 7: Clasificaci√≥n de clientes
datos_clase7 = {
    'edad': [25, 35, 45, 55, 65],
    'ingresos': [30000, 50000, 70000, 60000, 40000],
    'gasto_mensual': [800, 1500, 2500, 1800, 900],
    'cliente_valioso': ['No', 'S√≠', 'S√≠', 'S√≠', 'No']  # ‚Üê ETIQUETA CONOCIDA
}

df_clase7 = pd.DataFrame(datos_clase7)
print("=== CLASE 7: DATOS CON ETIQUETAS ===")
print("Sabemos qu√© clientes son valiosos...")
print(df_clase7)
print("\nObjetivo: Predecir si un NUEVO cliente ser√° valioso")

# En Clase 8: ¬øQu√© pasa si NO tenemos etiquetas?
print("\n=== CLASE 8: DATOS SIN ETIQUETAS ===")
datos_clase8 = df_clase7.drop('cliente_valioso', axis=1)
print("No sabemos qu√© tipos de clientes tenemos...")
print("Objetivo: DESCUBRIR grupos naturales en los datos")
print(datos_clase8)
```

#### **üîÑ Transici√≥n: De Supervisado a No Supervisado**

| Aspecto | Clase 7 (Supervisado) | Clase 8 (No Supervisado) |
|---------|----------------------|-------------------------|
| **Datos** | Con etiquetas conocidas | Sin etiquetas |
| **Objetivo** | Predecir/Clasificar | Descubrir patrones |
| **Evaluaci√≥n** | M√©tricas claras (accuracy, etc.) | M√©tricas internas (silhouette) |
| **Interpretaci√≥n** | Modelo predictivo | Exploraci√≥n de datos |

---

## üìã **8.1 Introducci√≥n al Aprendizaje No Supervisado**

### **üéØ ¬øQu√© es el Aprendizaje No Supervisado?**

El **Aprendizaje No Supervisado** es una rama del Machine Learning que encuentra patrones ocultos en datos **sin etiquetas** (target variables). A diferencia del aprendizaje supervisado, no tenemos la "respuesta correcta" para entrenar el modelo.

#### **üîç Caracter√≠sticas Principales:**

1. **Sin etiquetas**: Los datos no tienen variables objetivo conocidas
2. **Descubrimiento de patrones**: El objetivo es encontrar estructuras ocultas
3. **Exploratorio**: Se usa para entender mejor los datos
4. **Flexible**: No hay restricciones sobre qu√© patrones buscar

#### **üìä Tipos de Aprendizaje No Supervisado:**

| Tipo | Descripci√≥n | Ejemplos |
|------|-------------|----------|
| **Clustering** | Agrupa datos similares | Segmentaci√≥n de clientes, an√°lisis de genes |
| **Reducci√≥n de Dimensionalidad** | Reduce variables manteniendo informaci√≥n | PCA, t-SNE, UMAP |
| **Reglas de Asociaci√≥n** | Encuentra relaciones entre elementos | Market basket analysis |
| **Detecci√≥n de Anomal√≠as** | Identifica patrones inusuales | Detecci√≥n de fraudes |

#### **üéØ Casos de Uso Reales:**

```python
# Ejemplo conceptual: ¬øPor qu√© usar aprendizaje no supervisado?
import pandas as pd
import numpy as np

# Simulamos datos de clientes sin etiquetas
np.random.seed(42)
n_clientes = 1000

datos_clientes = {
    'edad': np.random.normal(35, 10, n_clientes),
    'ingresos_anuales': np.random.lognormal(10, 0.5, n_clientes),
    'gasto_mensual': np.random.gamma(2, 500, n_clientes),
    'frecuencia_compras': np.random.poisson(8, n_clientes),
    'satisfaccion': np.random.choice([1,2,3,4,5], n_clientes, p=[0.1,0.2,0.3,0.3,0.1])
}

df_clientes = pd.DataFrame(datos_clientes)

print("=== EJEMPLO DE DATOS SIN ETIQUETAS ===")
print("No sabemos qu√© tipos de clientes tenemos...")
print("¬øC√≥mo los agrupamos para estrategias de marketing?")
print(f"\nDatos de muestra:")
print(df_clientes.head())
print(f"\nEstad√≠sticas descriptivas:")
print(df_clientes.describe())
```

#### **‚öñÔ∏è Ventajas vs Desventajas:**

**‚úÖ Ventajas:**
- No necesita datos etiquetados (m√°s barato)
- Descubre patrones inesperados
- √ötil para exploraci√≥n de datos
- Reduce dimensionalidad

**‚ùå Desventajas:**
- M√°s dif√≠cil de evaluar
- Resultados subjetivos
- Requiere interpretaci√≥n humana
- Menos predictivo que supervisado

---

## üß† **8.2 Algoritmos de Clustering**

### **üéØ ¬øQu√© es el Clustering?**

El **clustering** es una t√©cnica clave en el aprendizaje no supervisado que se utiliza para agrupar un conjunto de datos no etiquetados en grupos o cl√∫steres de datos similares. A trav√©s del clustering, los datos que comparten caracter√≠sticas similares se agrupan en el mismo cl√∫ster, mientras que los datos que son diferentes se separan en cl√∫steres distintos.

#### **üîç ¬øC√≥mo Funciona?**

El proceso de clustering implica el uso de algoritmos que identifican similitudes y diferencias en los datos para formar estos grupos. Estos algoritmos no requieren informaci√≥n previa sobre las categor√≠as o etiquetas de los datos, lo que les permite operar en conjuntos de datos no etiquetados.

#### **üéØ Objetivos del Clustering:**
- **Agrupar**: Encontrar grupos naturales en los datos
- **Descubrir**: Identificar patrones ocultos
- **Simplificar**: Reducir complejidad de grandes datasets
- **Segmentar**: Crear categor√≠as para estrategias de negocio

#### **üìä Prop√≥sito en el An√°lisis de Datos No Etiquetados**

El objetivo principal del clustering es descubrir la estructura subyacente de un conjunto de datos no etiquetados. Al identificar y agrupar observaciones similares, el clustering permite a los analistas entender mejor las relaciones en los datos y extraer insights valiosos.

**Ejemplos de Aplicaci√≥n:**
- **Marketing**: Segmentar clientes en grupos con comportamientos similares
- **Biolog√≠a**: Descubrir nuevas especies al agrupar organismos con caracter√≠sticas similares
- **Medicina**: Clasificar tipos de c√©lulas o tejidos
- **Finanzas**: Detectar patrones de fraude o riesgo crediticio

---

## üîß **8.2.1 Algoritmos Populares de Clustering**

El clustering es una t√©cnica de aprendizaje no supervisado que agrupa datos similares en cl√∫steres. Entre los algoritmos m√°s utilizados en clustering se encuentran **K-means**, **DBSCAN** y el **clustering jer√°rquico**, cada uno con caracter√≠sticas y aplicaciones particulares.

### **1. üîπ K-Means**

**K-means** es uno de los algoritmos de clustering m√°s populares debido a su simplicidad y eficacia. Su objetivo es dividir un conjunto de datos en K cl√∫steres predefinidos, donde cada dato pertenece al cl√∫ster con el centroide m√°s cercano.

#### **üéØ Proceso del Algoritmo K-Means:**

1. **Inicializaci√≥n**: Se seleccionan aleatoriamente K centroides (puntos de referencia) en el espacio de los datos
2. **Asignaci√≥n**: Cada punto de datos se asigna al cl√∫ster cuyo centroide est√© m√°s cercano, minimizando la distancia euclidiana
3. **Actualizaci√≥n**: Se recalculan los centroides de los cl√∫steres bas√°ndose en los datos asignados
4. **Iteraci√≥n**: Los pasos de asignaci√≥n y actualizaci√≥n se repiten hasta que los centroides ya no cambian significativamente

#### **‚úÖ Ventajas:**
- Simple y r√°pido
- Escalable a grandes datasets
- Funciona bien con clusters esf√©ricos

#### **‚ùå Desventajas:**
- Requiere especificar el n√∫mero K de clusters
- Sensible a outliers
- Asume clusters de forma esf√©rica
- Sensible a la inicializaci√≥n aleatoria

### **2. üîπ DBSCAN (Density-Based Spatial Clustering)**

**DBSCAN** es un algoritmo basado en la densidad que agrupa puntos que est√°n densamente conectados, separ√°ndolos de los puntos menos densos, considerados como ruido.

#### **üéØ Conceptos Clave de DBSCAN:**

- **Vecindad Œµ (epsilon)**: Un par√°metro que define un radio alrededor de un punto
- **MinPts**: N√∫mero m√≠nimo de puntos dentro de la vecindad para que un punto sea considerado un punto central
- **Cl√∫steres**: Se forman conectando puntos densamente conectados
- **Ruido**: Los puntos aislados se consideran ruido

#### **‚úÖ Ventajas:**
- No requiere especificar el n√∫mero de clusters
- Detecta clusters de forma arbitraria
- Maneja outliers autom√°ticamente
- Robusto contra ruido

#### **‚ùå Desventajas:**
- Sensible a los par√°metros Œµ y MinPts
- Dif√≠cil de ajustar para datos con densidades variables
- No funciona bien con clusters de densidad muy diferente

### **3. üîπ Clustering Jer√°rquico**

El **clustering jer√°rquico** es un enfoque que construye una jerarqu√≠a de cl√∫steres. Existen dos m√©todos principales:

#### **üå≥ Tipos de Clustering Jer√°rquico:**

1. **Aglomerativo (bottom-up)**: 
   - Comienza tratando cada punto de datos como un cl√∫ster individual
   - Fusiona los cl√∫steres m√°s cercanos hasta que todos los puntos formen un solo cl√∫ster

2. **Divisivo (top-down)**:
   - Comienza con un solo cl√∫ster que contiene todos los puntos de datos
   - Lo divide en cl√∫steres m√°s peque√±os

#### **üìä Dendrograma:**
El resultado del clustering jer√°rquico se visualiza com√∫nmente con un **dendrograma**, un √°rbol que muestra la estructura de fusi√≥n o divisi√≥n.

#### **‚úÖ Ventajas:**
- No necesita predefinir el n√∫mero de clusters
- Proporciona una estructura completa de clusters
- Interpretable visualmente

#### **‚ùå Desventajas:**
- Computacionalmente costoso para grandes datasets
- Sensible a outliers
- Dif√≠cil de escalar

---

## üéì **8.2.2 Ejemplos para Clase**

### **üìö Ejemplo 1: Clustering Simple con Datos de Estudiantes**

Vamos a empezar con un ejemplo simple y did√°ctico que los alumnos pueden entender f√°cilmente.

```python
# EJEMPLO 1: Clustering de Estudiantes por Rendimiento
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Crear datos de estudiantes (simulados)
np.random.seed(42)
n_estudiantes = 100

# Generar datos de diferentes tipos de estudiantes
# Tipo 1: Estudiantes aplicados (altas notas, muchas horas de estudio)
aplicados = {
    'horas_estudio': np.random.normal(8, 1, 30),
    'nota_promedio': np.random.normal(8.5, 0.5, 30),
    'asistencia': np.random.normal(95, 3, 30)
}

# Tipo 2: Estudiantes promedio (notas y estudio moderados)
promedio = {
    'horas_estudio': np.random.normal(5, 1, 40),
    'nota_promedio': np.random.normal(6.5, 0.8, 40),
    'asistencia': np.random.normal(80, 5, 40)
}

# Tipo 3: Estudiantes con dificultades (bajas notas, pocas horas)
dificultades = {
    'horas_estudio': np.random.normal(2, 0.5, 30),
    'nota_promedio': np.random.normal(4, 0.7, 30),
    'asistencia': np.random.normal(60, 10, 30)
}

# Combinar todos los datos
datos_estudiantes = {
    'horas_estudio': np.concatenate([aplicados['horas_estudio'], promedio['horas_estudio'], dificultades['horas_estudio']]),
    'nota_promedio': np.concatenate([aplicados['nota_promedio'], promedio['nota_promedio'], dificultades['nota_promedio']]),
    'asistencia': np.concatenate([aplicados['asistencia'], promedio['asistencia'], dificultades['asistencia']])
}

df_estudiantes = pd.DataFrame(datos_estudiantes)

print("=== EJEMPLO 1: DATOS DE ESTUDIANTES ===")
print("Objetivo: Descubrir tipos de estudiantes sin etiquetas previas")
print(f"\nDataset shape: {df_estudiantes.shape}")
print(f"\nPrimeras 5 filas:")
print(df_estudiantes.head())
print(f"\nEstad√≠sticas descriptivas:")
print(df_estudiantes.describe())

# Visualizaci√≥n inicial
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.scatter(df_estudiantes['horas_estudio'], df_estudiantes['nota_promedio'], alpha=0.6)
plt.xlabel('Horas de Estudio')
plt.ylabel('Nota Promedio')
plt.title('Datos Originales: Horas vs Notas')

plt.subplot(1, 3, 2)
plt.scatter(df_estudiantes['horas_estudio'], df_estudiantes['asistencia'], alpha=0.6)
plt.xlabel('Horas de Estudio')
plt.ylabel('Asistencia (%)')
plt.title('Datos Originales: Horas vs Asistencia')

plt.subplot(1, 3, 3)
plt.scatter(df_estudiantes['nota_promedio'], df_estudiantes['asistencia'], alpha=0.6)
plt.xlabel('Nota Promedio')
plt.ylabel('Asistencia (%)')
plt.title('Datos Originales: Notas vs Asistencia')

plt.tight_layout()
plt.show()

# PASO 1: Preparar datos para clustering
print("\n=== PASO 1: PREPARACI√ìN DE DATOS ===")
print("¬øPor qu√© normalizar? Las variables tienen escalas diferentes:")
print(f"Horas de estudio: {df_estudiantes['horas_estudio'].min():.1f} - {df_estudiantes['horas_estudio'].max():.1f}")
print(f"Notas: {df_estudiantes['nota_promedio'].min():.1f} - {df_estudiantes['nota_promedio'].max():.1f}")
print(f"Asistencia: {df_estudiantes['asistencia'].min():.1f} - {df_estudiantes['asistencia'].max():.1f}")

# Normalizar datos
scaler = StandardScaler()
X_estudiantes = scaler.fit_transform(df_estudiantes)
print(f"\nDatos normalizados shape: {X_estudiantes.shape}")

# PASO 2: M√©todo del Codo para encontrar K √≥ptimo
print("\n=== PASO 2: ENCONTRAR N√öMERO √ìPTIMO DE CLUSTERS ===")

def metodo_codo(X, max_k=8):
    """M√©todo del codo para encontrar k √≥ptimo"""
    inertias = []
    k_range = range(1, max_k + 1)
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X)
        inertias.append(kmeans.inertia_)
    
    return k_range, inertias

k_range, inertias = metodo_codo(X_estudiantes)

plt.figure(figsize=(8, 5))
plt.plot(k_range, inertias, 'bo-')
plt.xlabel('N√∫mero de Clusters (k)')
plt.ylabel('Inercia')
plt.title('M√©todo del Codo - Estudiantes')
plt.grid(True)
plt.axvline(x=3, color='red', linestyle='--', label='K=3 (√≥ptimo)')
plt.legend()
plt.show()

print("Interpretaci√≥n: El 'codo' est√° en K=3, donde la inercia deja de disminuir r√°pidamente")

# PASO 3: Aplicar K-Means con K=3
print("\n=== PASO 3: APLICAR K-MEANS ===")
kmeans_estudiantes = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters_estudiantes = kmeans_estudiantes.fit_predict(X_estudiantes)

# Agregar clusters al dataframe
df_estudiantes['cluster'] = clusters_estudiantes

# PASO 4: Visualizar resultados
print("\n=== PASO 4: VISUALIZAR RESULTADOS ===")
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
scatter = plt.scatter(df_estudiantes['horas_estudio'], df_estudiantes['nota_promedio'], 
                     c=df_estudiantes['cluster'], cmap='viridis', alpha=0.7)
plt.xlabel('Horas de Estudio')
plt.ylabel('Nota Promedio')
plt.title('K-Means: Horas vs Notas')
plt.colorbar(scatter, label='Cluster')

plt.subplot(1, 3, 2)
scatter = plt.scatter(df_estudiantes['horas_estudio'], df_estudiantes['asistencia'], 
                     c=df_estudiantes['cluster'], cmap='viridis', alpha=0.7)
plt.xlabel('Horas de Estudio')
plt.ylabel('Asistencia (%)')
plt.title('K-Means: Horas vs Asistencia')
plt.colorbar(scatter, label='Cluster')

plt.subplot(1, 3, 3)
scatter = plt.scatter(df_estudiantes['nota_promedio'], df_estudiantes['asistencia'], 
                     c=df_estudiantes['cluster'], cmap='viridis', alpha=0.7)
plt.xlabel('Nota Promedio')
plt.ylabel('Asistencia (%)')
plt.title('K-Means: Notas vs Asistencia')
plt.colorbar(scatter, label='Cluster')

plt.tight_layout()
plt.show()

# PASO 5: Interpretar clusters
print("\n=== PASO 5: INTERPRETAR CLUSTERS ===")
for i in range(3):
    cluster_data = df_estudiantes[df_estudiantes['cluster'] == i]
    print(f"\nüîπ Cluster {i} ({len(cluster_data)} estudiantes):")
    print(f"   Horas de estudio promedio: {cluster_data['horas_estudio'].mean():.1f}")
    print(f"   Nota promedio: {cluster_data['nota_promedio'].mean():.1f}")
    print(f"   Asistencia promedio: {cluster_data['asistencia'].mean():.1f}%")
    
    # Interpretaci√≥n
    if cluster_data['horas_estudio'].mean() > 6:
        print("   üìö Interpretaci√≥n: ESTUDIANTES APLICADOS")
    elif cluster_data['horas_estudio'].mean() > 3:
        print("   üìñ Interpretaci√≥n: ESTUDIANTES PROMEDIO")
    else:
        print("   üìù Interpretaci√≥n: ESTUDIANTES CON DIFICULTADES")

print("\n‚úÖ CONCLUSI√ìN: El algoritmo descubri√≥ autom√°ticamente 3 tipos de estudiantes")
print("   sin necesidad de etiquetas previas!")
```

### **üìö Ejemplo 2: Clustering de Productos de E-commerce**

Un ejemplo m√°s complejo y realista para mostrar aplicaciones empresariales.

```python
# EJEMPLO 2: Segmentaci√≥n de Productos de E-commerce
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Crear datos de productos de e-commerce
np.random.seed(42)

# Generar diferentes tipos de productos
# Tipo 1: Productos premium (alto precio, baja rotaci√≥n, alta satisfacci√≥n)
premium = {
    'precio': np.random.normal(500, 100, 50),
    'ventas_mensuales': np.random.poisson(20, 50),
    'rating_promedio': np.random.normal(4.8, 0.2, 50),
    'stock_dias': np.random.normal(45, 10, 50)
}

# Tipo 2: Productos populares (precio medio, alta rotaci√≥n, buen rating)
populares = {
    'precio': np.random.normal(150, 30, 100),
    'ventas_mensuales': np.random.poisson(200, 100),
    'rating_promedio': np.random.normal(4.2, 0.3, 100),
    'stock_dias': np.random.normal(15, 5, 100)
}

# Tipo 3: Productos b√°sicos (bajo precio, rotaci√≥n media, rating variable)
basicos = {
    'precio': np.random.normal(50, 15, 80),
    'ventas_mensuales': np.random.poisson(80, 80),
    'rating_promedio': np.random.normal(3.5, 0.5, 80),
    'stock_dias': np.random.normal(30, 8, 80)
}

# Combinar datos
datos_productos = {
    'precio': np.concatenate([premium['precio'], populares['precio'], basicos['precio']]),
    'ventas_mensuales': np.concatenate([premium['ventas_mensuales'], populares['ventas_mensuales'], basicos['ventas_mensuales']]),
    'rating_promedio': np.concatenate([premium['rating_promedio'], populares['rating_promedio'], basicos['rating_promedio']]),
    'stock_dias': np.concatenate([premium['stock_dias'], populares['stock_dias'], basicos['stock_dias']])
}

df_productos = pd.DataFrame(datos_productos)

print("=== EJEMPLO 2: PRODUCTOS DE E-COMMERCE ===")
print("Objetivo: Segmentar productos para estrategias de marketing y pricing")
print(f"\nDataset shape: {df_productos.shape}")
print(f"\nEstad√≠sticas descriptivas:")
print(df_productos.describe())

# Visualizaci√≥n inicial
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(df_productos['precio'], df_productos['ventas_mensuales'], alpha=0.6)
plt.xlabel('Precio ($)')
plt.ylabel('Ventas Mensuales')
plt.title('Precio vs Ventas')

plt.subplot(1, 3, 2)
plt.scatter(df_productos['precio'], df_productos['rating_promedio'], alpha=0.6)
plt.xlabel('Precio ($)')
plt.ylabel('Rating Promedio')
plt.title('Precio vs Rating')

plt.subplot(1, 3, 3)
plt.scatter(df_productos['ventas_mensuales'], df_productos['rating_promedio'], alpha=0.6)
plt.xlabel('Ventas Mensuales')
plt.ylabel('Rating Promedio')
plt.title('Ventas vs Rating')

plt.tight_layout()
plt.show()

# Normalizar datos
scaler_productos = StandardScaler()
X_productos = scaler_productos.fit_transform(df_productos)

# Aplicar K-Means
kmeans_productos = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters_productos = kmeans_productos.fit_predict(X_productos)

# Agregar clusters
df_productos['cluster'] = clusters_productos

# Visualizar resultados
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
scatter = plt.scatter(df_productos['precio'], df_productos['ventas_mensuales'], 
                     c=df_productos['cluster'], cmap='viridis', alpha=0.7)
plt.xlabel('Precio ($)')
plt.ylabel('Ventas Mensuales')
plt.title('K-Means: Precio vs Ventas')
plt.colorbar(scatter, label='Cluster')

plt.subplot(1, 3, 2)
scatter = plt.scatter(df_productos['precio'], df_productos['rating_promedio'], 
                     c=df_productos['cluster'], cmap='viridis', alpha=0.7)
plt.xlabel('Precio ($)')
plt.ylabel('Rating Promedio')
plt.title('K-Means: Precio vs Rating')
plt.colorbar(scatter, label='Cluster')

plt.subplot(1, 3, 3)
scatter = plt.scatter(df_productos['ventas_mensuales'], df_productos['rating_promedio'], 
                     c=df_productos['cluster'], cmap='viridis', alpha=0.7)
plt.xlabel('Ventas Mensuales')
plt.ylabel('Rating Promedio')
plt.title('K-Means: Ventas vs Rating')
plt.colorbar(scatter, label='Cluster')

plt.tight_layout()
plt.show()

# An√°lisis de clusters
print("\n=== AN√ÅLISIS DE SEGMENTOS DE PRODUCTOS ===")
for i in range(3):
    cluster_data = df_productos[df_productos['cluster'] == i]
    print(f"\nüîπ Segmento {i} ({len(cluster_data)} productos):")
    print(f"   Precio promedio: ${cluster_data['precio'].mean():.0f}")
    print(f"   Ventas promedio: {cluster_data['ventas_mensuales'].mean():.0f} unidades/mes")
    print(f"   Rating promedio: {cluster_data['rating_promedio'].mean():.1f}/5")
    print(f"   Stock promedio: {cluster_data['stock_dias'].mean():.0f} d√≠as")
    
    # Estrategia de negocio
    if cluster_data['precio'].mean() > 300:
        print("   üíé Estrategia: PRODUCTOS PREMIUM - Marketing exclusivo, alta calidad")
    elif cluster_data['ventas_mensuales'].mean() > 150:
        print("   üöÄ Estrategia: PRODUCTOS POPULARES - Promociones, stock alto")
    else:
        print("   üì¶ Estrategia: PRODUCTOS B√ÅSICOS - Precios competitivos, rotaci√≥n media")

# Calcular m√©tricas de calidad
silhouette_avg = silhouette_score(X_productos, clusters_productos)
print(f"\nüìä M√©trica de Calidad:")
print(f"   Silhouette Score: {silhouette_avg:.3f} (0.5+ es bueno)")

print("\n‚úÖ APLICACI√ìN PR√ÅCTICA:")
print("   - Segmento 0: Estrategia de pricing premium")
print("   - Segmento 1: Campa√±as de marketing masivo")
print("   - Segmento 2: Optimizaci√≥n de inventario")
```

---

## üìã **8.2.4 Resumen de Algoritmos de Clustering**

### **üéØ Comparaci√≥n de Algoritmos Populares**

| Algoritmo | Mejor Para | Ventajas | Desventajas | Cu√°ndo Usar |
|-----------|------------|----------|-------------|-------------|
| **K-Means** | Clusters esf√©ricos bien separados | Simple, r√°pido, escalable | Requiere K, sensible a outliers | Datos con clusters claros y esf√©ricos |
| **DBSCAN** | Clusters de forma arbitraria | No requiere K, maneja outliers | Sensible a par√°metros | Datos con ruido, formas irregulares |
| **Jer√°rquico** | Exploraci√≥n de estructura | No requiere K, interpretable | Costoso computacionalmente | Datasets peque√±os, exploraci√≥n |

### **üîç Clustering Basado en Densidad (DBSCAN)**

Los m√©todos de clustering basados en densidad identifican grupos en un conjunto de datos considerando la densidad de puntos en el espacio de datos. A diferencia de otros m√©todos como K-means, que dependen de la distancia entre puntos y requieren definir el n√∫mero de cl√∫steres, estos m√©todos se enfocan en encontrar regiones densamente pobladas separadas por √°reas de baja densidad.

#### **üéØ ¬øC√≥mo Funcionan?**

El principio central es que los cl√∫steres se forman en √°reas contiguas de alta densidad, con las regiones de baja densidad actuando como separadores. Un algoritmo destacado es **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise), que funciona as√≠:

1. **Vecindad Œµ (epsilon)**: Define un radio alrededor de un punto. Si hay suficientes puntos dentro de este radio (MinPts), el √°rea es densa.
2. **Puntos Centrales y Frontera**: Un punto central tiene al menos MinPts en su vecindad Œµ; un punto frontera tiene menos pero est√° cerca de un punto central.
3. **Expansi√≥n de Cl√∫steres**: Un cl√∫ster crece incluyendo puntos en la vecindad Œµ hasta que no se puedan agregar m√°s.
4. **Ruido**: Los puntos que no pertenecen a una vecindad densa se consideran ruido o outliers.

#### **üéØ Prop√≥sito y Aplicaciones**

DBSCAN es √∫til para detectar cl√∫steres de formas arbitrarias y manejar outliers sin necesidad de especificar el n√∫mero de cl√∫steres. Es aplicado en:
- **An√°lisis geoespacial**: Agrupar ubicaciones por densidad
- **Segmentaci√≥n de clientes**: En marketing, donde los datos no tienen formas esf√©ricas definidas
- **Detecci√≥n de anomal√≠as**: Identificar patrones inusuales

### **üå≥ Clustering Jer√°rquico y Dendrogramas**

Un **dendrograma** es una representaci√≥n visual que muestra c√≥mo se agrupan y dividen los cl√∫steres en un proceso de clustering jer√°rquico. Cada bifurcaci√≥n en el diagrama representa un punto de uni√≥n entre los datos, ayudando a visualizar las relaciones entre diferentes cl√∫steres y la estructura jer√°rquica de los mismos.

#### **üéØ Interpretaci√≥n del Dendrograma:**
- **Altura de las l√≠neas**: Indica la distancia entre clusters
- **Bifurcaciones**: Muestran c√≥mo se fusionan los clusters
- **Corte horizontal**: Determina el n√∫mero final de clusters

### **‚úÖ Resumen Final**

En resumen, el clustering es una herramienta poderosa en el an√°lisis de datos no etiquetados, proporcionando un m√©todo para organizar y explorar grandes vol√∫menes de datos al identificar patrones y estructuras ocultas, sin necesidad de etiquetas o categor√≠as predefinidas.

**üéØ Consejos para Elegir el Algoritmo Correcto:**
1. **K-Means**: Ideal para clusters bien definidos, r√°pido y sencillo, pero sensible a los outliers y requiere que se especifique K.
2. **DBSCAN**: Eficaz para clusters de formas arbitrarias y robusto contra outliers, no requiere especificar el n√∫mero de clusters, pero depende de la correcta elecci√≥n de par√°metros.
3. **Clustering Jer√°rquico**: Ofrece una estructura completa de clusters y no necesita predefinir el n√∫mero de clusters, aunque puede ser costoso en t√©rminos de tiempo de c√°lculo para grandes datasets.

Estos algoritmos son fundamentales en el an√°lisis de datos no etiquetados, proporcionando diferentes enfoques para descubrir patrones ocultos y estructurar la informaci√≥n de manera significativa.

---

## üìä **8.2.3 Ejemplo Pr√°ctico Avanzado: Segmentaci√≥n de Clientes**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Crear dataset realista de clientes
np.random.seed(42)
n_clientes = 500

# Simular diferentes tipos de clientes
# Tipo 1: J√≥venes con ingresos bajos, gastos moderados
jovenes = {
    'edad': np.random.normal(25, 3, 150),
    'ingresos': np.random.normal(30000, 5000, 150),
    'gasto_mensual': np.random.normal(800, 200, 150),
    'frecuencia': np.random.poisson(6, 150)
}

# Tipo 2: Adultos medios con ingresos altos, gastos altos
adultos_ricos = {
    'edad': np.random.normal(45, 5, 200),
    'ingresos': np.random.normal(80000, 10000, 200),
    'gasto_mensual': np.random.normal(2500, 500, 200),
    'frecuencia': np.random.poisson(12, 200)
}

# Tipo 3: Adultos mayores con ingresos medios, gastos bajos
mayores = {
    'edad': np.random.normal(65, 5, 150),
    'ingresos': np.random.normal(50000, 8000, 150),
    'gasto_mensual': np.random.normal(600, 150, 150),
    'frecuencia': np.random.poisson(3, 150)
}

# Combinar todos los datos
data = {
    'edad': np.concatenate([jovenes['edad'], adultos_ricos['edad'], mayores['edad']]),
    'ingresos': np.concatenate([jovenes['ingresos'], adultos_ricos['ingresos'], mayores['ingresos']]),
    'gasto_mensual': np.concatenate([jovenes['gasto_mensual'], adultos_ricos['gasto_mensual'], mayores['gasto_mensual']]),
    'frecuencia': np.concatenate([jovenes['frecuencia'], adultos_ricos['frecuencia'], mayores['frecuencia']])
}

df = pd.DataFrame(data)

# Agregar ruido para hacer m√°s realista
df['edad'] += np.random.normal(0, 2, len(df))
df['ingresos'] += np.random.normal(0, 2000, len(df))
df['gasto_mesual'] += np.random.normal(0, 100, len(df))

print("=== DATASET DE CLIENTES ===")
print(f"Forma del dataset: {df.shape}")
print(f"\nEstad√≠sticas descriptivas:")
print(df.describe())

# Visualizaci√≥n inicial
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(df['edad'], df['gasto_mensual'], alpha=0.6)
plt.xlabel('Edad')
plt.ylabel('Gasto Mensual')
plt.title('Edad vs Gasto Mensual')

plt.subplot(1, 3, 2)
plt.scatter(df['ingresos'], df['gasto_mensual'], alpha=0.6)
plt.xlabel('Ingresos')
plt.ylabel('Gasto Mensual')
plt.title('Ingresos vs Gasto Mensual')

plt.subplot(1, 3, 3)
plt.scatter(df['frecuencia'], df['gasto_mensual'], alpha=0.6)
plt.xlabel('Frecuencia de Compras')
plt.ylabel('Gasto Mensual')
plt.title('Frecuencia vs Gasto Mensual')

plt.tight_layout()
plt.show()
```

---

## üß© **8.2.1 Principales enfoques de Clustering y sus caracter√≠sticas**

### 1. üîπ **Clustering por Particiones (K-Means)**

Divide los datos en *k* grupos predefinidos maximizando la similitud dentro de los cl√∫steres y minimizando la similitud entre ellos.

#### **üéØ K-Means en Acci√≥n:**

```python
# Preparar datos para clustering
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[['edad', 'ingresos', 'gasto_mensual', 'frecuencia']])

# M√©todo del Codo para encontrar k √≥ptimo
def metodo_codo(X, max_k=10):
    """Encuentra el n√∫mero √≥ptimo de clusters usando el m√©todo del codo"""
    inertias = []
    k_range = range(1, max_k + 1)
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X)
        inertias.append(kmeans.inertia_)
    
    return k_range, inertias

# Aplicar m√©todo del codo
k_range, inertias = metodo_codo(X_scaled)

# Visualizar m√©todo del codo
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(k_range, inertias, 'bo-')
plt.xlabel('N√∫mero de Clusters (k)')
plt.ylabel('Inercia')
plt.title('M√©todo del Codo')
plt.grid(True)

# M√©todo de Silhouette
def metodo_silhouette(X, max_k=10):
    """Encuentra el n√∫mero √≥ptimo de clusters usando silhouette"""
    silhouette_scores = []
    k_range = range(2, max_k + 1)
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X)
        silhouette_avg = silhouette_score(X, cluster_labels)
        silhouette_scores.append(silhouette_avg)
    
    return k_range, silhouette_scores

k_range_sil, silhouette_scores = metodo_silhouette(X_scaled)

plt.subplot(1, 2, 2)
plt.plot(k_range_sil, silhouette_scores, 'ro-')
plt.xlabel('N√∫mero de Clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('M√©todo de Silhouette')
plt.grid(True)

plt.tight_layout()
plt.show()

# Aplicar K-Means con k=3 (basado en nuestro conocimiento del dataset)
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X_scaled)

# Agregar clusters al dataframe
df['cluster'] = cluster_labels

# Visualizar resultados
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
scatter = plt.scatter(df['edad'], df['gasto_mensual'], c=df['cluster'], cmap='viridis', alpha=0.6)
plt.xlabel('Edad')
plt.ylabel('Gasto Mensual')
plt.title('K-Means: Edad vs Gasto')
plt.colorbar(scatter)

plt.subplot(1, 3, 2)
scatter = plt.scatter(df['ingresos'], df['gasto_mensual'], c=df['cluster'], cmap='viridis', alpha=0.6)
plt.xlabel('Ingresos')
plt.ylabel('Gasto Mensual')
plt.title('K-Means: Ingresos vs Gasto')
plt.colorbar(scatter)

plt.subplot(1, 3, 3)
scatter = plt.scatter(df['frecuencia'], df['gasto_mensual'], c=df['cluster'], cmap='viridis', alpha=0.6)
plt.xlabel('Frecuencia')
plt.ylabel('Gasto Mensual')
plt.title('K-Means: Frecuencia vs Gasto')
plt.colorbar(scatter)

plt.tight_layout()
plt.show()

# An√°lisis de clusters
print("\n=== AN√ÅLISIS DE CLUSTERS K-MEANS ===")
for i in range(3):
    cluster_data = df[df['cluster'] == i]
    print(f"\nCluster {i}:")
    print(f"  Tama√±o: {len(cluster_data)} clientes ({len(cluster_data)/len(df)*100:.1f}%)")
    print(f"  Edad promedio: {cluster_data['edad'].mean():.1f}")
    print(f"  Ingresos promedio: ${cluster_data['ingresos'].mean():,.0f}")
    print(f"  Gasto promedio: ${cluster_data['gasto_mensual'].mean():,.0f}")
    print(f"  Frecuencia promedio: {cluster_data['frecuencia'].mean():.1f}")

# Calcular silhouette score
silhouette_avg = silhouette_score(X_scaled, cluster_labels)
print(f"\nSilhouette Score: {silhouette_avg:.3f}")
```

**üéØ Interpretaci√≥n de Resultados:**
- **Cluster 0**: J√≥venes con ingresos bajos
- **Cluster 1**: Adultos mayores con gastos conservadores  
- **Cluster 2**: Adultos medios con altos ingresos y gastos

üëâ *Ventajas:* Simple, r√°pido, escalable
üëâ *Desventajas:* Requiere definir k, sensible a outliers, asume clusters esf√©ricos

---

### 2. üîπ **Clustering Jer√°rquico**

Crea una estructura tipo √°rbol (dendrograma) que refleja c√≥mo se agrupan los datos paso a paso.

#### **üå≥ Clustering Jer√°rquico en Acci√≥n:**

```python
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist

# Aplicar clustering jer√°rquico
hierarchical = AgglomerativeClustering(n_clusters=3)
hierarchical_labels = hierarchical.fit_predict(X_scaled)

# Crear dendrograma
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
linkage_matrix = linkage(X_scaled, method='ward')
dendrogram(linkage_matrix, truncate_mode='level', p=5)
plt.title('Dendrograma - Clustering Jer√°rquico')
plt.xlabel('√çndice de Muestra')
plt.ylabel('Distancia')

# Visualizar clusters jer√°rquicos
plt.subplot(1, 3, 2)
scatter = plt.scatter(df['edad'], df['gasto_mensual'], c=hierarchical_labels, cmap='viridis', alpha=0.6)
plt.xlabel('Edad')
plt.ylabel('Gasto Mensual')
plt.title('Clustering Jer√°rquico')
plt.colorbar(scatter)

plt.subplot(1, 3, 3)
scatter = plt.scatter(df['ingresos'], df['gasto_mensual'], c=hierarchical_labels, cmap='viridis', alpha=0.6)
plt.xlabel('Ingresos')
plt.ylabel('Gasto Mensual')
plt.title('Clustering Jer√°rquico')
plt.colorbar(scatter)

plt.tight_layout()
plt.show()

print("\n=== COMPARACI√ìN K-MEANS vs JER√ÅRQUICO ===")
print(f"K-Means Silhouette: {silhouette_score(X_scaled, cluster_labels):.3f}")
print(f"Jer√°rquico Silhouette: {silhouette_score(X_scaled, hierarchical_labels):.3f}")
```

**üéØ Tipos de Clustering Jer√°rquico:**
- **Aglomerativo**: Funde cl√∫steres de abajo hacia arriba
- **Divisivo**: Divide de arriba hacia abajo

üëâ *Ventajas:* No necesita predefinir k, interpretable
üëâ *Desventajas:* Costoso computacionalmente, sensible a outliers

---

### 3. üîπ **Clustering por Densidad (DBSCAN)**

Agrupa puntos que est√°n densamente conectados entre s√≠, detectando autom√°ticamente outliers.

#### **üéØ DBSCAN en Acci√≥n:**

```python
from sklearn.neighbors import NearestNeighbors

# Funci√≥n para encontrar eps √≥ptimo usando k-distance graph
def encontrar_eps_optimo(X, k=4):
    """Encuentra el eps √≥ptimo usando el gr√°fico de k-distancia"""
    neighbors = NearestNeighbors(n_neighbors=k)
    neighbors_fit = neighbors.fit(X)
    distances, indices = neighbors_fit.kneighbors(X)
    distances = np.sort(distances[:, k-1], axis=0)
    return distances

# Encontrar eps √≥ptimo
distances = encontrar_eps_optimo(X_scaled)
eps_optimo = distances[int(len(distances) * 0.1)]  # Usar percentil 10

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(distances)
plt.axhline(y=eps_optimo, color='r', linestyle='--', label=f'eps √≥ptimo: {eps_optimo:.2f}')
plt.xlabel('Puntos ordenados por distancia')
plt.ylabel('Distancia al k-√©simo vecino')
plt.title('M√©todo de k-distancia para encontrar eps')
plt.legend()
plt.grid(True)

# Aplicar DBSCAN
dbscan = DBSCAN(eps=eps_optimo, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_scaled)

# Visualizar resultados DBSCAN
plt.subplot(1, 2, 2)
scatter = plt.scatter(df['edad'], df['gasto_mensual'], c=dbscan_labels, cmap='viridis', alpha=0.6)
plt.xlabel('Edad')
plt.ylabel('Gasto Mensual')
plt.title(f'DBSCAN (eps={eps_optimo:.2f})')
plt.colorbar(scatter)

plt.tight_layout()
plt.show()

# An√°lisis de resultados DBSCAN
n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
n_noise = list(dbscan_labels).count(-1)

print(f"\n=== RESULTADOS DBSCAN ===")
print(f"N√∫mero de clusters encontrados: {n_clusters}")
print(f"N√∫mero de puntos de ruido: {n_noise}")
print(f"Porcentaje de ruido: {n_noise/len(dbscan_labels)*100:.1f}%")

# Comparar todos los m√©todos
plt.figure(figsize=(15, 5))

algoritmos = [
    ('K-Means', cluster_labels),
    ('Jer√°rquico', hierarchical_labels),
    ('DBSCAN', dbscan_labels)
]

for i, (nombre, labels) in enumerate(algoritmos):
    plt.subplot(1, 3, i+1)
    scatter = plt.scatter(df['edad'], df['gasto_mensual'], c=labels, cmap='viridis', alpha=0.6)
    plt.xlabel('Edad')
    plt.ylabel('Gasto Mensual')
    plt.title(f'{nombre}')
    plt.colorbar(scatter)

plt.tight_layout()
plt.show()

# Comparar m√©tricas
print(f"\n=== COMPARACI√ìN DE ALGORITMOS ===")
for nombre, labels in algoritmos:
    if len(set(labels)) > 1:  # Solo si hay m√°s de un cluster
        score = silhouette_score(X_scaled, labels)
        print(f"{nombre}: Silhouette Score = {score:.3f}")
    else:
        print(f"{nombre}: No se puede calcular silhouette (solo un cluster)")
```

**üéØ Caracter√≠sticas de DBSCAN:**
- **eps**: Distancia m√°xima entre puntos para ser considerados vecinos
- **min_samples**: N√∫mero m√≠nimo de puntos para formar un cluster
- **Outliers**: Puntos marcados como -1 (ruido)

üëâ *Ventajas:* Detecta clusters de forma arbitraria, maneja ruido, no necesita k
üëâ *Desventajas:* Sensible a par√°metros, dif√≠cil con clusters de densidad variable

---

### 4. üîπ **Clustering Basado en Grid**
Divide el espacio en celdas y agrupa seg√∫n densidad local en cada celda.
- **Wavecluster**: Transformaci√≥n de onda
- **STING**: Grillas estad√≠sticas jer√°rquicas
- **CLIQUE**: Alta dimensi√≥n
üëâ *Ventajas:* Eficiente en altas dimensiones
üëâ *Desventajas:* Sensible al tama√±o de grilla

---

### 5. üîπ **Clustering Basado en Modelos**
Asume que los datos se generan a partir de un modelo estad√≠stico.
- **GMM**: Distribuciones normales m√∫ltiples
- **COBWEB**: √Årbol jer√°rquico categ√≥rico
- **SOMs**: Red neuronal auto-organizativa
üëâ *Ventajas:* Modelos probabil√≠sticos flexibles
üëâ *Desventajas:* Requiere supuestos sobre distribuci√≥n

---

## üìä **8.3 Reducci√≥n de Dimensionalidad**

### **üéØ ¬øQu√© es la Reducci√≥n de Dimensionalidad?**

La **reducci√≥n de dimensionalidad** es una t√©cnica clave en el an√°lisis de datos complejos que busca simplificar los conjuntos de datos sin perder informaci√≥n relevante. En la pr√°ctica, los conjuntos de datos suelen tener un gran n√∫mero de variables o caracter√≠sticas, lo que puede hacer que el an√°lisis sea complicado y computacionalmente costoso. La reducci√≥n de dimensionalidad permite disminuir el n√∫mero de variables en un dataset, facilitando as√≠ la visualizaci√≥n, el almacenamiento y el procesamiento de los datos.

#### **üéØ Objetivos de la Reducci√≥n de Dimensionalidad:**

1. **Simplificaci√≥n del Modelo**: Reducir la complejidad del modelo al disminuir el n√∫mero de variables sin sacrificar la precisi√≥n en las predicciones o an√°lisis.

2. **Mejora de la Interpretabilidad**: Facilitar la comprensi√≥n y visualizaci√≥n de los datos al representarlos en un espacio de menor dimensi√≥n.

3. **Mitigaci√≥n de la Maldici√≥n de la Dimensionalidad**: Enfrentar los problemas que surgen cuando los datos tienen demasiadas dimensiones, lo que puede llevar a un sobreajuste del modelo y a una interpretaci√≥n err√≥nea de los resultados.

#### **‚úÖ Beneficios en el An√°lisis de Datos:**

- **Mejora del Rendimiento Computacional**: Menos dimensiones implican menos datos que procesar, lo que se traduce en algoritmos m√°s r√°pidos y eficientes.

- **Reducci√≥n de Ruido**: Eliminar variables irrelevantes o redundantes puede mejorar la calidad del an√°lisis al enfocarse solo en las caracter√≠sticas m√°s importantes.

- **Facilitaci√≥n de la Visualizaci√≥n**: Los datos en menor dimensi√≥n pueden representarse m√°s f√°cilmente en gr√°ficos, ayudando a detectar patrones y relaciones que no ser√≠an evidentes en un espacio de mayor dimensi√≥n.

#### **üîç ¬øPor qu√© reducir dimensiones?**

1. **Visualizaci√≥n**: Solo podemos ver 2-3 dimensiones
2. **Curse of Dimensionality**: M√°s dimensiones = m√°s datos necesarios
3. **Ruido**: Dimensiones irrelevantes confunden algoritmos
4. **Computaci√≥n**: Menos dimensiones = m√°s r√°pido

---

## üîß **8.3.1 T√©cnicas de Reducci√≥n: PCA (Principal Component Analysis)**

El **An√°lisis de Componentes Principales (PCA)** es una de las t√©cnicas m√°s utilizadas en la reducci√≥n de dimensionalidad en conjuntos de datos. Su objetivo principal es transformar un conjunto de variables posiblemente correlacionadas en un conjunto de valores de variables no correlacionadas, llamadas componentes principales.

### **üéØ ¬øQu√© es PCA?**

PCA es un m√©todo estad√≠stico que convierte un conjunto de observaciones de variables posiblemente correlacionadas en un conjunto de valores de variables no correlacionadas denominadas **componentes principales**. Este proceso se lleva a cabo de tal manera que:

- El **primer componente principal** tiene la mayor varianza posible (explica la mayor parte de la variabilidad en los datos)
- Cada **componente sucesivo** tiene la mayor varianza posible bajo la restricci√≥n de ser ortogonal a los componentes anteriores

### **üîç Aplicaci√≥n de PCA en la Reducci√≥n de Dimensiones:**

1. **Identificaci√≥n de la Varianza**: PCA identifica las direcciones (componentes principales) en las que la varianza en los datos es m√°xima. Esto permite capturar la estructura esencial de los datos con menos dimensiones.

2. **Transformaci√≥n de Datos**: Los datos originales se proyectan sobre los componentes principales seleccionados, reduciendo as√≠ el n√∫mero de dimensiones mientras se retiene la mayor parte de la informaci√≥n original.

3. **Eliminaci√≥n de Ruido**: Al centrarse en los componentes principales que capturan la mayor parte de la variabilidad, PCA ayuda a eliminar el ruido y las redundancias de los datos.

### **‚úÖ Beneficios de Usar PCA:**

- **Simplificaci√≥n**: PCA reduce la cantidad de datos a procesar, facilitando el an√°lisis y mejorando el rendimiento computacional.
- **Visualizaci√≥n**: La reducci√≥n de dimensiones mediante PCA permite una mejor visualizaci√≥n de los datos, especialmente cuando se reduce a 2 o 3 dimensiones.
- **Optimizaci√≥n de Modelos**: Al eliminar la multicolinealidad y concentrar la varianza en un menor n√∫mero de componentes, los modelos pueden ser m√°s precisos y menos propensos al sobreajuste.

---

## üéì **8.3.2 Ejemplo Pr√°ctico: PCA con Datos de Clientes**

Vamos a ver PCA en acci√≥n con un ejemplo completo y bien explicado:

```python
# EJEMPLO: PCA con Datos de Clientes
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Crear dataset con m√∫ltiples variables
np.random.seed(42)
n_clientes = 200

# Generar datos correlacionados (simulando comportamiento real)
# Variables relacionadas con el comportamiento del cliente
datos_clientes = {
    'edad': np.random.normal(35, 12, n_clientes),
    'ingresos_anuales': np.random.normal(50000, 15000, n_clientes),
    'gasto_mensual': np.random.normal(2000, 600, n_clientes),
    'frecuencia_compras': np.random.poisson(8, n_clientes),
    'satisfaccion': np.random.normal(4.2, 0.8, n_clientes),
    'tiempo_cliente': np.random.normal(24, 8, n_clientes),  # meses
    'productos_comprados': np.random.poisson(15, n_clientes),
    'descuentos_usados': np.random.poisson(3, n_clientes)
}

# Agregar correlaciones realistas
datos_clientes['gasto_mensual'] += datos_clientes['ingresos_anuales'] * 0.02  # Correlaci√≥n positiva
datos_clientes['frecuencia_compras'] += datos_clientes['gasto_mensual'] * 0.01
datos_clientes['satisfaccion'] += datos_clientes['tiempo_cliente'] * 0.01

df_clientes = pd.DataFrame(datos_clientes)

print("=== EJEMPLO: PCA CON DATOS DE CLIENTES ===")
print("Objetivo: Reducir 8 variables a 2-3 componentes principales")
print(f"\nDataset shape: {df_clientes.shape}")
print(f"\nVariables originales:")
print(df_clientes.columns.tolist())
print(f"\nEstad√≠sticas descriptivas:")
print(df_clientes.describe())

# PASO 1: An√°lisis de correlaciones
print("\n=== PASO 1: AN√ÅLISIS DE CORRELACIONES ===")
correlation_matrix = df_clientes.corr()
print("Matriz de correlaciones:")
print(correlation_matrix.round(3))

# Visualizar correlaciones
plt.figure(figsize=(10, 8))
import seaborn as sns
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
            square=True, fmt='.2f')
plt.title('Matriz de Correlaciones - Variables Originales')
plt.tight_layout()
plt.show()

print("\nObservaci√≥n: Hay correlaciones fuertes entre variables")
print("Esto indica redundancia que PCA puede eliminar")

# PASO 2: Normalizar datos
print("\n=== PASO 2: NORMALIZACI√ìN DE DATOS ===")
print("¬øPor qu√© normalizar? Las variables tienen escalas muy diferentes:")
for col in df_clientes.columns:
    print(f"{col}: {df_clientes[col].min():.1f} - {df_clientes[col].max():.1f}")

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_clientes)
print(f"\nDatos normalizados shape: {X_scaled.shape}")

# PASO 3: Aplicar PCA
print("\n=== PASO 3: APLICAR PCA ===")

# Primero, analizar cu√°ntos componentes necesitamos
pca_full = PCA()
pca_full.fit(X_scaled)

# Calcular varianza explicada acumulada
explained_variance_ratio = pca_full.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

# Visualizar varianza explicada
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)
plt.xlabel('Componente Principal')
plt.ylabel('Varianza Explicada')
plt.title('Varianza Explicada por Componente')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')
plt.axhline(y=0.95, color='red', linestyle='--', label='95% de varianza')
plt.axhline(y=0.85, color='orange', linestyle='--', label='85% de varianza')
plt.xlabel('N√∫mero de Componentes')
plt.ylabel('Varianza Explicada Acumulada')
plt.title('Varianza Explicada Acumulada')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Mostrar informaci√≥n detallada
print("Varianza explicada por cada componente:")
for i, var in enumerate(explained_variance_ratio):
    print(f"PC{i+1}: {var:.1%}")

print(f"\nVarianza explicada acumulada:")
for i, cum_var in enumerate(cumulative_variance):
    print(f"PC1-PC{i+1}: {cum_var:.1%}")

# Decidir n√∫mero de componentes (usar 2 para visualizaci√≥n)
n_components = 2
print(f"\nDecisi√≥n: Usar {n_components} componentes principales")
print(f"Esto explica el {cumulative_variance[n_components-1]:.1%} de la varianza")

# Aplicar PCA con 2 componentes
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_scaled)

print(f"\nDatos transformados shape: {X_pca.shape}")
print("Reducci√≥n: 8 variables ‚Üí 2 componentes principales")

# PASO 4: Interpretar componentes principales
print("\n=== PASO 4: INTERPRETAR COMPONENTES PRINCIPALES ===")

# Crear dataframe con los componentes
components_df = pd.DataFrame(
    pca.components_.T,
    columns=[f'PC{i+1}' for i in range(n_components)],
    index=df_clientes.columns
)

print("Cargas de los componentes principales:")
print(components_df.round(3))

# Visualizar cargas
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.barh(components_df.index, components_df['PC1'])
plt.xlabel('Carga en PC1')
plt.title('Cargas del Primer Componente Principal')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.barh(components_df.index, components_df['PC2'])
plt.xlabel('Carga en PC2')
plt.title('Cargas del Segundo Componente Principal')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Interpretar componentes
print("\nüîç INTERPRETACI√ìN DE COMPONENTES:")
print("PC1 (Primer Componente):")
pc1_high = components_df['PC1'].abs().nlargest(3)
for var, load in pc1_high.items():
    direction = "positiva" if components_df.loc[var, 'PC1'] > 0 else "negativa"
    print(f"  - {var}: carga {direction} fuerte ({load:.3f})")

print("\nPC2 (Segundo Componente):")
pc2_high = components_df['PC2'].abs().nlargest(3)
for var, load in pc2_high.items():
    direction = "positiva" if components_df.loc[var, 'PC2'] > 0 else "negativa"
    print(f"  - {var}: carga {direction} fuerte ({load:.3f})")

# PASO 5: Visualizar datos en 2D
print("\n=== PASO 5: VISUALIZACI√ìN EN 2D ===")

# Aplicar clustering en el espacio reducido para colorear puntos
kmeans_pca = KMeans(n_clusters=3, random_state=42)
clusters_pca = kmeans_pca.fit_predict(X_pca)

plt.figure(figsize=(15, 5))

# Visualizaci√≥n original (primeras 2 variables)
plt.subplot(1, 3, 1)
plt.scatter(df_clientes['edad'], df_clientes['ingresos_anuales'], 
           c=clusters_pca, cmap='viridis', alpha=0.6)
plt.xlabel('Edad')
plt.ylabel('Ingresos Anuales')
plt.title('Datos Originales (2 variables)')

# Visualizaci√≥n PCA
plt.subplot(1, 3, 2)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_pca, 
                     cmap='viridis', alpha=0.6)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title('PCA: Reducci√≥n a 2D')
plt.colorbar(scatter, label='Cluster')

# Comparar con datos originales (otras 2 variables)
plt.subplot(1, 3, 3)
plt.scatter(df_clientes['gasto_mensual'], df_clientes['frecuencia_compras'], 
           c=clusters_pca, cmap='viridis', alpha=0.6)
plt.xlabel('Gasto Mensual')
plt.ylabel('Frecuencia Compras')
plt.title('Datos Originales (otras 2 variables)')

plt.tight_layout()
plt.show()

# PASO 6: An√°lisis de clusters en espacio PCA
print("\n=== PASO 6: AN√ÅLISIS DE CLUSTERS EN ESPACIO PCA ===")

# Crear dataframe con datos PCA
df_pca = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(n_components)])
df_pca['cluster'] = clusters_pca

for i in range(3):
    cluster_data = df_pca[df_pca['cluster'] == i]
    print(f"\nüîπ Cluster {i} ({len(cluster_data)} clientes):")
    print(f"   PC1 promedio: {cluster_data['PC1'].mean():.2f}")
    print(f"   PC2 promedio: {cluster_data['PC2'].mean():.2f}")

print("\n‚úÖ BENEFICIOS OBTENIDOS CON PCA:")
print("   - Reducci√≥n de 8 variables a 2 componentes")
print("   - Mantenimiento del 85%+ de la informaci√≥n")
print("   - Eliminaci√≥n de redundancias")
print("   - Mejor visualizaci√≥n de patrones")
print("   - Clustering m√°s eficiente")

# PASO 7: Comparar rendimiento
print("\n=== PASO 7: COMPARAR RENDIMIENTO ===")

# Clustering en datos originales
kmeans_original = KMeans(n_clusters=3, random_state=42)
clusters_original = kmeans_original.fit_predict(X_scaled)

# Comparar inercias
print(f"Inercia con datos originales: {kmeans_original.inertia_:.2f}")
print(f"Inercia con datos PCA: {kmeans_pca.inertia_:.2f}")
print(f"Mejora en eficiencia: {((kmeans_original.inertia_ - kmeans_pca.inertia_) / kmeans_original.inertia_ * 100):.1f}%")

print("\nüéØ CONCLUSI√ìN:")
print("PCA nos permiti√≥ reducir significativamente la dimensionalidad")
print("manteniendo la informaci√≥n esencial y mejorando la interpretabilidad")
```

---

## üöÄ **8.3.3 Aplicaciones Pr√°cticas de Reducci√≥n de Dimensionalidad**

La reducci√≥n de dimensionalidad es una t√©cnica ampliamente utilizada en la ciencia de datos, no solo para simplificar conjuntos de datos complejos, sino tambi√©n para mejorar la precisi√≥n y eficiencia de los modelos de predicci√≥n y facilitar la visualizaci√≥n de datos.

### **üéØ Mejora de Modelos de Predicci√≥n**

#### **1. Mitigaci√≥n de la Maldici√≥n de la Dimensionalidad:**
A medida que el n√∫mero de caracter√≠sticas en un conjunto de datos aumenta, los modelos de predicci√≥n pueden volverse m√°s propensos al sobreajuste, ya que se incrementa la complejidad del modelo. La reducci√≥n de dimensionalidad ayuda a simplificar el modelo al eliminar variables irrelevantes o redundantes, mejorando as√≠ su capacidad de generalizaci√≥n.

#### **2. Reducci√≥n del Ruido:**
Los datos de alta dimensionalidad a menudo contienen ruido, es decir, variables que no contribuyen significativamente al modelo o incluso pueden confundirlo. T√©cnicas como PCA permiten identificar y conservar solo las componentes principales que explican la mayor parte de la varianza, reduciendo as√≠ el ruido y mejorando la precisi√≥n del modelo.

#### **3. Optimizaci√≥n Computacional:**
Menos dimensiones significan menos datos que procesar. Esto reduce el tiempo y los recursos computacionales necesarios para entrenar modelos de aprendizaje autom√°tico, permitiendo trabajar con conjuntos de datos m√°s grandes o con algoritmos m√°s complejos sin comprometer el rendimiento.

### **üìä Visualizaci√≥n de Datos Complejos**

#### **1. Simplificaci√≥n de la Representaci√≥n Gr√°fica:**
Visualizar datos en alta dimensionalidad es un desaf√≠o. La reducci√≥n de dimensionalidad permite proyectar datos multidimensionales en 2 o 3 dimensiones, haciendo posible la representaci√≥n gr√°fica y la identificaci√≥n visual de patrones, tendencias y agrupamientos.

#### **2. Descubrimiento de Patrones Ocultos:**
Al reducir las dimensiones, se puede visualizar la estructura subyacente de los datos que de otro modo permanecer√≠a oculta en un espacio de alta dimensionalidad. Por ejemplo, PCA puede ayudar a identificar grupos o cl√∫steres de datos que no son evidentes en las dimensiones originales.

#### **3. Facilitaci√≥n de la Interpretaci√≥n de Datos:**
La representaci√≥n de datos en un espacio de menor dimensi√≥n facilita la interpretaci√≥n y el an√°lisis, permitiendo a los analistas de datos y a los responsables de la toma de decisiones comprender mejor las relaciones entre las variables y las tendencias generales en los datos.

### **üéØ Casos de Uso Comunes:**

| Aplicaci√≥n | T√©cnica | Beneficio |
|------------|---------|-----------|
| **An√°lisis de Im√°genes** | PCA, t-SNE | Reducir p√≠xeles a caracter√≠sticas principales |
| **An√°lisis de Texto** | LSA, LDA | Reducir dimensiones de vectores de palabras |
| **Gen√≥mica** | PCA, UMAP | Analizar miles de genes simult√°neamente |
| **Finanzas** | PCA, Factor Analysis | Identificar factores de riesgo principales |
| **Marketing** | PCA, MDS | Segmentar clientes en espacios reducidos |

---

## üìã **8.3.4 Resumen de Reducci√≥n de Dimensionalidad**

### **‚úÖ En Resumen:**

La reducci√≥n de dimensionalidad es una herramienta esencial en la ciencia de datos, particularmente cuando se trabaja con conjuntos de datos grandes y complejos, donde el objetivo es mantener la mayor cantidad de informaci√≥n posible mientras se simplifican los modelos y se mejora la interpretaci√≥n.

**üéØ T√©cnicas Principales:**
- **PCA**: Para datos lineales, preserva varianza
- **t-SNE**: Para visualizaci√≥n, preserva estructura local
- **UMAP**: Balance entre estructura local y global
- **Factor Analysis**: Para datos con estructura factorial

**üîç Cu√°ndo Usar:**
- Datasets con muchas variables (>10)
- Variables altamente correlacionadas
- Necesidad de visualizaci√≥n
- Problemas de rendimiento computacional
- Eliminaci√≥n de ruido

**‚ö†Ô∏è Consideraciones:**
- P√©rdida de interpretabilidad
- Posible p√©rdida de informaci√≥n
- Sensibilidad a escalas
- Supuestos sobre la estructura de datos

---

## üõí **8.4 Reglas de Asociaci√≥n**

### **üéØ Market Basket Analysis:**

```python
from mlxtend.frequent_patterns import apriori, association_rules

# Ejemplo de transacciones
transacciones = [
    ['Leche', 'Pan', 'Huevos'],
    ['Pan', 'Mantequilla'],
    ['Leche', 'Mantequilla', 'Huevos'],
    ['Pan', 'Leche']
]

# Convertir a formato binario
te = TransactionEncoder()
te_ary = te.fit(transacciones).transform(transacciones)
df_transacciones = pd.DataFrame(te_ary, columns=te.columns_)

# Encontrar itemsets frecuentes
frequent_itemsets = apriori(df_transacciones, min_support=0.5, use_colnames=True)

# Generar reglas
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)
print("Reglas de asociaci√≥n:")
print(rules[['antecedents', 'consequents', 'support', 'confidence']])
```

**üéØ M√©tricas:**
- **Soporte**: Frecuencia de la regla
- **Confianza**: Probabilidad condicional
- **Lift**: Mejora sobre aleatorio

---

## üìà **8.5 Evaluaci√≥n y Comparaci√≥n**

### **üéØ M√©tricas de Evaluaci√≥n:**

```python
from sklearn.metrics import silhouette_score, adjusted_rand_score

# M√©tricas internas (no necesitan etiquetas verdaderas)
silhouette = silhouette_score(X_scaled, cluster_labels)
print(f"Silhouette Score: {silhouette:.3f}")

# M√©tricas externas (necesitan etiquetas verdaderas)
ari = adjusted_rand_score(true_labels, cluster_labels)
print(f"Adjusted Rand Index: {ari:.3f}")

# Comparar algoritmos
algoritmos = [('K-Means', cluster_labels), ('DBSCAN', dbscan_labels)]
for nombre, labels in algoritmos:
    score = silhouette_score(X_scaled, labels)
    print(f"{nombre}: {score:.3f}")
```

**üéØ Interpretaci√≥n:**
- **Silhouette**: [-1, 1] - 1 es mejor
- **ARI**: [-1, 1] - 1 es mejor
- **Davies-Bouldin**: [0, ‚àû) - Menor es mejor

---

## üéØ **8.6 Actividad Pr√°ctica**

### **üìã Ejercicio: Segmentaci√≥n de Clientes E-commerce**

```python
# Dataset realista de e-commerce
np.random.seed(42)
n_clientes = 500

# Crear perfiles de clientes
perfiles = {
    'joven_tecnologico': {
        'edad': np.random.normal(25, 3, 125),
        'ingresos': np.random.normal(35000, 5000, 125),
        'gasto_electronica': np.random.gamma(2, 200, 125),
        'frecuencia_online': np.random.poisson(15, 125)
    },
    'adulto_familia': {
        'edad': np.random.normal(40, 5, 200),
        'ingresos': np.random.normal(60000, 10000, 200),
        'gasto_electronica': np.random.gamma(1.5, 150, 200),
        'frecuencia_online': np.random.poisson(8, 200)
    },
    'mayor_conservador': {
        'edad': np.random.normal(65, 5, 175),
        'ingresos': np.random.normal(45000, 8000, 175),
        'gasto_electronica': np.random.gamma(1, 50, 175),
        'frecuencia_online': np.random.poisson(3, 175)
    }
}

# Combinar datos
data_ecommerce = {
    'edad': np.concatenate([p['edad'] for p in perfiles.values()]),
    'ingresos': np.concatenate([p['ingresos'] for p in perfiles.values()]),
    'gasto_electronica': np.concatenate([p['gasto_electronica'] for p in perfiles.values()]),
    'frecuencia_online': np.concatenate([p['frecuencia_online'] for p in perfiles.values()])
}

df_ecommerce = pd.DataFrame(data_ecommerce)

# Aplicar clustering
X_ecommerce = StandardScaler().fit_transform(df_ecommerce)
kmeans_ecommerce = KMeans(n_clusters=3, random_state=42)
segmentos = kmeans_ecommerce.fit_predict(X_ecommerce)

# An√°lisis de segmentos
df_ecommerce['segmento'] = segmentos
for i in range(3):
    segmento = df_ecommerce[df_ecommerce['segmento'] == i]
    print(f"\nSegmento {i} ({len(segmento)} clientes):")
    print(f"  Edad promedio: {segmento['edad'].mean():.1f} a√±os")
    print(f"  Ingresos promedio: ${segmento['ingresos'].mean():,.0f}")
    print(f"  Gasto electr√≥nica: ${segmento['gasto_electronica'].mean():.0f}")

# Visualizaci√≥n
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
scatter = plt.scatter(df_ecommerce['edad'], df_ecommerce['ingresos'], c=segmentos, cmap='viridis', alpha=0.6)
plt.xlabel('Edad')
plt.ylabel('Ingresos')
plt.title('Segmentaci√≥n de Clientes')
plt.colorbar(scatter)

plt.subplot(1, 2, 2)
scatter = plt.scatter(df_ecommerce['gasto_electronica'], df_ecommerce['frecuencia_online'], c=segmentos, cmap='viridis', alpha=0.6)
plt.xlabel('Gasto Electr√≥nica')
plt.ylabel('Frecuencia Online')
plt.title('Segmentaci√≥n de Clientes')
plt.colorbar(scatter)
plt.show()
```

---

## üìö **8.7 Recursos Complementarios**

### **üìñ Lecturas:**
- "Pattern Recognition and Machine Learning" - Christopher Bishop
- "The Elements of Statistical Learning" - Hastie, Tibshirani, Friedman

### **üõ†Ô∏è Librer√≠as:**
```python
import sklearn.cluster          # K-Means, DBSCAN
import sklearn.decomposition   # PCA
import mlxtend                 # Reglas de asociaci√≥n
import umap                    # UMAP
```

### **üéØ Casos de Uso:**
| Industria | Aplicaci√≥n | Algoritmo |
|-----------|------------|-----------|
| Retail | Segmentaci√≥n clientes | K-Means |
| Banca | Detecci√≥n fraudes | DBSCAN |
| Salud | An√°lisis genes | Hierarchical |
| Marketing | Market Basket | Apriori |

---

## üìù **8.8 Glosario**

| T√©rmino | Definici√≥n |
|---------|------------|
| **Clustering** | Agrupaci√≥n de objetos similares |
| **Centroide** | Punto promedio de un cluster |
| **Silhouette** | M√©trica de calidad del clustering |
| **Outlier** | Punto que no pertenece a ning√∫n cluster |
| **PCA** | Reducci√≥n de dimensionalidad lineal |
| **Soporte** | Frecuencia de un itemset |
| **Confianza** | Probabilidad condicional |

---

## ü§ñ **8.9 Agrupaci√≥n y Segmentaci√≥n con IA**

### **üöÄ Tendencias Futuras:**
1. **Clustering con Deep Learning**: Autoencoders para reducci√≥n de dimensionalidad
2. **Clustering Adaptativo**: Actualizaci√≥n en tiempo real
3. **Interpretabilidad**: SHAP para explicar clusters
4. **Clustering Multimodal**: Combinar texto, im√°genes y datos tabulares

### **üéØ Aplicaciones Avanzadas:**
- **Segmentaci√≥n Din√°mica**: Clusters que evolucionan en el tiempo
- **Clustering Federado**: Privacidad en clustering distribuido
- **Clustering √âtico**: Evitar sesgos en segmentaci√≥n

---

## ‚úÖ **Resumen de la Clase 8**

### **üéØ Lo que aprendiste:**
1. **Fundamentos**: Aprendizaje no supervisado
2. **Algoritmos**: K-Means, DBSCAN, Jer√°rquico
3. **Reducci√≥n de Dimensionalidad**: PCA
4. **Reglas de Asociaci√≥n**: Market Basket Analysis
5. **Evaluaci√≥n**: M√©tricas de calidad
6. **Aplicaciones**: Segmentaci√≥n de clientes

### **üí° Consejos Clave:**
- **Empezar simple**: K-Means es un buen punto de partida
- **Visualizar siempre**: Los gr√°ficos revelan patrones
- **Validar resultados**: Usar m√∫ltiples m√©tricas
- **Interpretar en contexto**: Los clusters deben tener sentido de negocio

¬°Ahora puedes descubrir patrones ocultos en tus datos! üéâ

