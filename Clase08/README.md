[Material Diapositivas](https://docs.google.com/presentation/d/14g5eAuGgTasx_D4dJ7w7txt2qOZEY7EqvrTVTMddh74/edit#slide=id.p44)

# ğŸ“š Clase 8: Aprendizaje No Supervisado

---

## ğŸ”„ **Repaso Clase 7: Aprendizaje Supervisado**

### **ğŸ“š Â¿QuÃ© vimos en la Clase 7?**

En la **Clase 7** exploramos el **Aprendizaje Supervisado**, donde tenemos datos con etiquetas conocidas (target variables) para entrenar nuestros modelos.

#### **ğŸ¯ Conceptos Clave de la Clase 7:**

1. **ClasificaciÃ³n**: Predecir categorÃ­as (ej: spam/no spam)
   - **Ãrboles de DecisiÃ³n**: Reglas if-else interpretables
   - **Random Forest**: MÃºltiples Ã¡rboles para mayor precisiÃ³n
   - **MÃ©tricas**: Accuracy, Precision, Recall, F1-Score

2. **RegresiÃ³n**: Predecir valores numÃ©ricos (ej: precio de casa)
   - **RegresiÃ³n Lineal**: RelaciÃ³n lineal entre variables
   - **RegresiÃ³n MÃºltiple**: MÃºltiples variables predictoras
   - **MÃ©tricas**: MSE, RMSE, RÂ²

#### **ğŸ”— ConexiÃ³n con Clase 8:**

```python
# REPASO: En Clase 7 tenÃ­amos datos ETIQUETADOS
import pandas as pd
import numpy as np

# Ejemplo de Clase 7: ClasificaciÃ³n de clientes
datos_clase7 = {
    'edad': [25, 35, 45, 55, 65],
    'ingresos': [30000, 50000, 70000, 60000, 40000],
    'gasto_mensual': [800, 1500, 2500, 1800, 900],
    'cliente_valioso': ['No', 'SÃ­', 'SÃ­', 'SÃ­', 'No']  # â† ETIQUETA CONOCIDA
}

df_clase7 = pd.DataFrame(datos_clase7)
print("=== CLASE 7: DATOS CON ETIQUETAS ===")
print("Sabemos quÃ© clientes son valiosos...")
print(df_clase7)
print("\nObjetivo: Predecir si un NUEVO cliente serÃ¡ valioso")

# En Clase 8: Â¿QuÃ© pasa si NO tenemos etiquetas?
print("\n=== CLASE 8: DATOS SIN ETIQUETAS ===")
datos_clase8 = df_clase7.drop('cliente_valioso', axis=1)
print("No sabemos quÃ© tipos de clientes tenemos...")
print("Objetivo: DESCUBRIR grupos naturales en los datos")
print(datos_clase8)
```

#### **ğŸ”„ TransiciÃ³n: De Supervisado a No Supervisado**

| Aspecto | Clase 7 (Supervisado) | Clase 8 (No Supervisado) |
|---------|----------------------|-------------------------|
| **Datos** | Con etiquetas conocidas | Sin etiquetas |
| **Objetivo** | Predecir/Clasificar | Descubrir patrones |
| **EvaluaciÃ³n** | MÃ©tricas claras (accuracy, etc.) | MÃ©tricas internas (silhouette) |
| **InterpretaciÃ³n** | Modelo predictivo | ExploraciÃ³n de datos |

---

## ğŸ“‹ **8.1 IntroducciÃ³n al Aprendizaje No Supervisado**

### **ğŸ¯ Â¿QuÃ© es el Aprendizaje No Supervisado?**

El **Aprendizaje No Supervisado** es una rama del Machine Learning que encuentra patrones ocultos en datos **sin etiquetas** (target variables). A diferencia del aprendizaje supervisado, no tenemos la "respuesta correcta" para entrenar el modelo.

#### **ğŸ” CaracterÃ­sticas Principales:**

1. **Sin etiquetas**: Los datos no tienen variables objetivo conocidas
2. **Descubrimiento de patrones**: El objetivo es encontrar estructuras ocultas
3. **Exploratorio**: Se usa para entender mejor los datos
4. **Flexible**: No hay restricciones sobre quÃ© patrones buscar

#### **ğŸ“Š Tipos de Aprendizaje No Supervisado:**

| Tipo | DescripciÃ³n | Ejemplos |
|------|-------------|----------|
| **Clustering** | Agrupa datos similares | SegmentaciÃ³n de clientes, anÃ¡lisis de genes |
| **ReducciÃ³n de Dimensionalidad** | Reduce variables manteniendo informaciÃ³n | PCA, t-SNE, UMAP |
| **Reglas de AsociaciÃ³n** | Encuentra relaciones entre elementos | Market basket analysis |
| **DetecciÃ³n de AnomalÃ­as** | Identifica patrones inusuales | DetecciÃ³n de fraudes |

#### **ğŸ¯ Casos de Uso Reales:**

```python
# Ejemplo conceptual: Â¿Por quÃ© usar aprendizaje no supervisado?
import pandas as pd
import numpy as np

# Simulamos datos de clientes sin etiquetas
np.random.seed(42)
n_clientes = 1000

datos_clientes = {
    'edad': np.random.normal(35, 10, n_clientes),
    'ingresos_anuales': np.random.lognormal(10, 0.5, n_clientes),
    'gasto_mensual': np.random.gamma(2, 500, n_clientes),
    'frecuencia_compras': np.random.poisson(8, n_clientes),
    'satisfaccion': np.random.choice([1,2,3,4,5], n_clientes, p=[0.1,0.2,0.3,0.3,0.1])
}

df_clientes = pd.DataFrame(datos_clientes)

print("=== EJEMPLO DE DATOS SIN ETIQUETAS ===")
print("No sabemos quÃ© tipos de clientes tenemos...")
print("Â¿CÃ³mo los agrupamos para estrategias de marketing?")
print(f"\nDatos de muestra:")
print(df_clientes.head())
print(f"\nEstadÃ­sticas descriptivas:")
print(df_clientes.describe())
```

#### **âš–ï¸ Ventajas vs Desventajas:**

**âœ… Ventajas:**
- No necesita datos etiquetados (mÃ¡s barato)
- Descubre patrones inesperados
- Ãštil para exploraciÃ³n de datos
- Reduce dimensionalidad

**âŒ Desventajas:**
- MÃ¡s difÃ­cil de evaluar
- Resultados subjetivos
- Requiere interpretaciÃ³n humana
- Menos predictivo que supervisado

---

## ğŸ§  **8.2 Algoritmos de Clustering**

### **ğŸ¯ Â¿QuÃ© es el Clustering?**

El **clustering** es una tÃ©cnica clave en el aprendizaje no supervisado que se utiliza para agrupar un conjunto de datos no etiquetados en grupos o clÃºsteres de datos similares. A travÃ©s del clustering, los datos que comparten caracterÃ­sticas similares se agrupan en el mismo clÃºster, mientras que los datos que son diferentes se separan en clÃºsteres distintos.

#### **ğŸ” Â¿CÃ³mo Funciona?**

El proceso de clustering implica el uso de algoritmos que identifican similitudes y diferencias en los datos para formar estos grupos. Estos algoritmos no requieren informaciÃ³n previa sobre las categorÃ­as o etiquetas de los datos, lo que les permite operar en conjuntos de datos no etiquetados.

#### **ğŸ¯ Objetivos del Clustering:**
- **Agrupar**: Encontrar grupos naturales en los datos
- **Descubrir**: Identificar patrones ocultos
- **Simplificar**: Reducir complejidad de grandes datasets
- **Segmentar**: Crear categorÃ­as para estrategias de negocio

#### **ğŸ“Š PropÃ³sito en el AnÃ¡lisis de Datos No Etiquetados**

El objetivo principal del clustering es descubrir la estructura subyacente de un conjunto de datos no etiquetados. Al identificar y agrupar observaciones similares, el clustering permite a los analistas entender mejor las relaciones en los datos y extraer insights valiosos.

**Ejemplos de AplicaciÃ³n:**
- **Marketing**: Segmentar clientes en grupos con comportamientos similares
- **BiologÃ­a**: Descubrir nuevas especies al agrupar organismos con caracterÃ­sticas similares
- **Medicina**: Clasificar tipos de cÃ©lulas o tejidos
- **Finanzas**: Detectar patrones de fraude o riesgo crediticio

---

## ğŸ”§ **8.2.1 Algoritmos Populares de Clustering**

El clustering es una tÃ©cnica de aprendizaje no supervisado que agrupa datos similares en clÃºsteres. Entre los algoritmos mÃ¡s utilizados en clustering se encuentran **K-means**, **DBSCAN** y el **clustering jerÃ¡rquico**, cada uno con caracterÃ­sticas y aplicaciones particulares.

### **1. ğŸ”¹ K-Means**

**K-means** es uno de los algoritmos de clustering mÃ¡s populares debido a su simplicidad y eficacia. Su objetivo es dividir un conjunto de datos en K clÃºsteres predefinidos, donde cada dato pertenece al clÃºster con el centroide mÃ¡s cercano.

#### **ğŸ¯ Proceso del Algoritmo K-Means:**

1. **InicializaciÃ³n**: Se seleccionan aleatoriamente K centroides (puntos de referencia) en el espacio de los datos
2. **AsignaciÃ³n**: Cada punto de datos se asigna al clÃºster cuyo centroide estÃ© mÃ¡s cercano, minimizando la distancia euclidiana
3. **ActualizaciÃ³n**: Se recalculan los centroides de los clÃºsteres basÃ¡ndose en los datos asignados
4. **IteraciÃ³n**: Los pasos de asignaciÃ³n y actualizaciÃ³n se repiten hasta que los centroides ya no cambian significativamente

#### **âœ… Ventajas:**
- Simple y rÃ¡pido
- Escalable a grandes datasets
- Funciona bien con clusters esfÃ©ricos

#### **âŒ Desventajas:**
- Requiere especificar el nÃºmero K de clusters
- Sensible a outliers
- Asume clusters de forma esfÃ©rica
- Sensible a la inicializaciÃ³n aleatoria

### **2. ğŸ”¹ DBSCAN (Density-Based Spatial Clustering)**

**DBSCAN** es un algoritmo basado en la densidad que agrupa puntos que estÃ¡n densamente conectados, separÃ¡ndolos de los puntos menos densos, considerados como ruido.

#### **ğŸ¯ Conceptos Clave de DBSCAN:**

- **Vecindad Îµ (epsilon)**: Un parÃ¡metro que define un radio alrededor de un punto
- **MinPts**: NÃºmero mÃ­nimo de puntos dentro de la vecindad para que un punto sea considerado un punto central
- **ClÃºsteres**: Se forman conectando puntos densamente conectados
- **Ruido**: Los puntos aislados se consideran ruido

#### **âœ… Ventajas:**
- No requiere especificar el nÃºmero de clusters
- Detecta clusters de forma arbitraria
- Maneja outliers automÃ¡ticamente
- Robusto contra ruido

#### **âŒ Desventajas:**
- Sensible a los parÃ¡metros Îµ y MinPts
- DifÃ­cil de ajustar para datos con densidades variables
- No funciona bien con clusters de densidad muy diferente

### **3. ğŸ”¹ Clustering JerÃ¡rquico**

El **clustering jerÃ¡rquico** es un enfoque que construye una jerarquÃ­a de clÃºsteres. Existen dos mÃ©todos principales:

#### **ğŸŒ³ Tipos de Clustering JerÃ¡rquico:**

1. **Aglomerativo (bottom-up)**: 
   - Comienza tratando cada punto de datos como un clÃºster individual
   - Fusiona los clÃºsteres mÃ¡s cercanos hasta que todos los puntos formen un solo clÃºster

2. **Divisivo (top-down)**:
   - Comienza con un solo clÃºster que contiene todos los puntos de datos
   - Lo divide en clÃºsteres mÃ¡s pequeÃ±os

#### **ğŸ“Š Dendrograma:**
El resultado del clustering jerÃ¡rquico se visualiza comÃºnmente con un **dendrograma**, un Ã¡rbol que muestra la estructura de fusiÃ³n o divisiÃ³n.

#### **âœ… Ventajas:**
- No necesita predefinir el nÃºmero de clusters
- Proporciona una estructura completa de clusters
- Interpretable visualmente

#### **âŒ Desventajas:**
- Computacionalmente costoso para grandes datasets
- Sensible a outliers
- DifÃ­cil de escalar

---

## ğŸ“ **8.2.2 Ejemplos para Clase**

### **ğŸ“š Ejemplo 1: Clustering Simple con Datos de Estudiantes**

Vamos a empezar con un ejemplo simple y didÃ¡ctico que los alumnos pueden entender fÃ¡cilmente.

```python
# EJEMPLO 1: Clustering de Estudiantes por Rendimiento
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Crear datos de estudiantes (simulados)
np.random.seed(42)
n_estudiantes = 100

# Generar datos de diferentes tipos de estudiantes
# Tipo 1: Estudiantes aplicados (altas notas, muchas horas de estudio)
aplicados = {
    'horas_estudio': np.random.normal(8, 1, 30),
    'nota_promedio': np.random.normal(8.5, 0.5, 30),
    'asistencia': np.random.normal(95, 3, 30)
}

# Tipo 2: Estudiantes promedio (notas y estudio moderados)
promedio = {
    'horas_estudio': np.random.normal(5, 1, 40),
    'nota_promedio': np.random.normal(6.5, 0.8, 40),
    'asistencia': np.random.normal(80, 5, 40)
}

# Tipo 3: Estudiantes con dificultades (bajas notas, pocas horas)
dificultades = {
    'horas_estudio': np.random.normal(2, 0.5, 30),
    'nota_promedio': np.random.normal(4, 0.7, 30),
    'asistencia': np.random.normal(60, 10, 30)
}

# Combinar todos los datos
datos_estudiantes = {
    'horas_estudio': np.concatenate([aplicados['horas_estudio'], promedio['horas_estudio'], dificultades['horas_estudio']]),
    'nota_promedio': np.concatenate([aplicados['nota_promedio'], promedio['nota_promedio'], dificultades['nota_promedio']]),
    'asistencia': np.concatenate([aplicados['asistencia'], promedio['asistencia'], dificultades['asistencia']])
}

df_estudiantes = pd.DataFrame(datos_estudiantes)

print("=== EJEMPLO 1: DATOS DE ESTUDIANTES ===")
print("Objetivo: Descubrir tipos de estudiantes sin etiquetas previas")
print(f"\nDataset shape: {df_estudiantes.shape}")
print(f"\nPrimeras 5 filas:")
print(df_estudiantes.head())
print(f"\nEstadÃ­sticas descriptivas:")
print(df_estudiantes.describe())

# VisualizaciÃ³n inicial
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.scatter(df_estudiantes['horas_estudio'], df_estudiantes['nota_promedio'], alpha=0.6)
plt.xlabel('Horas de Estudio')
plt.ylabel('Nota Promedio')
plt.title('Datos Originales: Horas vs Notas')

plt.subplot(1, 3, 2)
plt.scatter(df_estudiantes['horas_estudio'], df_estudiantes['asistencia'], alpha=0.6)
plt.xlabel('Horas de Estudio')
plt.ylabel('Asistencia (%)')
plt.title('Datos Originales: Horas vs Asistencia')

plt.subplot(1, 3, 3)
plt.scatter(df_estudiantes['nota_promedio'], df_estudiantes['asistencia'], alpha=0.6)
plt.xlabel('Nota Promedio')
plt.ylabel('Asistencia (%)')
plt.title('Datos Originales: Notas vs Asistencia')

plt.tight_layout()
plt.show()

# PASO 1: Preparar datos para clustering
print("\n=== PASO 1: PREPARACIÃ“N DE DATOS ===")
print("Â¿Por quÃ© normalizar? Las variables tienen escalas diferentes:")
print(f"Horas de estudio: {df_estudiantes['horas_estudio'].min():.1f} - {df_estudiantes['horas_estudio'].max():.1f}")
print(f"Notas: {df_estudiantes['nota_promedio'].min():.1f} - {df_estudiantes['nota_promedio'].max():.1f}")
print(f"Asistencia: {df_estudiantes['asistencia'].min():.1f} - {df_estudiantes['asistencia'].max():.1f}")

# Normalizar datos
scaler = StandardScaler()
X_estudiantes = scaler.fit_transform(df_estudiantes)
print(f"\nDatos normalizados shape: {X_estudiantes.shape}")

# PASO 2: MÃ©todo del Codo para encontrar K Ã³ptimo
print("\n=== PASO 2: ENCONTRAR NÃšMERO Ã“PTIMO DE CLUSTERS ===")

def metodo_codo(X, max_k=8):
    """MÃ©todo del codo para encontrar k Ã³ptimo"""
    inertias = []
    k_range = range(1, max_k + 1)
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X)
        inertias.append(kmeans.inertia_)
    
    return k_range, inertias

k_range, inertias = metodo_codo(X_estudiantes)

plt.figure(figsize=(8, 5))
plt.plot(k_range, inertias, 'bo-')
plt.xlabel('NÃºmero de Clusters (k)')
plt.ylabel('Inercia')
plt.title('MÃ©todo del Codo - Estudiantes')
plt.grid(True)
plt.axvline(x=3, color='red', linestyle='--', label='K=3 (Ã³ptimo)')
plt.legend()
plt.show()

print("InterpretaciÃ³n: El 'codo' estÃ¡ en K=3, donde la inercia deja de disminuir rÃ¡pidamente")

# PASO 3: Aplicar K-Means con K=3
print("\n=== PASO 3: APLICAR K-MEANS ===")
kmeans_estudiantes = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters_estudiantes = kmeans_estudiantes.fit_predict(X_estudiantes)

# Agregar clusters al dataframe
df_estudiantes['cluster'] = clusters_estudiantes

# PASO 4: Visualizar resultados
print("\n=== PASO 4: VISUALIZAR RESULTADOS ===")
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
scatter = plt.scatter(df_estudiantes['horas_estudio'], df_estudiantes['nota_promedio'], 
                     c=df_estudiantes['cluster'], cmap='viridis', alpha=0.7)
plt.xlabel('Horas de Estudio')
plt.ylabel('Nota Promedio')
plt.title('K-Means: Horas vs Notas')
plt.colorbar(scatter, label='Cluster')

plt.subplot(1, 3, 2)
scatter = plt.scatter(df_estudiantes['horas_estudio'], df_estudiantes['asistencia'], 
                     c=df_estudiantes['cluster'], cmap='viridis', alpha=0.7)
plt.xlabel('Horas de Estudio')
plt.ylabel('Asistencia (%)')
plt.title('K-Means: Horas vs Asistencia')
plt.colorbar(scatter, label='Cluster')

plt.subplot(1, 3, 3)
scatter = plt.scatter(df_estudiantes['nota_promedio'], df_estudiantes['asistencia'], 
                     c=df_estudiantes['cluster'], cmap='viridis', alpha=0.7)
plt.xlabel('Nota Promedio')
plt.ylabel('Asistencia (%)')
plt.title('K-Means: Notas vs Asistencia')
plt.colorbar(scatter, label='Cluster')

plt.tight_layout()
plt.show()

# PASO 5: Interpretar clusters
print("\n=== PASO 5: INTERPRETAR CLUSTERS ===")
for i in range(3):
    cluster_data = df_estudiantes[df_estudiantes['cluster'] == i]
    print(f"\nğŸ”¹ Cluster {i} ({len(cluster_data)} estudiantes):")
    print(f"   Horas de estudio promedio: {cluster_data['horas_estudio'].mean():.1f}")
    print(f"   Nota promedio: {cluster_data['nota_promedio'].mean():.1f}")
    print(f"   Asistencia promedio: {cluster_data['asistencia'].mean():.1f}%")
    
    # InterpretaciÃ³n
    if cluster_data['horas_estudio'].mean() > 6:
        print("   ğŸ“š InterpretaciÃ³n: ESTUDIANTES APLICADOS")
    elif cluster_data['horas_estudio'].mean() > 3:
        print("   ğŸ“– InterpretaciÃ³n: ESTUDIANTES PROMEDIO")
    else:
        print("   ğŸ“ InterpretaciÃ³n: ESTUDIANTES CON DIFICULTADES")

print("\nâœ… CONCLUSIÃ“N: El algoritmo descubriÃ³ automÃ¡ticamente 3 tipos de estudiantes")
print("   sin necesidad de etiquetas previas!")
```

### **ğŸ“š Ejemplo 2: Clustering de Productos de E-commerce**

Un ejemplo mÃ¡s complejo y realista para mostrar aplicaciones empresariales.

```python
# EJEMPLO 2: SegmentaciÃ³n de Productos de E-commerce
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Crear datos de productos de e-commerce
np.random.seed(42)

# Generar diferentes tipos de productos
# Tipo 1: Productos premium (alto precio, baja rotaciÃ³n, alta satisfacciÃ³n)
premium = {
    'precio': np.random.normal(500, 100, 50),
    'ventas_mensuales': np.random.poisson(20, 50),
    'rating_promedio': np.random.normal(4.8, 0.2, 50),
    'stock_dias': np.random.normal(45, 10, 50)
}

# Tipo 2: Productos populares (precio medio, alta rotaciÃ³n, buen rating)
populares = {
    'precio': np.random.normal(150, 30, 100),
    'ventas_mensuales': np.random.poisson(200, 100),
    'rating_promedio': np.random.normal(4.2, 0.3, 100),
    'stock_dias': np.random.normal(15, 5, 100)
}

# Tipo 3: Productos bÃ¡sicos (bajo precio, rotaciÃ³n media, rating variable)
basicos = {
    'precio': np.random.normal(50, 15, 80),
    'ventas_mensuales': np.random.poisson(80, 80),
    'rating_promedio': np.random.normal(3.5, 0.5, 80),
    'stock_dias': np.random.normal(30, 8, 80)
}

# Combinar datos
datos_productos = {
    'precio': np.concatenate([premium['precio'], populares['precio'], basicos['precio']]),
    'ventas_mensuales': np.concatenate([premium['ventas_mensuales'], populares['ventas_mensuales'], basicos['ventas_mensuales']]),
    'rating_promedio': np.concatenate([premium['rating_promedio'], populares['rating_promedio'], basicos['rating_promedio']]),
    'stock_dias': np.concatenate([premium['stock_dias'], populares['stock_dias'], basicos['stock_dias']])
}

df_productos = pd.DataFrame(datos_productos)

print("=== EJEMPLO 2: PRODUCTOS DE E-COMMERCE ===")
print("Objetivo: Segmentar productos para estrategias de marketing y pricing")
print(f"\nDataset shape: {df_productos.shape}")
print(f"\nEstadÃ­sticas descriptivas:")
print(df_productos.describe())

# VisualizaciÃ³n inicial
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(df_productos['precio'], df_productos['ventas_mensuales'], alpha=0.6)
plt.xlabel('Precio ($)')
plt.ylabel('Ventas Mensuales')
plt.title('Precio vs Ventas')

plt.subplot(1, 3, 2)
plt.scatter(df_productos['precio'], df_productos['rating_promedio'], alpha=0.6)
plt.xlabel('Precio ($)')
plt.ylabel('Rating Promedio')
plt.title('Precio vs Rating')

plt.subplot(1, 3, 3)
plt.scatter(df_productos['ventas_mensuales'], df_productos['rating_promedio'], alpha=0.6)
plt.xlabel('Ventas Mensuales')
plt.ylabel('Rating Promedio')
plt.title('Ventas vs Rating')

plt.tight_layout()
plt.show()

# Normalizar datos
scaler_productos = StandardScaler()
X_productos = scaler_productos.fit_transform(df_productos)

# Aplicar K-Means
kmeans_productos = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters_productos = kmeans_productos.fit_predict(X_productos)

# Agregar clusters
df_productos['cluster'] = clusters_productos

# Visualizar resultados
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
scatter = plt.scatter(df_productos['precio'], df_productos['ventas_mensuales'], 
                     c=df_productos['cluster'], cmap='viridis', alpha=0.7)
plt.xlabel('Precio ($)')
plt.ylabel('Ventas Mensuales')
plt.title('K-Means: Precio vs Ventas')
plt.colorbar(scatter, label='Cluster')

plt.subplot(1, 3, 2)
scatter = plt.scatter(df_productos['precio'], df_productos['rating_promedio'], 
                     c=df_productos['cluster'], cmap='viridis', alpha=0.7)
plt.xlabel('Precio ($)')
plt.ylabel('Rating Promedio')
plt.title('K-Means: Precio vs Rating')
plt.colorbar(scatter, label='Cluster')

plt.subplot(1, 3, 3)
scatter = plt.scatter(df_productos['ventas_mensuales'], df_productos['rating_promedio'], 
                     c=df_productos['cluster'], cmap='viridis', alpha=0.7)
plt.xlabel('Ventas Mensuales')
plt.ylabel('Rating Promedio')
plt.title('K-Means: Ventas vs Rating')
plt.colorbar(scatter, label='Cluster')

plt.tight_layout()
plt.show()

# AnÃ¡lisis de clusters
print("\n=== ANÃLISIS DE SEGMENTOS DE PRODUCTOS ===")
for i in range(3):
    cluster_data = df_productos[df_productos['cluster'] == i]
    print(f"\nğŸ”¹ Segmento {i} ({len(cluster_data)} productos):")
    print(f"   Precio promedio: ${cluster_data['precio'].mean():.0f}")
    print(f"   Ventas promedio: {cluster_data['ventas_mensuales'].mean():.0f} unidades/mes")
    print(f"   Rating promedio: {cluster_data['rating_promedio'].mean():.1f}/5")
    print(f"   Stock promedio: {cluster_data['stock_dias'].mean():.0f} dÃ­as")
    
    # Estrategia de negocio
    if cluster_data['precio'].mean() > 300:
        print("   ğŸ’ Estrategia: PRODUCTOS PREMIUM - Marketing exclusivo, alta calidad")
    elif cluster_data['ventas_mensuales'].mean() > 150:
        print("   ğŸš€ Estrategia: PRODUCTOS POPULARES - Promociones, stock alto")
    else:
        print("   ğŸ“¦ Estrategia: PRODUCTOS BÃSICOS - Precios competitivos, rotaciÃ³n media")

# Calcular mÃ©tricas de calidad
silhouette_avg = silhouette_score(X_productos, clusters_productos)
print(f"\nğŸ“Š MÃ©trica de Calidad:")
print(f"   Silhouette Score: {silhouette_avg:.3f} (0.5+ es bueno)")

print("\nâœ… APLICACIÃ“N PRÃCTICA:")
print("   - Segmento 0: Estrategia de pricing premium")
print("   - Segmento 1: CampaÃ±as de marketing masivo")
print("   - Segmento 2: OptimizaciÃ³n de inventario")
```

---

## ğŸ“‹ **8.2.4 Resumen de Algoritmos de Clustering**

### **ğŸ¯ ComparaciÃ³n de Algoritmos Populares**

| Algoritmo | Mejor Para | Ventajas | Desventajas | CuÃ¡ndo Usar |
|-----------|------------|----------|-------------|-------------|
| **K-Means** | Clusters esfÃ©ricos bien separados | Simple, rÃ¡pido, escalable | Requiere K, sensible a outliers | Datos con clusters claros y esfÃ©ricos |
| **DBSCAN** | Clusters de forma arbitraria | No requiere K, maneja outliers | Sensible a parÃ¡metros | Datos con ruido, formas irregulares |
| **JerÃ¡rquico** | ExploraciÃ³n de estructura | No requiere K, interpretable | Costoso computacionalmente | Datasets pequeÃ±os, exploraciÃ³n |

### **ğŸ” Clustering Basado en Densidad (DBSCAN)**

Los mÃ©todos de clustering basados en densidad identifican grupos en un conjunto de datos considerando la densidad de puntos en el espacio de datos. A diferencia de otros mÃ©todos como K-means, que dependen de la distancia entre puntos y requieren definir el nÃºmero de clÃºsteres, estos mÃ©todos se enfocan en encontrar regiones densamente pobladas separadas por Ã¡reas de baja densidad.

#### **ğŸ¯ Â¿CÃ³mo Funcionan?**

El principio central es que los clÃºsteres se forman en Ã¡reas contiguas de alta densidad, con las regiones de baja densidad actuando como separadores. Un algoritmo destacado es **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise), que funciona asÃ­:

1. **Vecindad Îµ (epsilon)**: Define un radio alrededor de un punto. Si hay suficientes puntos dentro de este radio (MinPts), el Ã¡rea es densa.
2. **Puntos Centrales y Frontera**: Un punto central tiene al menos MinPts en su vecindad Îµ; un punto frontera tiene menos pero estÃ¡ cerca de un punto central.
3. **ExpansiÃ³n de ClÃºsteres**: Un clÃºster crece incluyendo puntos en la vecindad Îµ hasta que no se puedan agregar mÃ¡s.
4. **Ruido**: Los puntos que no pertenecen a una vecindad densa se consideran ruido o outliers.

#### **ğŸ¯ PropÃ³sito y Aplicaciones**

DBSCAN es Ãºtil para detectar clÃºsteres de formas arbitrarias y manejar outliers sin necesidad de especificar el nÃºmero de clÃºsteres. Es aplicado en:
- **AnÃ¡lisis geoespacial**: Agrupar ubicaciones por densidad
- **SegmentaciÃ³n de clientes**: En marketing, donde los datos no tienen formas esfÃ©ricas definidas
- **DetecciÃ³n de anomalÃ­as**: Identificar patrones inusuales

### **ğŸŒ³ Clustering JerÃ¡rquico y Dendrogramas**

Un **dendrograma** es una representaciÃ³n visual que muestra cÃ³mo se agrupan y dividen los clÃºsteres en un proceso de clustering jerÃ¡rquico. Cada bifurcaciÃ³n en el diagrama representa un punto de uniÃ³n entre los datos, ayudando a visualizar las relaciones entre diferentes clÃºsteres y la estructura jerÃ¡rquica de los mismos.

#### **ğŸ¯ InterpretaciÃ³n del Dendrograma:**
- **Altura de las lÃ­neas**: Indica la distancia entre clusters
- **Bifurcaciones**: Muestran cÃ³mo se fusionan los clusters
- **Corte horizontal**: Determina el nÃºmero final de clusters

### **âœ… Resumen Final**

En resumen, el clustering es una herramienta poderosa en el anÃ¡lisis de datos no etiquetados, proporcionando un mÃ©todo para organizar y explorar grandes volÃºmenes de datos al identificar patrones y estructuras ocultas, sin necesidad de etiquetas o categorÃ­as predefinidas.

**ğŸ¯ Consejos para Elegir el Algoritmo Correcto:**
1. **K-Means**: Ideal para clusters bien definidos, rÃ¡pido y sencillo, pero sensible a los outliers y requiere que se especifique K.
2. **DBSCAN**: Eficaz para clusters de formas arbitrarias y robusto contra outliers, no requiere especificar el nÃºmero de clusters, pero depende de la correcta elecciÃ³n de parÃ¡metros.
3. **Clustering JerÃ¡rquico**: Ofrece una estructura completa de clusters y no necesita predefinir el nÃºmero de clusters, aunque puede ser costoso en tÃ©rminos de tiempo de cÃ¡lculo para grandes datasets.

Estos algoritmos son fundamentales en el anÃ¡lisis de datos no etiquetados, proporcionando diferentes enfoques para descubrir patrones ocultos y estructurar la informaciÃ³n de manera significativa.

---

## ğŸ“Š **8.2.3 Ejemplo PrÃ¡ctico Avanzado: SegmentaciÃ³n de Clientes**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Crear dataset realista de clientes
np.random.seed(42)
n_clientes = 500

# Simular diferentes tipos de clientes
# Tipo 1: JÃ³venes con ingresos bajos, gastos moderados
jovenes = {
    'edad': np.random.normal(25, 3, 150),
    'ingresos': np.random.normal(30000, 5000, 150),
    'gasto_mensual': np.random.normal(800, 200, 150),
    'frecuencia': np.random.poisson(6, 150)
}

# Tipo 2: Adultos medios con ingresos altos, gastos altos
adultos_ricos = {
    'edad': np.random.normal(45, 5, 200),
    'ingresos': np.random.normal(80000, 10000, 200),
    'gasto_mensual': np.random.normal(2500, 500, 200),
    'frecuencia': np.random.poisson(12, 200)
}

# Tipo 3: Adultos mayores con ingresos medios, gastos bajos
mayores = {
    'edad': np.random.normal(65, 5, 150),
    'ingresos': np.random.normal(50000, 8000, 150),
    'gasto_mensual': np.random.normal(600, 150, 150),
    'frecuencia': np.random.poisson(3, 150)
}

# Combinar todos los datos
data = {
    'edad': np.concatenate([jovenes['edad'], adultos_ricos['edad'], mayores['edad']]),
    'ingresos': np.concatenate([jovenes['ingresos'], adultos_ricos['ingresos'], mayores['ingresos']]),
    'gasto_mensual': np.concatenate([jovenes['gasto_mensual'], adultos_ricos['gasto_mensual'], mayores['gasto_mensual']]),
    'frecuencia': np.concatenate([jovenes['frecuencia'], adultos_ricos['frecuencia'], mayores['frecuencia']])
}

df = pd.DataFrame(data)

# Agregar ruido para hacer mÃ¡s realista
df['edad'] += np.random.normal(0, 2, len(df))
df['ingresos'] += np.random.normal(0, 2000, len(df))
df['gasto_mesual'] += np.random.normal(0, 100, len(df))

print("=== DATASET DE CLIENTES ===")
print(f"Forma del dataset: {df.shape}")
print(f"\nEstadÃ­sticas descriptivas:")
print(df.describe())

# VisualizaciÃ³n inicial
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(df['edad'], df['gasto_mensual'], alpha=0.6)
plt.xlabel('Edad')
plt.ylabel('Gasto Mensual')
plt.title('Edad vs Gasto Mensual')

plt.subplot(1, 3, 2)
plt.scatter(df['ingresos'], df['gasto_mensual'], alpha=0.6)
plt.xlabel('Ingresos')
plt.ylabel('Gasto Mensual')
plt.title('Ingresos vs Gasto Mensual')

plt.subplot(1, 3, 3)
plt.scatter(df['frecuencia'], df['gasto_mensual'], alpha=0.6)
plt.xlabel('Frecuencia de Compras')
plt.ylabel('Gasto Mensual')
plt.title('Frecuencia vs Gasto Mensual')

plt.tight_layout()
plt.show()
```

---

## ğŸ§© **8.2.1 Principales enfoques de Clustering y sus caracterÃ­sticas**

### 1. ğŸ”¹ **Clustering por Particiones (K-Means)**

Divide los datos en *k* grupos predefinidos maximizando la similitud dentro de los clÃºsteres y minimizando la similitud entre ellos.

#### **ğŸ¯ K-Means en AcciÃ³n:**

```python
# Preparar datos para clustering
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[['edad', 'ingresos', 'gasto_mensual', 'frecuencia']])

# MÃ©todo del Codo para encontrar k Ã³ptimo
def metodo_codo(X, max_k=10):
    """Encuentra el nÃºmero Ã³ptimo de clusters usando el mÃ©todo del codo"""
    inertias = []
    k_range = range(1, max_k + 1)
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X)
        inertias.append(kmeans.inertia_)
    
    return k_range, inertias

# Aplicar mÃ©todo del codo
k_range, inertias = metodo_codo(X_scaled)

# Visualizar mÃ©todo del codo
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(k_range, inertias, 'bo-')
plt.xlabel('NÃºmero de Clusters (k)')
plt.ylabel('Inercia')
plt.title('MÃ©todo del Codo')
plt.grid(True)

# MÃ©todo de Silhouette
def metodo_silhouette(X, max_k=10):
    """Encuentra el nÃºmero Ã³ptimo de clusters usando silhouette"""
    silhouette_scores = []
    k_range = range(2, max_k + 1)
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X)
        silhouette_avg = silhouette_score(X, cluster_labels)
        silhouette_scores.append(silhouette_avg)
    
    return k_range, silhouette_scores

k_range_sil, silhouette_scores = metodo_silhouette(X_scaled)

plt.subplot(1, 2, 2)
plt.plot(k_range_sil, silhouette_scores, 'ro-')
plt.xlabel('NÃºmero de Clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('MÃ©todo de Silhouette')
plt.grid(True)

plt.tight_layout()
plt.show()

# Aplicar K-Means con k=3 (basado en nuestro conocimiento del dataset)
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X_scaled)

# Agregar clusters al dataframe
df['cluster'] = cluster_labels

# Visualizar resultados
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
scatter = plt.scatter(df['edad'], df['gasto_mensual'], c=df['cluster'], cmap='viridis', alpha=0.6)
plt.xlabel('Edad')
plt.ylabel('Gasto Mensual')
plt.title('K-Means: Edad vs Gasto')
plt.colorbar(scatter)

plt.subplot(1, 3, 2)
scatter = plt.scatter(df['ingresos'], df['gasto_mensual'], c=df['cluster'], cmap='viridis', alpha=0.6)
plt.xlabel('Ingresos')
plt.ylabel('Gasto Mensual')
plt.title('K-Means: Ingresos vs Gasto')
plt.colorbar(scatter)

plt.subplot(1, 3, 3)
scatter = plt.scatter(df['frecuencia'], df['gasto_mensual'], c=df['cluster'], cmap='viridis', alpha=0.6)
plt.xlabel('Frecuencia')
plt.ylabel('Gasto Mensual')
plt.title('K-Means: Frecuencia vs Gasto')
plt.colorbar(scatter)

plt.tight_layout()
plt.show()

# AnÃ¡lisis de clusters
print("\n=== ANÃLISIS DE CLUSTERS K-MEANS ===")
for i in range(3):
    cluster_data = df[df['cluster'] == i]
    print(f"\nCluster {i}:")
    print(f"  TamaÃ±o: {len(cluster_data)} clientes ({len(cluster_data)/len(df)*100:.1f}%)")
    print(f"  Edad promedio: {cluster_data['edad'].mean():.1f}")
    print(f"  Ingresos promedio: ${cluster_data['ingresos'].mean():,.0f}")
    print(f"  Gasto promedio: ${cluster_data['gasto_mensual'].mean():,.0f}")
    print(f"  Frecuencia promedio: {cluster_data['frecuencia'].mean():.1f}")

# Calcular silhouette score
silhouette_avg = silhouette_score(X_scaled, cluster_labels)
print(f"\nSilhouette Score: {silhouette_avg:.3f}")
```

**ğŸ¯ InterpretaciÃ³n de Resultados:**
- **Cluster 0**: JÃ³venes con ingresos bajos
- **Cluster 1**: Adultos mayores con gastos conservadores  
- **Cluster 2**: Adultos medios con altos ingresos y gastos

ğŸ‘‰ *Ventajas:* Simple, rÃ¡pido, escalable
ğŸ‘‰ *Desventajas:* Requiere definir k, sensible a outliers, asume clusters esfÃ©ricos

---

### 2. ğŸ”¹ **Clustering JerÃ¡rquico**

Crea una estructura tipo Ã¡rbol (dendrograma) que refleja cÃ³mo se agrupan los datos paso a paso.

#### **ğŸŒ³ Clustering JerÃ¡rquico en AcciÃ³n:**

```python
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist

# Aplicar clustering jerÃ¡rquico
hierarchical = AgglomerativeClustering(n_clusters=3)
hierarchical_labels = hierarchical.fit_predict(X_scaled)

# Crear dendrograma
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
linkage_matrix = linkage(X_scaled, method='ward')
dendrogram(linkage_matrix, truncate_mode='level', p=5)
plt.title('Dendrograma - Clustering JerÃ¡rquico')
plt.xlabel('Ãndice de Muestra')
plt.ylabel('Distancia')

# Visualizar clusters jerÃ¡rquicos
plt.subplot(1, 3, 2)
scatter = plt.scatter(df['edad'], df['gasto_mensual'], c=hierarchical_labels, cmap='viridis', alpha=0.6)
plt.xlabel('Edad')
plt.ylabel('Gasto Mensual')
plt.title('Clustering JerÃ¡rquico')
plt.colorbar(scatter)

plt.subplot(1, 3, 3)
scatter = plt.scatter(df['ingresos'], df['gasto_mensual'], c=hierarchical_labels, cmap='viridis', alpha=0.6)
plt.xlabel('Ingresos')
plt.ylabel('Gasto Mensual')
plt.title('Clustering JerÃ¡rquico')
plt.colorbar(scatter)

plt.tight_layout()
plt.show()

print("\n=== COMPARACIÃ“N K-MEANS vs JERÃRQUICO ===")
print(f"K-Means Silhouette: {silhouette_score(X_scaled, cluster_labels):.3f}")
print(f"JerÃ¡rquico Silhouette: {silhouette_score(X_scaled, hierarchical_labels):.3f}")
```

**ğŸ¯ Tipos de Clustering JerÃ¡rquico:**
- **Aglomerativo**: Funde clÃºsteres de abajo hacia arriba
- **Divisivo**: Divide de arriba hacia abajo

ğŸ‘‰ *Ventajas:* No necesita predefinir k, interpretable
ğŸ‘‰ *Desventajas:* Costoso computacionalmente, sensible a outliers

---

### 3. ğŸ”¹ **Clustering por Densidad (DBSCAN)**

Agrupa puntos que estÃ¡n densamente conectados entre sÃ­, detectando automÃ¡ticamente outliers.

#### **ğŸ¯ DBSCAN en AcciÃ³n:**

```python
from sklearn.neighbors import NearestNeighbors

# FunciÃ³n para encontrar eps Ã³ptimo usando k-distance graph
def encontrar_eps_optimo(X, k=4):
    """Encuentra el eps Ã³ptimo usando el grÃ¡fico de k-distancia"""
    neighbors = NearestNeighbors(n_neighbors=k)
    neighbors_fit = neighbors.fit(X)
    distances, indices = neighbors_fit.kneighbors(X)
    distances = np.sort(distances[:, k-1], axis=0)
    return distances

# Encontrar eps Ã³ptimo
distances = encontrar_eps_optimo(X_scaled)
eps_optimo = distances[int(len(distances) * 0.1)]  # Usar percentil 10

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(distances)
plt.axhline(y=eps_optimo, color='r', linestyle='--', label=f'eps Ã³ptimo: {eps_optimo:.2f}')
plt.xlabel('Puntos ordenados por distancia')
plt.ylabel('Distancia al k-Ã©simo vecino')
plt.title('MÃ©todo de k-distancia para encontrar eps')
plt.legend()
plt.grid(True)

# Aplicar DBSCAN
dbscan = DBSCAN(eps=eps_optimo, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_scaled)

# Visualizar resultados DBSCAN
plt.subplot(1, 2, 2)
scatter = plt.scatter(df['edad'], df['gasto_mensual'], c=dbscan_labels, cmap='viridis', alpha=0.6)
plt.xlabel('Edad')
plt.ylabel('Gasto Mensual')
plt.title(f'DBSCAN (eps={eps_optimo:.2f})')
plt.colorbar(scatter)

plt.tight_layout()
plt.show()

# AnÃ¡lisis de resultados DBSCAN
n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
n_noise = list(dbscan_labels).count(-1)

print(f"\n=== RESULTADOS DBSCAN ===")
print(f"NÃºmero de clusters encontrados: {n_clusters}")
print(f"NÃºmero de puntos de ruido: {n_noise}")
print(f"Porcentaje de ruido: {n_noise/len(dbscan_labels)*100:.1f}%")

# Comparar todos los mÃ©todos
plt.figure(figsize=(15, 5))

algoritmos = [
    ('K-Means', cluster_labels),
    ('JerÃ¡rquico', hierarchical_labels),
    ('DBSCAN', dbscan_labels)
]

for i, (nombre, labels) in enumerate(algoritmos):
    plt.subplot(1, 3, i+1)
    scatter = plt.scatter(df['edad'], df['gasto_mensual'], c=labels, cmap='viridis', alpha=0.6)
    plt.xlabel('Edad')
    plt.ylabel('Gasto Mensual')
    plt.title(f'{nombre}')
    plt.colorbar(scatter)

plt.tight_layout()
plt.show()

# Comparar mÃ©tricas
print(f"\n=== COMPARACIÃ“N DE ALGORITMOS ===")
for nombre, labels in algoritmos:
    if len(set(labels)) > 1:  # Solo si hay mÃ¡s de un cluster
        score = silhouette_score(X_scaled, labels)
        print(f"{nombre}: Silhouette Score = {score:.3f}")
    else:
        print(f"{nombre}: No se puede calcular silhouette (solo un cluster)")
```

**ğŸ¯ CaracterÃ­sticas de DBSCAN:**
- **eps**: Distancia mÃ¡xima entre puntos para ser considerados vecinos
- **min_samples**: NÃºmero mÃ­nimo de puntos para formar un cluster
- **Outliers**: Puntos marcados como -1 (ruido)

ğŸ‘‰ *Ventajas:* Detecta clusters de forma arbitraria, maneja ruido, no necesita k
ğŸ‘‰ *Desventajas:* Sensible a parÃ¡metros, difÃ­cil con clusters de densidad variable

---

### 4. ğŸ”¹ **Clustering Basado en Grid**
Divide el espacio en celdas y agrupa segÃºn densidad local en cada celda.
- **Wavecluster**: TransformaciÃ³n de onda
- **STING**: Grillas estadÃ­sticas jerÃ¡rquicas
- **CLIQUE**: Alta dimensiÃ³n
ğŸ‘‰ *Ventajas:* Eficiente en altas dimensiones
ğŸ‘‰ *Desventajas:* Sensible al tamaÃ±o de grilla

---

### 5. ğŸ”¹ **Clustering Basado en Modelos**
Asume que los datos se generan a partir de un modelo estadÃ­stico.
- **GMM**: Distribuciones normales mÃºltiples
- **COBWEB**: Ãrbol jerÃ¡rquico categÃ³rico
- **SOMs**: Red neuronal auto-organizativa
ğŸ‘‰ *Ventajas:* Modelos probabilÃ­sticos flexibles
ğŸ‘‰ *Desventajas:* Requiere supuestos sobre distribuciÃ³n

---

## ğŸ“Š **8.3 ReducciÃ³n de Dimensionalidad**

### **ğŸ¯ Â¿Por quÃ© reducir dimensiones?**
1. **VisualizaciÃ³n**: Solo podemos ver 2-3 dimensiones
2. **Curse of Dimensionality**: MÃ¡s dimensiones = mÃ¡s datos necesarios
3. **Ruido**: Dimensiones irrelevantes confunden algoritmos
4. **ComputaciÃ³n**: Menos dimensiones = mÃ¡s rÃ¡pido

### **ğŸ”§ PCA (Principal Component Analysis):**

```python
from sklearn.decomposition import PCA

# Aplicar PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Visualizar resultados
plt.figure(figsize=(10, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title('PCA - ReducciÃ³n a 2D')
plt.colorbar()
plt.show()

print(f"Varianza explicada: {sum(pca.explained_variance_ratio_):.1%}")
```

---

## ğŸ›’ **8.4 Reglas de AsociaciÃ³n**

### **ğŸ¯ Market Basket Analysis:**

```python
from mlxtend.frequent_patterns import apriori, association_rules

# Ejemplo de transacciones
transacciones = [
    ['Leche', 'Pan', 'Huevos'],
    ['Pan', 'Mantequilla'],
    ['Leche', 'Mantequilla', 'Huevos'],
    ['Pan', 'Leche']
]

# Convertir a formato binario
te = TransactionEncoder()
te_ary = te.fit(transacciones).transform(transacciones)
df_transacciones = pd.DataFrame(te_ary, columns=te.columns_)

# Encontrar itemsets frecuentes
frequent_itemsets = apriori(df_transacciones, min_support=0.5, use_colnames=True)

# Generar reglas
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)
print("Reglas de asociaciÃ³n:")
print(rules[['antecedents', 'consequents', 'support', 'confidence']])
```

**ğŸ¯ MÃ©tricas:**
- **Soporte**: Frecuencia de la regla
- **Confianza**: Probabilidad condicional
- **Lift**: Mejora sobre aleatorio

---

## ğŸ“ˆ **8.5 EvaluaciÃ³n y ComparaciÃ³n**

### **ğŸ¯ MÃ©tricas de EvaluaciÃ³n:**

```python
from sklearn.metrics import silhouette_score, adjusted_rand_score

# MÃ©tricas internas (no necesitan etiquetas verdaderas)
silhouette = silhouette_score(X_scaled, cluster_labels)
print(f"Silhouette Score: {silhouette:.3f}")

# MÃ©tricas externas (necesitan etiquetas verdaderas)
ari = adjusted_rand_score(true_labels, cluster_labels)
print(f"Adjusted Rand Index: {ari:.3f}")

# Comparar algoritmos
algoritmos = [('K-Means', cluster_labels), ('DBSCAN', dbscan_labels)]
for nombre, labels in algoritmos:
    score = silhouette_score(X_scaled, labels)
    print(f"{nombre}: {score:.3f}")
```

**ğŸ¯ InterpretaciÃ³n:**
- **Silhouette**: [-1, 1] - 1 es mejor
- **ARI**: [-1, 1] - 1 es mejor
- **Davies-Bouldin**: [0, âˆ) - Menor es mejor

---

## ğŸ¯ **8.6 Actividad PrÃ¡ctica**

### **ğŸ“‹ Ejercicio: SegmentaciÃ³n de Clientes E-commerce**

```python
# Dataset realista de e-commerce
np.random.seed(42)
n_clientes = 500

# Crear perfiles de clientes
perfiles = {
    'joven_tecnologico': {
        'edad': np.random.normal(25, 3, 125),
        'ingresos': np.random.normal(35000, 5000, 125),
        'gasto_electronica': np.random.gamma(2, 200, 125),
        'frecuencia_online': np.random.poisson(15, 125)
    },
    'adulto_familia': {
        'edad': np.random.normal(40, 5, 200),
        'ingresos': np.random.normal(60000, 10000, 200),
        'gasto_electronica': np.random.gamma(1.5, 150, 200),
        'frecuencia_online': np.random.poisson(8, 200)
    },
    'mayor_conservador': {
        'edad': np.random.normal(65, 5, 175),
        'ingresos': np.random.normal(45000, 8000, 175),
        'gasto_electronica': np.random.gamma(1, 50, 175),
        'frecuencia_online': np.random.poisson(3, 175)
    }
}

# Combinar datos
data_ecommerce = {
    'edad': np.concatenate([p['edad'] for p in perfiles.values()]),
    'ingresos': np.concatenate([p['ingresos'] for p in perfiles.values()]),
    'gasto_electronica': np.concatenate([p['gasto_electronica'] for p in perfiles.values()]),
    'frecuencia_online': np.concatenate([p['frecuencia_online'] for p in perfiles.values()])
}

df_ecommerce = pd.DataFrame(data_ecommerce)

# Aplicar clustering
X_ecommerce = StandardScaler().fit_transform(df_ecommerce)
kmeans_ecommerce = KMeans(n_clusters=3, random_state=42)
segmentos = kmeans_ecommerce.fit_predict(X_ecommerce)

# AnÃ¡lisis de segmentos
df_ecommerce['segmento'] = segmentos
for i in range(3):
    segmento = df_ecommerce[df_ecommerce['segmento'] == i]
    print(f"\nSegmento {i} ({len(segmento)} clientes):")
    print(f"  Edad promedio: {segmento['edad'].mean():.1f} aÃ±os")
    print(f"  Ingresos promedio: ${segmento['ingresos'].mean():,.0f}")
    print(f"  Gasto electrÃ³nica: ${segmento['gasto_electronica'].mean():.0f}")

# VisualizaciÃ³n
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
scatter = plt.scatter(df_ecommerce['edad'], df_ecommerce['ingresos'], c=segmentos, cmap='viridis', alpha=0.6)
plt.xlabel('Edad')
plt.ylabel('Ingresos')
plt.title('SegmentaciÃ³n de Clientes')
plt.colorbar(scatter)

plt.subplot(1, 2, 2)
scatter = plt.scatter(df_ecommerce['gasto_electronica'], df_ecommerce['frecuencia_online'], c=segmentos, cmap='viridis', alpha=0.6)
plt.xlabel('Gasto ElectrÃ³nica')
plt.ylabel('Frecuencia Online')
plt.title('SegmentaciÃ³n de Clientes')
plt.colorbar(scatter)
plt.show()
```

---

## ğŸ“š **8.7 Recursos Complementarios**

### **ğŸ“– Lecturas:**
- "Pattern Recognition and Machine Learning" - Christopher Bishop
- "The Elements of Statistical Learning" - Hastie, Tibshirani, Friedman

### **ğŸ› ï¸ LibrerÃ­as:**
```python
import sklearn.cluster          # K-Means, DBSCAN
import sklearn.decomposition   # PCA
import mlxtend                 # Reglas de asociaciÃ³n
import umap                    # UMAP
```

### **ğŸ¯ Casos de Uso:**
| Industria | AplicaciÃ³n | Algoritmo |
|-----------|------------|-----------|
| Retail | SegmentaciÃ³n clientes | K-Means |
| Banca | DetecciÃ³n fraudes | DBSCAN |
| Salud | AnÃ¡lisis genes | Hierarchical |
| Marketing | Market Basket | Apriori |

---

## ğŸ“ **8.8 Glosario**

| TÃ©rmino | DefiniciÃ³n |
|---------|------------|
| **Clustering** | AgrupaciÃ³n de objetos similares |
| **Centroide** | Punto promedio de un cluster |
| **Silhouette** | MÃ©trica de calidad del clustering |
| **Outlier** | Punto que no pertenece a ningÃºn cluster |
| **PCA** | ReducciÃ³n de dimensionalidad lineal |
| **Soporte** | Frecuencia de un itemset |
| **Confianza** | Probabilidad condicional |

---

## ğŸ¤– **8.9 AgrupaciÃ³n y SegmentaciÃ³n con IA**

### **ğŸš€ Tendencias Futuras:**
1. **Clustering con Deep Learning**: Autoencoders para reducciÃ³n de dimensionalidad
2. **Clustering Adaptativo**: ActualizaciÃ³n en tiempo real
3. **Interpretabilidad**: SHAP para explicar clusters
4. **Clustering Multimodal**: Combinar texto, imÃ¡genes y datos tabulares

### **ğŸ¯ Aplicaciones Avanzadas:**
- **SegmentaciÃ³n DinÃ¡mica**: Clusters que evolucionan en el tiempo
- **Clustering Federado**: Privacidad en clustering distribuido
- **Clustering Ã‰tico**: Evitar sesgos en segmentaciÃ³n

---

## âœ… **Resumen de la Clase 8**

### **ğŸ¯ Lo que aprendiste:**
1. **Fundamentos**: Aprendizaje no supervisado
2. **Algoritmos**: K-Means, DBSCAN, JerÃ¡rquico
3. **ReducciÃ³n de Dimensionalidad**: PCA
4. **Reglas de AsociaciÃ³n**: Market Basket Analysis
5. **EvaluaciÃ³n**: MÃ©tricas de calidad
6. **Aplicaciones**: SegmentaciÃ³n de clientes

### **ğŸ’¡ Consejos Clave:**
- **Empezar simple**: K-Means es un buen punto de partida
- **Visualizar siempre**: Los grÃ¡ficos revelan patrones
- **Validar resultados**: Usar mÃºltiples mÃ©tricas
- **Interpretar en contexto**: Los clusters deben tener sentido de negocio

Â¡Ahora puedes descubrir patrones ocultos en tus datos! ğŸ‰

