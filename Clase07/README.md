### **¬øQu√© es Machine Learning?**  
**Machine Learning (ML)** o *Aprendizaje Autom√°tico* es una rama de la **Inteligencia Artificial (IA)** que se centra en desarrollar algoritmos y modelos capaces de aprender patrones a partir de datos, **sin ser programados expl√≠citamente**. En lugar de seguir reglas predefinidas, los sistemas de ML **mejoran su desempe√±o con la experiencia** (datos).  

Su objetivo principal es **generalizar** a partir de ejemplos para realizar predicciones o tomar decisiones en situaciones nuevas.

---

### **Particularidades y Caracter√≠sticas Principales**  

1. **Aprendizaje basado en datos**  
   - ML requiere **datos hist√≥ricos o de entrenamiento** para identificar patrones y relaciones.  
   - A diferencia de la programaci√≥n tradicional (donde las reglas son fijas), en ML **el modelo "aprende" de los datos**.  

2. **Capacidad predictiva y adaptativa**  
   - Los modelos de ML pueden **predecir resultados futuros** (ej.: ventas, fallos en equipos) o **clasificar informaci√≥n** (ej.: spam/no spam).  
   - Algunos sistemas se adaptan a cambios en los datos (ej.: recomendaciones en tiempo real en Netflix o Amazon).  

3. **Tipos principales de aprendizaje**  
   - **Supervisado**: Usa datos etiquetados (ej.: predecir precios de casas basado en ejemplos pasados).  
   - **No supervisado**: Encuentra patrones en datos sin etiquetas (ej.: agrupaci√≥n de clientes por comportamiento).  
   - **Por refuerzo**: Aprende mediante prueba/error y recompensas (ej.: robots que aprenden a caminar).  

4. **Automatizaci√≥n y escalabilidad**  
   - ML permite automatizar tareas complejas (ej.: diagn√≥stico m√©dico, detecci√≥n de fraudes).  
   - Escala bien con grandes vol√∫menes de datos (*Big Data*).  

5. **√ânfasis en la evaluaci√≥n**  
   - Los modelos se validan con m√©tricas como **precisi√≥n, recall o error cuadr√°tico medio** para garantizar su fiabilidad.  

6. **Dependencia de la calidad de los datos**  
   - El rendimiento del ML est√° ligado a la **calidad, cantidad y representatividad** de los datos.  
   - Problemas como *sesgos* o *datos incompletos* afectan los resultados.  

7. **Uso de algoritmos diversos**  
   - Desde m√©todos cl√°sicos (*regresi√≥n lineal, √°rboles de decisi√≥n*) hasta t√©cnicas avanzadas (*redes neuronales, deep learning*).  

---

### **Diferencia clave con Data Science**  
Mientras **Data Science** abarca todo el ciclo de an√°lisis de datos (limpieza, visualizaci√≥n, estad√≠stica, etc.), **ML es una herramienta dentro de DS** enfocada espec√≠ficamente en **automatizar el aprendizaje** para predicci√≥n o toma de decisiones.  

**Ejemplo pr√°ctico**:  
- Un modelo de ML podr√≠a predecir el *churn* (abandono) de clientes en una empresa, mientras que un data scientist tambi√©n analizar√≠a *por qu√©* ocurre y c√≥mo comunicarlo.  

En resumen, **Machine Learning es la tecnolog√≠a que permite a las m√°quinas "aprender" de los datos para resolver problemas complejos de manera aut√≥noma o semiaut√≥noma**. 

---


### _Si quisi√©ramos resolver un problema donde tenemos informaci√≥n georeferenciada de clientes ¬øc√≥mo podr√≠amos utilizar el ML para incrementar las ventas de un producto?_

Para incrementar las ventas de un producto utilizando **Machine Learning (ML)** con datos georreferenciados de clientes, puedes aplicar diversas estrategias basadas en an√°lisis espacial y modelos predictivos. Aqu√≠ te detallo un enfoque estructurado:

---

### **1. An√°lisis Exploratorio de Datos (EDA) Geoespacial**  
- **Visualizaci√≥n de datos**:  
  - Usar mapas de calor (*heatmaps*) para identificar zonas con alta concentraci√≥n de clientes o ventas.  
  - Segmentar por zonas geogr√°ficas (barrios, ciudades, c√≥digos postales).  
- **Detecci√≥n de patrones**:  
  - Correlacionar ubicaci√≥n con variables como ingresos, edad, clima o proximidad a puntos de inter√©s (ej.: centros comerciales).  

**Herramientas**: Python (`geopandas`, `folium`), Power BI (integrado con Mapas).  

---

### **2. Segmentaci√≥n de Clientes por Ubicaci√≥n**  
- **Clustering no supervisado** (ej.: *K-means* o *DBSCAN*) para agrupar clientes con caracter√≠sticas similares:  
  - Crear clusters basados en:  
    - Ubicaci√≥n (coordenadas).  
    - Comportamiento de compra + datos demogr√°ficos locales.  
  - Ejemplo: Identificar "zonas de alto potencial" con clientes similares a los que ya compran el producto.  

---

### **3. Modelos Predictivos para Ventas**  
- **Aprendizaje supervisado** (ej.: *Random Forest, XGBoost*) para predecir:  
  - **Propensi√≥n de compra**: Qu√© clientes (o zonas) tienen mayor probabilidad de comprar el producto.  
  - **Demanda geogr√°fica**: D√≥nde habr√° mayor demanda en funci√≥n de variables temporales (ej.: festividades locales).  
- **Variables de entrada**:  
  - Datos geogr√°ficos (latitud, longitud, distancia a tiendas).  
  - Datos socioecon√≥micos de la zona (nivel de ingresos, densidad poblacional).  
  - Historial de ventas en la regi√≥n.  

---

### **4. Recomendaciones Geo-Personalizadas**  
- **Sistemas de recomendaci√≥n** con filtrado colaborativo o basado en contenido:  
  - Sugerir productos populares en la zona (ej.: "En tu barrio, otros clientes compran X").  
  - Adaptar promociones seg√∫n el perfil geogr√°fico (ej.: descuentos en zonas con menor penetraci√≥n).  

---

### **5. Optimizaci√≥n Log√≠stica y Ubicaci√≥n de Puntos de Venta**  
- **Modelos de ubicaci√≥n √≥ptima**:  
  - Usar *algoritmos de optimizaci√≥n* (ej.: *p-median*) para decidir d√≥nde abrir nuevas tiendas o colocar anuncios.  
  - Ejemplo: "Las zonas con radio de 5 km sin cobertura tienen un 20% de clientes potenciales sin atender".  

---

### **6. Campa√±as de Marketing Dirigido**  
- **Geo-targeting publicitario**:  
  - Entrenar modelos para identificar zonas donde campa√±as espec√≠ficas (ej.: SMS, redes sociales) tendr√°n mayor ROI.  
  - Ejemplo: Anuncios en Facebook Ads para un radio de 10 km alrededor de tiendas con stock alto.  

---

### **7. Ejemplo Pr√°ctico**  
**Problema**: Una cadena de cafeter√≠as quiere aumentar ventas en Ciudad de M√©xico.  
**Soluci√≥n con ML**:  
1. Agrupa clientes por colonia usando *DBSCAN*.  
2. Entrena un modelo para predecir ventas seg√∫n:  
   - Proximidad a estaciones de metro.  
   - Nivel socioecon√≥mico (datos del INEGI).  
3. Descubre que las colonias *Roma* y *Condesa* tienen alta demanda los fines de semana.  
4. Lanza promociones "2x1" los s√°bados en esas zonas via WhatsApp.  

---

### **Beneficios**  
- **Reducci√≥n de costos**: Enfoque en zonas de alto impacto.  
- **Personalizaci√≥n**: Ofertas relevantes por ubicaci√≥n.  
- **Escalabilidad**: Aplicable a m√∫ltiples regiones o productos.  

---

## üìö Repaso Clase Anterior (Clase 6)

### Teor√≠a Fundamental del EDA y Preprocesamiento

En la **Clase 6** establecimos las bases fundamentales para el an√°lisis de datos que ahora aplicaremos en **Machine Learning**:

#### üîç **An√°lisis Exploratorio de Datos (EDA)**
- **Filosof√≠a**: Acercarse a los datos sin prejuicios para descubrir patrones inesperados
- **Objetivo**: Entender la estructura de los datos antes de aplicar modelos predictivos
- **Herramientas**: Estad√≠stica descriptiva + visualizaci√≥n de datos

#### üìä **Estad√≠stica Descriptiva**
- **Medidas de tendencia central**: Media, mediana, moda
- **Medidas de dispersi√≥n**: Varianza, desviaci√≥n est√°ndar, IQR
- **Distribuciones**: Normal, uniforme, y su visualizaci√≥n con histogramas
- **Correlaci√≥n**: Relaci√≥n entre variables (importante: correlaci√≥n ‚â† causalidad)

#### üßπ **Preprocesamiento de Datos**
- **Limpieza**: Manejo de valores faltantes y outliers
- **Transformaci√≥n**: Normalizaci√≥n, codificaci√≥n de variables categ√≥ricas
- **Integraci√≥n**: Combinar datos de m√∫ltiples fuentes
- **Reducci√≥n**: PCA para simplificar la dimensionalidad

---

### üí° Ejemplo 1: An√°lisis Estad√≠stico Descriptivo

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Datos de ventas mensuales de una empresa
np.random.seed(42)
ventas = np.random.normal(50000, 12000, 24)  # 2 a√±os de datos
ventas = np.append(ventas, [150000, -5000])  # outliers

df_ventas = pd.DataFrame({"Ventas_Mensuales": ventas})

# Medidas descriptivas
print("=== ESTAD√çSTICAS DESCRIPTIVAS ===")
print(f"Media: {df_ventas['Ventas_Mensuales'].mean():.2f}")
print(f"Mediana: {df_ventas['Ventas_Mensuales'].median():.2f}")
print(f"Desviaci√≥n est√°ndar: {df_ventas['Ventas_Mensuales'].std():.2f}")

# Visualizaci√≥n
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
sns.histplot(df_ventas["Ventas_Mensuales"], kde=True, bins=15)
plt.title("Distribuci√≥n de Ventas Mensuales")

plt.subplot(1,2,2)
sns.boxplot(x=df_ventas["Ventas_Mensuales"])
plt.title("Boxplot - Detecci√≥n de Outliers")
plt.show()
```

**üéØ Objetivo**: Identificar patrones en los datos de ventas y detectar valores at√≠picos que podr√≠an afectar modelos predictivos.

---

### üí° Ejemplo 2: Preprocesamiento con PCA

```python
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Dataset con m√∫ltiples variables correlacionadas
np.random.seed(42)
X1 = np.random.normal(100, 20, 100)
X2 = X1 * 0.8 + np.random.normal(0, 5, 100)  # correlacionada con X1
X3 = np.random.normal(50, 10, 100)           # independiente

df_features = pd.DataFrame({
    "Ingresos": X1,
    "Gastos": X2, 
    "Ahorros": X3
})

print("=== CORRELACIONES ORIGINALES ===")
print(df_features.corr())

# Estandarizaci√≥n (importante para PCA)
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_features)

# Aplicar PCA
pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_scaled)

print(f"\n=== VARIANZA EXPLICADA ===")
print(f"PC1: {pca.explained_variance_ratio_[0]:.2%}")
print(f"PC2: {pca.explained_variance_ratio_[1]:.2%}")

# Visualizaci√≥n
df_pca = pd.DataFrame(pca_result, columns=["PC1", "PC2"])
plt.figure(figsize=(6,6))
sns.scatterplot(x="PC1", y="PC2", data=df_pca, s=60)
plt.title("Datos transformados con PCA")
plt.show()
```

**üéØ Objetivo**: Reducir la dimensionalidad eliminando redundancia entre variables correlacionadas, preparando datos m√°s limpios para algoritmos de ML.

---

### üîó **Conexi√≥n con Machine Learning**

Los conceptos de la **Clase 6** son **fundamentales** para el √©xito en ML:

- **EDA** ‚Üí Nos ayuda a entender qu√© variables son relevantes para predecir
- **Estad√≠stica descriptiva** ‚Üí Identifica distribuciones y relaciones que los algoritmos pueden aprovechar
- **Preprocesamiento** ‚Üí Asegura que los datos est√©n limpios y listos para entrenar modelos
- **PCA** ‚Üí Reduce complejidad y mejora el rendimiento de los algoritmos

---

# ML

*  ## [Aprendizaje Supervizado](clase_7/aprendizaje-supervisado.md)
* ## [Aprendizaje no Supervisado](clase_7/aprendizaje-no-supervisado.md)

* ## [Paso a paso para un ML funcional](clase_7/paso-a-pas.md)


---
### Apartado especial para lo que es Feature Selection

### üßπ 1. M√©todos de Filtro (*Filter Methods*)

Piensa en estos como un primer filtro r√°pido.

* **C√≥mo funcionan:** Eval√∫an cada caracter√≠stica de forma individual usando pruebas estad√≠sticas que miden su relaci√≥n con la variable objetivo (la etiqueta).
* **Velocidad:** Muy r√°pidos, ya que no requieren entrenar modelos.
* **Ejemplos:**

  * **Prueba de Chi-cuadrado** (para variables categ√≥ricas)
  * **Prueba ANOVA**
  * **Informaci√≥n mutua**
  * **Coeficiente de correlaci√≥n**

üìå **Ventajas:** R√°pidos y no dependen del modelo
üìå **Desventajas:** No consideran las interacciones entre caracter√≠sticas

---

### üß™ 2. M√©todos Envolventes (*Wrapper Methods*)

Estos son como probar diferentes combinaciones de ropa para ver cu√°l te queda mejor.

* **C√≥mo funcionan:** Prueban m√∫ltiples subconjuntos de caracter√≠sticas entrenando modelos, y eligen el subconjunto que da el mejor rendimiento.
* **Velocidad:** Lentos, porque entrenan muchos modelos.
* **Ejemplos:**

  * **Eliminaci√≥n recursiva de caracter√≠sticas (RFE)**
  * **Selecci√≥n hacia adelante**
  * **Eliminaci√≥n hacia atr√°s**

üìå **Ventajas:** Tienen en cuenta interacciones entre caracter√≠sticas
üìå **Desventajas:** Muy costosos en tiempo y recursos computacionales

---

### ‚öôÔ∏è 3. M√©todos Embebidos (*Embedded Methods*)

Estos seleccionan caracter√≠sticas **mientras entrenan** el modelo.

* **C√≥mo funcionan:** La selecci√≥n ocurre como parte del proceso de entrenamiento.
* **Ejemplos:**

  * **Lasso (regularizaci√≥n L1)** ‚Äì reduce coeficientes a cero
  * **√Årboles de decisi√≥n / Bosques aleatorios** ‚Äì calculan importancia de cada variable
  * **Elastic Net** ‚Äì combina L1 y L2

üìå **Ventajas:** M√°s eficientes y consideran interacciones
üìå **Desventajas:** Dependientes del modelo utilizado

---

### Tabla Resumen:

| Tipo de m√©todo | ¬øUsa modelo? | ¬øConsidera interacciones? | Velocidad | Ejemplos                      |
| -------------- | ------------ | ------------------------- | --------- | ----------------------------- |
| Filtro         | ‚ùå No         | ‚ùå No                      | üöÄ R√°pido | Chi-cuadrado, correlaci√≥n     |
| Envolvente     | ‚úÖ S√≠         | ‚úÖ S√≠                      | üê¢ Lento  | RFE, selecci√≥n hacia adelante |
| Embebido       | ‚úÖ S√≠         | ‚úÖ S√≠                      | ‚ö° Medio   | Lasso, √°rboles de decisi√≥n    |


## Recomendaciones para aprender:
¬°Claro! Vamos a ver cada uno de estos m√©todos de selecci√≥n de caracter√≠sticas supervisadas con **ejemplos, ventajas y desventajas**, relacion√°ndolos con los tipos que ya vimos: filtro, envolvente o embebido.

---

### üìâ 1. **Variance Threshold**

üîπ **Tipo:** Filtro
üîπ **Qu√© hace:** Elimina caracter√≠sticas con **baja varianza**, es decir, que no cambian mucho entre ejemplos (por ejemplo, una columna donde casi todos los valores son iguales).

#### ‚úÖ Ventajas:

* Muy simple y r√°pido de aplicar
* No necesita etiquetas (tambi√©n se puede usar en problemas no supervisados)

#### ‚ùå Desventajas:

* No considera la relaci√≥n con la variable objetivo
* Puede eliminar caracter√≠sticas √∫tiles si tienen poca varianza pero alta relevancia

#### üß™ Ejemplo en Python:

```python
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)
X_reduced = selector.fit_transform(X)
```

---

### ‚≠ê 2. **SelectKBest**

üîπ **Tipo:** Filtro
üîπ **Qu√© hace:** Selecciona las **k mejores caracter√≠sticas** seg√∫n una m√©trica estad√≠stica que mide la relaci√≥n con la variable objetivo (como ANOVA, chi-cuadrado, etc.).

#### ‚úÖ Ventajas:

* F√°cil de interpretar y aplicar
* R√°pido y eficiente
* Permite usar distintas m√©tricas

#### ‚ùå Desventajas:

* No considera interacciones entre variables
* Necesita que t√∫ elijas el valor de *k* (el n√∫mero de caracter√≠sticas a conservar)

#### üß™ Ejemplo en Python:

```python
from sklearn.feature_selection import SelectKBest, f_classif
selector = SelectKBest(score_func=f_classif, k=10)
X_kbest = selector.fit_transform(X, y)
```

---

### üîÑ 3. **RFE (Recursive Feature Elimination)**

üîπ **Tipo:** Envolvente
üîπ **Qu√© hace:** Usa un modelo (como una regresi√≥n o un SVM) para eliminar **recursivamente** las caracter√≠sticas menos importantes hasta quedarse con las m√°s relevantes.

#### ‚úÖ Ventajas:

* Considera interacciones entre variables
* Da muy buenos resultados si se elige bien el modelo base

#### ‚ùå Desventajas:

* Lento, especialmente con muchos datos o caracter√≠sticas
* Depende fuertemente del modelo usado

#### üß™ Ejemplo en Python:

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
rfe = RFE(estimator=model, n_features_to_select=5)
X_rfe = rfe.fit_transform(X, y)
```

---

### üå≥ 4. **Boruta**

üîπ **Tipo:** Envolvente (basado en √°rboles, como Random Forest)
üîπ **Qu√© hace:** Crea versiones "aleatorias" de las caracter√≠sticas y las compara con las reales. Solo conserva las que son mejores que las versiones aleatorias (shadow features).

#### ‚úÖ Ventajas:

* Muy robusto y preciso
* Tiene en cuenta interacciones y relaciones no lineales
* Funciona bien con datos complejos

#### ‚ùå Desventajas:

* Bastante lento (usa muchos modelos de Random Forest)
* No est√° en `scikit-learn` directamente (hay que usar una librer√≠a aparte como `boruta_py`)

#### üß™ Ejemplo con `boruta_py`:

```python
from boruta import BorutaPy
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)
boruta_selector = BorutaPy(estimator=model, n_estimators='auto', random_state=42)
boruta_selector.fit(X.values, y.values)
```

---

### üîç Resumen Comparativo

| M√©todo             | Tipo       | Interacci√≥n | Velocidad    | Modelo requerido | Ventaja principal                    |
| ------------------ | ---------- | ----------- | ------------ | ---------------- | ------------------------------------ |
| Variance Threshold | Filtro     | ‚ùå No        | üöÄ R√°pido    | ‚ùå No             | Muy simple y r√°pido                  |
| SelectKBest        | Filtro     | ‚ùå No        | üöÄ R√°pido    | ‚ùå No             | M√©tricas estad√≠sticas supervisadas   |
| RFE                | Envolvente | ‚úÖ S√≠        | üê¢ Lento     | ‚úÖ S√≠             | Preciso si el modelo es adecuado     |
| Boruta             | Envolvente | ‚úÖ S√≠        | üêå Muy lento | ‚úÖ S√≠             | Robusto frente a ruido y redundancia |

