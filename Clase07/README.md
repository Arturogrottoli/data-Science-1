## üìö Repaso Clase Anterior (Clase 6)

### Teor√≠a Fundamental del EDA y Preprocesamiento

En la **Clase 6** establecimos las bases fundamentales para el an√°lisis de datos que ahora aplicaremos en **Machine Learning**:

#### üîç **An√°lisis Exploratorio de Datos (EDA)**
- **Filosof√≠a**: Acercarse a los datos sin prejuicios para descubrir patrones inesperados
- **Objetivo**: Entender la estructura de los datos antes de aplicar modelos predictivos
- **Herramientas**: Estad√≠stica descriptiva + visualizaci√≥n de datos

#### üìä **Estad√≠stica Descriptiva**
- **Medidas de tendencia central**: Media, mediana, moda
- **Medidas de dispersi√≥n**: Varianza, desviaci√≥n est√°ndar, IQR
- **Distribuciones**: Normal, uniforme, y su visualizaci√≥n con histogramas
- **Correlaci√≥n**: Relaci√≥n entre variables (importante: correlaci√≥n ‚â† causalidad)

#### üßπ **Preprocesamiento de Datos**
- **Limpieza**: Manejo de valores faltantes y outliers
- **Transformaci√≥n**: Normalizaci√≥n, codificaci√≥n de variables categ√≥ricas
- **Integraci√≥n**: Combinar datos de m√∫ltiples fuentes
- **Reducci√≥n**: PCA para simplificar la dimensionalidad

---

### üí° Ejemplo 1: An√°lisis Estad√≠stico Descriptivo

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Datos de ventas mensuales de una empresa
np.random.seed(42)
ventas = np.random.normal(50000, 12000, 24)  # 2 a√±os de datos
ventas = np.append(ventas, [150000, -5000])  # outliers

df_ventas = pd.DataFrame({"Ventas_Mensuales": ventas})

# Medidas descriptivas
print("=== ESTAD√çSTICAS DESCRIPTIVAS ===")
print(f"Media: {df_ventas['Ventas_Mensuales'].mean():.2f}")
print(f"Mediana: {df_ventas['Ventas_Mensuales'].median():.2f}")
print(f"Desviaci√≥n est√°ndar: {df_ventas['Ventas_Mensuales'].std():.2f}")

# Visualizaci√≥n
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
sns.histplot(df_ventas["Ventas_Mensuales"], kde=True, bins=15)
plt.title("Distribuci√≥n de Ventas Mensuales")

plt.subplot(1,2,2)
sns.boxplot(x=df_ventas["Ventas_Mensuales"])
plt.title("Boxplot - Detecci√≥n de Outliers")
plt.show()
```

**üéØ Objetivo**: Identificar patrones en los datos de ventas y detectar valores at√≠picos que podr√≠an afectar modelos predictivos.

---

### üí° Ejemplo 2: Preprocesamiento con PCA

```python
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Dataset con m√∫ltiples variables correlacionadas
np.random.seed(42)
X1 = np.random.normal(100, 20, 100)
X2 = X1 * 0.8 + np.random.normal(0, 5, 100)  # correlacionada con X1
X3 = np.random.normal(50, 10, 100)           # independiente

df_features = pd.DataFrame({
    "Ingresos": X1,
    "Gastos": X2, 
    "Ahorros": X3
})

print("=== CORRELACIONES ORIGINALES ===")
print(df_features.corr())

# Estandarizaci√≥n (importante para PCA)
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_features)

# Aplicar PCA
pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_scaled)

print(f"\n=== VARIANZA EXPLICADA ===")
print(f"PC1: {pca.explained_variance_ratio_[0]:.2%}")
print(f"PC2: {pca.explained_variance_ratio_[1]:.2%}")

# Visualizaci√≥n
df_pca = pd.DataFrame(pca_result, columns=["PC1", "PC2"])
plt.figure(figsize=(6,6))
sns.scatterplot(x="PC1", y="PC2", data=df_pca, s=60)
plt.title("Datos transformados con PCA")
plt.show()
```

**üéØ Objetivo**: Reducir la dimensionalidad eliminando redundancia entre variables correlacionadas, preparando datos m√°s limpios para algoritmos de ML.

---

### üîó **Conexi√≥n con Machine Learning**

Los conceptos de la **Clase 6** son **fundamentales** para el √©xito en ML:

- **EDA** ‚Üí Nos ayuda a entender qu√© variables son relevantes para predecir
- **Estad√≠stica descriptiva** ‚Üí Identifica distribuciones y relaciones que los algoritmos pueden aprovechar
- **Preprocesamiento** ‚Üí Asegura que los datos est√©n limpios y listos para entrenar modelos
- **PCA** ‚Üí Reduce complejidad y mejora el rendimiento de los algoritmos

---

### **¬øQu√© es Machine Learning?**  
**Machine Learning (ML)** o *Aprendizaje Autom√°tico* es una rama de la **Inteligencia Artificial (IA)** que se centra en desarrollar algoritmos y modelos capaces de aprender patrones a partir de datos, **sin ser programados expl√≠citamente**. En lugar de seguir reglas predefinidas, los sistemas de ML **mejoran su desempe√±o con la experiencia** (datos).  

Su objetivo principal es **generalizar** a partir de ejemplos para realizar predicciones o tomar decisiones en situaciones nuevas.

---

### **Particularidades y Caracter√≠sticas Principales**  

1. **Aprendizaje basado en datos**  
   - ML requiere **datos hist√≥ricos o de entrenamiento** para identificar patrones y relaciones.  
   - A diferencia de la programaci√≥n tradicional (donde las reglas son fijas), en ML **el modelo "aprende" de los datos**.  

2. **Capacidad predictiva y adaptativa**  
   - Los modelos de ML pueden **predecir resultados futuros** (ej.: ventas, fallos en equipos) o **clasificar informaci√≥n** (ej.: spam/no spam).  
   - Algunos sistemas se adaptan a cambios en los datos (ej.: recomendaciones en tiempo real en Netflix o Amazon).  

3. **Tipos principales de aprendizaje**  
   - **Supervisado**: Usa datos etiquetados (ej.: predecir precios de casas basado en ejemplos pasados).  
   - **No supervisado**: Encuentra patrones en datos sin etiquetas (ej.: agrupaci√≥n de clientes por comportamiento).  
   - **Por refuerzo**: Aprende mediante prueba/error y recompensas (ej.: robots que aprenden a caminar).  

4. **Automatizaci√≥n y escalabilidad**  
   - ML permite automatizar tareas complejas (ej.: diagn√≥stico m√©dico, detecci√≥n de fraudes).  
   - Escala bien con grandes vol√∫menes de datos (*Big Data*).  

5. **√ânfasis en la evaluaci√≥n**  
   - Los modelos se validan con m√©tricas como **precisi√≥n, recall o error cuadr√°tico medio** para garantizar su fiabilidad.  

6. **Dependencia de la calidad de los datos**  
   - El rendimiento del ML est√° ligado a la **calidad, cantidad y representatividad** de los datos.  
   - Problemas como *sesgos* o *datos incompletos* afectan los resultados.  

7. **Uso de algoritmos diversos**  
   - Desde m√©todos cl√°sicos (*regresi√≥n lineal, √°rboles de decisi√≥n*) hasta t√©cnicas avanzadas (*redes neuronales, deep learning*).  

---

### **Diferencia clave con Data Science**  
Mientras **Data Science** abarca todo el ciclo de an√°lisis de datos (limpieza, visualizaci√≥n, estad√≠stica, etc.), **ML es una herramienta dentro de DS** enfocada espec√≠ficamente en **automatizar el aprendizaje** para predicci√≥n o toma de decisiones.  

**Ejemplo pr√°ctico**:  
- Un modelo de ML podr√≠a predecir el *churn* (abandono) de clientes en una empresa, mientras que un data scientist tambi√©n analizar√≠a *por qu√©* ocurre y c√≥mo comunicarlo.  

En resumen, **Machine Learning es la tecnolog√≠a que permite a las m√°quinas "aprender" de los datos para resolver problemas complejos de manera aut√≥noma o semiaut√≥noma**. 

---


### _Si quisi√©ramos resolver un problema donde tenemos informaci√≥n georeferenciada de clientes ¬øc√≥mo podr√≠amos utilizar el ML para incrementar las ventas de un producto?_

Para incrementar las ventas de un producto utilizando **Machine Learning (ML)** con datos georreferenciados de clientes, puedes aplicar diversas estrategias basadas en an√°lisis espacial y modelos predictivos. Aqu√≠ te detallo un enfoque estructurado:

---

### **1. An√°lisis Exploratorio de Datos (EDA) Geoespacial**  
- **Visualizaci√≥n de datos**:  
  - Usar mapas de calor (*heatmaps*) para identificar zonas con alta concentraci√≥n de clientes o ventas.  
  - Segmentar por zonas geogr√°ficas (barrios, ciudades, c√≥digos postales).  
- **Detecci√≥n de patrones**:  
  - Correlacionar ubicaci√≥n con variables como ingresos, edad, clima o proximidad a puntos de inter√©s (ej.: centros comerciales).  

**Herramientas**: Python (`geopandas`, `folium`), Power BI (integrado con Mapas).  

---

### **2. Segmentaci√≥n de Clientes por Ubicaci√≥n**  
- **Clustering no supervisado** (ej.: *K-means* o *DBSCAN*) para agrupar clientes con caracter√≠sticas similares:  
  - Crear clusters basados en:  
    - Ubicaci√≥n (coordenadas).  
    - Comportamiento de compra + datos demogr√°ficos locales.  
  - Ejemplo: Identificar "zonas de alto potencial" con clientes similares a los que ya compran el producto.  

---

### **3. Modelos Predictivos para Ventas**  
- **Aprendizaje supervisado** (ej.: *Random Forest, XGBoost*) para predecir:  
  - **Propensi√≥n de compra**: Qu√© clientes (o zonas) tienen mayor probabilidad de comprar el producto.  
  - **Demanda geogr√°fica**: D√≥nde habr√° mayor demanda en funci√≥n de variables temporales (ej.: festividades locales).  
- **Variables de entrada**:  
  - Datos geogr√°ficos (latitud, longitud, distancia a tiendas).  
  - Datos socioecon√≥micos de la zona (nivel de ingresos, densidad poblacional).  
  - Historial de ventas en la regi√≥n.  

---

### **4. Recomendaciones Geo-Personalizadas**  
- **Sistemas de recomendaci√≥n** con filtrado colaborativo o basado en contenido:  
  - Sugerir productos populares en la zona (ej.: "En tu barrio, otros clientes compran X").  
  - Adaptar promociones seg√∫n el perfil geogr√°fico (ej.: descuentos en zonas con menor penetraci√≥n).  

---

### **5. Optimizaci√≥n Log√≠stica y Ubicaci√≥n de Puntos de Venta**  
- **Modelos de ubicaci√≥n √≥ptima**:  
  - Usar *algoritmos de optimizaci√≥n* (ej.: *p-median*) para decidir d√≥nde abrir nuevas tiendas o colocar anuncios.  
  - Ejemplo: "Las zonas con radio de 5 km sin cobertura tienen un 20% de clientes potenciales sin atender".  

---

### **6. Campa√±as de Marketing Dirigido**  
- **Geo-targeting publicitario**:  
  - Entrenar modelos para identificar zonas donde campa√±as espec√≠ficas (ej.: SMS, redes sociales) tendr√°n mayor ROI.  
  - Ejemplo: Anuncios en Facebook Ads para un radio de 10 km alrededor de tiendas con stock alto.  

---

### **7. Ejemplo Pr√°ctico**  
**Problema**: Una cadena de cafeter√≠as quiere aumentar ventas en Ciudad de M√©xico.  
**Soluci√≥n con ML**:  
1. Agrupa clientes por colonia usando *DBSCAN*.  
2. Entrena un modelo para predecir ventas seg√∫n:  
   - Proximidad a estaciones de metro.  
   - Nivel socioecon√≥mico (datos del INEGI).  
3. Descubre que las colonias *Roma* y *Condesa* tienen alta demanda los fines de semana.  
4. Lanza promociones "2x1" los s√°bados en esas zonas via WhatsApp.  

---

### **Beneficios**  
- **Reducci√≥n de costos**: Enfoque en zonas de alto impacto.  
- **Personalizaci√≥n**: Ofertas relevantes por ubicaci√≥n.  
- **Escalabilidad**: Aplicable a m√∫ltiples regiones o productos.  

# ML

*  ## [Aprendizaje Supervizado](clase_7/aprendizaje-supervisado.md)
* ## [Aprendizaje no Supervisado](clase_7/aprendizaje-no-supervisado.md)

* ## [Paso a paso para un ML funcional](clase_7/paso-a-pas.md)


---

## üöÄ Implementaci√≥n Pr√°ctica de Machine Learning

### üìã **7.6 Implementaci√≥n Pr√°ctica**

La implementaci√≥n pr√°ctica de Machine Learning es el proceso que transforma la teor√≠a en soluciones reales. Esta fase es crucial porque determina el √©xito o fracaso de un proyecto de ML en el mundo real.

---

### üßπ **1. Preparaci√≥n de Datos**

La preparaci√≥n de datos es el **paso m√°s cr√≠tico** en cualquier proyecto de ML. Se estima que el 80% del tiempo en un proyecto de ML se dedica a la preparaci√≥n y limpieza de datos.

#### **üéØ Filosof√≠a de la Preparaci√≥n de Datos**
- **Principio**: "Garbage in, garbage out" - Si los datos de entrada son de mala calidad, el modelo ser√° in√∫til
- **Objetivo**: Transformar datos brutos en un formato limpio y estructurado
- **Enfoque**: Iterativo y sistem√°tico

#### **üìä Ejemplo Pr√°ctico: Dataset de Ventas de Tienda**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer

# Cargar datos (simulando un dataset real con problemas t√≠picos)
np.random.seed(42)
n_samples = 1000

# Crear datos con problemas reales
data = {
    'edad': np.random.normal(35, 10, n_samples),
    'ingresos': np.random.lognormal(10, 0.5, n_samples),
    'genero': np.random.choice(['M', 'F', 'masculino', 'femenino', None], n_samples, p=[0.4, 0.3, 0.1, 0.1, 0.1]),
    'ciudad': np.random.choice(['Buenos Aires', 'C√≥rdoba', 'Mendoza', 'BA', 'Cordoba', None], n_samples),
    'ventas_mes': np.random.gamma(2, 1000, n_samples),
    'satisfaccion': np.random.choice([1, 2, 3, 4, 5, None], n_samples, p=[0.1, 0.15, 0.2, 0.3, 0.2, 0.05])
}

df = pd.DataFrame(data)

# Agregar algunos outliers
df.loc[df.index[:50], 'ventas_mes'] *= 10
df.loc[df.index[50:60], 'edad'] = 200  # Valores imposibles

print("=== ESTADO INICIAL DE LOS DATOS ===")
print(f"Forma del dataset: {df.shape}")
print(f"\nValores faltantes por columna:")
print(df.isnull().sum())
print(f"\nTipos de datos:")
print(df.dtypes)
```

#### **üîß 1.1 Limpieza de Datos**

```python
# 1. Manejo de Valores Faltantes
print("\n=== LIMPIEZA DE DATOS ===")

# Estrategia para valores faltantes
def limpiar_valores_faltantes(df):
    df_limpio = df.copy()
    
    # Para variables num√©ricas: usar la mediana
    numeric_columns = ['edad', 'ingresos', 'ventas_mes']
    for col in numeric_columns:
        if df_limpio[col].isnull().sum() > 0:
            median_value = df_limpio[col].median()
            df_limpio[col].fillna(median_value, inplace=True)
            print(f"Imputado {col} con mediana: {median_value:.2f}")
    
    # Para variables categ√≥ricas: usar la moda
    categorical_columns = ['genero', 'ciudad', 'satisfaccion']
    for col in categorical_columns:
        if df_limpio[col].isnull().sum() > 0:
            mode_value = df_limpio[col].mode()[0]
            df_limpio[col].fillna(mode_value, inplace=True)
            print(f"Imputado {col} con moda: {mode_value}")
    
    return df_limpio

df_limpio = limpiar_valores_faltantes(df)

# 2. Correcci√≥n de Inconsistencias
def estandarizar_categorias(df):
    df_estandarizado = df.copy()
    
    # Estandarizar g√©nero
    df_estandarizado['genero'] = df_estandarizado['genero'].map({
        'M': 'Masculino', 'masculino': 'Masculino',
        'F': 'Femenino', 'femenino': 'Femenino'
    })
    
    # Estandarizar ciudades
    df_estandarizado['ciudad'] = df_estandarizado['ciudad'].map({
        'BA': 'Buenos Aires', 'Cordoba': 'C√≥rdoba'
    })
    
    return df_estandarizado

df_estandarizado = estandarizar_categorias(df_limpio)

# 3. Manejo de Outliers
def detectar_y_tratar_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    print(f"Outliers detectados en {column}: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)")
    
    # Estrategia: Capar los valores extremos
    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)
    return df

# Tratar outliers en ventas_mes y edad
df_final = detectar_y_tratar_outliers(df_estandarizado, 'ventas_mes')
df_final = detectar_y_tratar_outliers(df_final, 'edad')

print(f"\nDataset final: {df_final.shape}")
print(f"Valores faltantes restantes: {df_final.isnull().sum().sum()}")
```

#### **üîÑ 1.2 Transformaci√≥n de Datos**

```python
# Transformaci√≥n de variables
def transformar_datos(df):
    df_transformado = df.copy()
    
    # 1. Normalizaci√≥n/Estandarizaci√≥n
    scaler = StandardScaler()
    numeric_columns = ['edad', 'ingresos', 'ventas_mes']
    df_transformado[numeric_columns] = scaler.fit_transform(df_transformado[numeric_columns])
    
    # 2. Codificaci√≥n de variables categ√≥ricas
    # One-hot encoding para ciudad
    ciudad_encoded = pd.get_dummies(df_transformado['ciudad'], prefix='ciudad')
    df_transformado = pd.concat([df_transformado, ciudad_encoded], axis=1)
    
    # Label encoding para g√©nero y satisfacci√≥n
    le_genero = LabelEncoder()
    df_transformado['genero_encoded'] = le_genero.fit_transform(df_transformado['genero'])
    
    # 3. Transformaci√≥n logar√≠tmica (si es necesario)
    # Para variables con sesgo positivo
    if df['ingresos'].skew() > 1:
        df_transformado['ingresos_log'] = np.log1p(df['ingresos'])
    
    return df_transformado

df_transformado = transformar_datos(df_final)

print("\n=== DATOS TRANSFORMADOS ===")
print(f"Forma final: {df_transformado.shape}")
print(f"Columnas: {list(df_transformado.columns)}")
```

---

### ‚úÖ **2. Validaci√≥n del Modelo**

La validaci√≥n es esencial para asegurar que nuestro modelo funcionar√° en datos reales.

#### **üìä 2.1 Divisi√≥n del Conjunto de Datos**

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Preparar datos para modelado
# Asumimos que queremos predecir 'ventas_mes' basado en otras variables
X = df_transformado.drop(['ventas_mes', 'genero', 'ciudad'], axis=1)
y = df['ventas_mes']  # Usar valores originales, no normalizados

# Divisi√≥n 70-20-10: Train-Validation-Test
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.22, random_state=42)

print(f"Conjunto de entrenamiento: {X_train.shape[0]} muestras")
print(f"Conjunto de validaci√≥n: {X_val.shape[0]} muestras")
print(f"Conjunto de prueba: {X_test.shape[0]} muestras")
```

#### **üîÑ 2.2 Validaci√≥n Cruzada**

```python
from sklearn.model_selection import cross_val_score, KFold

def evaluar_modelo_con_cv(modelo, X, y, cv=5):
    """Funci√≥n para evaluar modelo con validaci√≥n cruzada"""
    
    # Configurar validaci√≥n cruzada
    kf = KFold(n_splits=cv, shuffle=True, random_state=42)
    
    # M√©tricas a evaluar
    mse_scores = cross_val_score(modelo, X, y, cv=kf, scoring='neg_mean_squared_error')
    r2_scores = cross_val_score(modelo, X, y, cv=kf, scoring='r2')
    
    return {
        'MSE_mean': -mse_scores.mean(),
        'MSE_std': mse_scores.std(),
        'R2_mean': r2_scores.mean(),
        'R2_std': r2_scores.std()
    }

# Entrenar modelo
modelo = RandomForestRegressor(n_estimators=100, random_state=42)
modelo.fit(X_train, y_train)

# Evaluaci√≥n con validaci√≥n cruzada
cv_results = evaluar_modelo_con_cv(modelo, X_train, y_train)

print("\n=== RESULTADOS DE VALIDACI√ìN CRUZADA ===")
print(f"MSE promedio: {cv_results['MSE_mean']:.2f} ¬± {cv_results['MSE_std']:.2f}")
print(f"R¬≤ promedio: {cv_results['R2_mean']:.3f} ¬± {cv_results['R2_std']:.3f}")
```

#### **üìà 2.3 M√©tricas de Evaluaci√≥n**

```python
def evaluar_modelo_completo(modelo, X_test, y_test):
    """Evaluaci√≥n completa del modelo"""
    
    # Predicciones
    y_pred = modelo.predict(X_test)
    
    # M√©tricas de regresi√≥n
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    
    # Error absoluto medio
    mae = np.mean(np.abs(y_test - y_pred))
    
    # Error porcentual absoluto medio
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    
    resultados = {
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'R¬≤': r2,
        'MAPE': mape
    }
    
    return resultados, y_pred

# Evaluar en conjunto de prueba
resultados, predicciones = evaluar_modelo_completo(modelo, X_test, y_test)

print("\n=== M√âTRICAS EN CONJUNTO DE PRUEBA ===")
for metrica, valor in resultados.items():
    print(f"{metrica}: {valor:.3f}")

# Visualizaci√≥n de resultados
plt.figure(figsize=(15, 5))

# 1. Predicciones vs Valores Reales
plt.subplot(1, 3, 1)
plt.scatter(y_test, predicciones, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Valores Reales')
plt.ylabel('Predicciones')
plt.title(f'Predicciones vs Reales\nR¬≤ = {resultados["R¬≤"]:.3f}')

# 2. Residuos
plt.subplot(1, 3, 2)
residuos = y_test - predicciones
plt.scatter(predicciones, residuos, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicciones')
plt.ylabel('Residuos')
plt.title('An√°lisis de Residuos')

# 3. Distribuci√≥n de errores
plt.subplot(1, 3, 3)
plt.hist(residuos, bins=30, alpha=0.7, edgecolor='black')
plt.xlabel('Residuos')
plt.ylabel('Frecuencia')
plt.title('Distribuci√≥n de Errores')

plt.tight_layout()
plt.show()
```

#### **‚öôÔ∏è 2.4 Ajuste de Hiperpar√°metros**

```python
from sklearn.model_selection import GridSearchCV

def ajustar_hiperparametros(modelo, X_train, y_train, param_grid, cv=3):
    """Ajuste de hiperpar√°metros con GridSearch"""
    
    grid_search = GridSearchCV(
        modelo, 
        param_grid, 
        cv=cv, 
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    return grid_search

# Definir grid de par√°metros para Random Forest
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

print("\n=== AJUSTE DE HIPERPAR√ÅMETROS ===")
print("Buscando mejores par√°metros...")

# Ajustar hiperpar√°metros
grid_search = ajustar_hiperparametros(
    RandomForestRegressor(random_state=42), 
    X_train, y_train, 
    param_grid
)

print(f"\nMejores par√°metros: {grid_search.best_params_}")
print(f"Mejor score (negativo MSE): {grid_search.best_score_:.2f}")

# Evaluar modelo optimizado
modelo_optimizado = grid_search.best_estimator_
resultados_optimizado, predicciones_optimizado = evaluar_modelo_completo(modelo_optimizado, X_test, y_test)

print("\n=== COMPARACI√ìN DE MODELOS ===")
print("Modelo Original vs Optimizado:")
print(f"R¬≤ Original: {resultados['R¬≤']:.3f}")
print(f"R¬≤ Optimizado: {resultados_optimizado['R¬≤']:.3f}")
print(f"Mejora: {(resultados_optimizado['R¬≤'] - resultados['R¬≤'])*100:.1f}%")
```

---

### üöÄ **3. Despliegue del Modelo**

El despliegue es donde el modelo pasa del laboratorio al mundo real.

#### **üì¶ 3.1 Preparaci√≥n para Producci√≥n**

```python
import joblib
import json

def guardar_modelo_completo(modelo, scaler, columnas, metadatos, ruta_base='modelo_ml'):
    """Guardar modelo y metadatos para producci√≥n"""
    
    # 1. Guardar modelo
    joblib.dump(modelo, f'{ruta_base}_modelo.pkl')
    
    # 2. Guardar scaler
    joblib.dump(scaler, f'{ruta_base}_scaler.pkl')
    
    # 3. Guardar informaci√≥n de columnas
    with open(f'{ruta_base}_columnas.json', 'w') as f:
        json.dump(columnas, f)
    
    # 4. Guardar metadatos
    with open(f'{ruta_base}_metadatos.json', 'w') as f:
        json.dump(metadatos, f)
    
    print(f"Modelo guardado en: {ruta_base}_*")

# Preparar metadatos
metadatos = {
    'version': '1.0',
    'fecha_entrenamiento': '2024-01-15',
    'algoritmo': 'RandomForestRegressor',
    'metricas_entrenamiento': resultados_optimizado,
    'caracteristicas': list(X.columns),
    'descripcion': 'Modelo para predecir ventas mensuales basado en perfil del cliente'
}

# Guardar modelo
guardar_modelo_completo(
    modelo_optimizado, 
    scaler, 
    list(X.columns), 
    metadatos
)
```

#### **üîß 3.2 Funci√≥n de Predicci√≥n**

```python
class PredictorVentas:
    """Clase para hacer predicciones en producci√≥n"""
    
    def __init__(self, ruta_modelo='modelo_ml'):
        self.modelo = joblib.load(f'{ruta_modelo}_modelo.pkl')
        self.scaler = joblib.load(f'{ruta_modelo}_scaler.pkl')
        
        with open(f'{ruta_modelo}_columnas.json', 'r') as f:
            self.columnas_esperadas = json.load(f)
        
        with open(f'{ruta_modelo}_metadatos.json', 'r') as f:
            self.metadatos = json.load(f)
    
    def preprocesar_datos(self, datos_cliente):
        """Preprocesar datos de un cliente individual"""
        
        # Convertir a DataFrame
        df_cliente = pd.DataFrame([datos_cliente])
        
        # Aplicar mismas transformaciones que en entrenamiento
        df_cliente['genero'] = df_cliente['genero'].map({
            'M': 'Masculino', 'masculino': 'Masculino',
            'F': 'Femenino', 'femenino': 'Femenino'
        })
        
        df_cliente['ciudad'] = df_cliente['ciudad'].map({
            'BA': 'Buenos Aires', 'Cordoba': 'C√≥rdoba'
        })
        
        # One-hot encoding para ciudad
        ciudad_encoded = pd.get_dummies(df_cliente['ciudad'], prefix='ciudad')
        df_cliente = pd.concat([df_cliente, ciudad_encoded], axis=1)
        
        # Label encoding para g√©nero
        le_genero = LabelEncoder()
        df_cliente['genero_encoded'] = le_genero.fit_transform(df_cliente['genero'])
        
        # Normalizar variables num√©ricas
        numeric_columns = ['edad', 'ingresos']
        df_cliente[numeric_columns] = self.scaler.transform(df_cliente[numeric_columns])
        
        # Asegurar que todas las columnas esperadas est√©n presentes
        for col in self.columnas_esperadas:
            if col not in df_cliente.columns:
                df_cliente[col] = 0
        
        # Reordenar columnas
        df_cliente = df_cliente[self.columnas_esperadas]
        
        return df_cliente
    
    def predecir(self, datos_cliente):
        """Hacer predicci√≥n para un cliente"""
        
        # Preprocesar datos
        X_cliente = self.preprocesar_datos(datos_cliente)
        
        # Hacer predicci√≥n
        prediccion = self.modelo.predict(X_cliente)[0]
        
        # Calcular intervalo de confianza (aproximado)
        # Nota: Para un intervalo real necesitar√≠as m√°s informaci√≥n del modelo
        std_error = 0.1 * prediccion  # Aproximaci√≥n simple
        intervalo = (prediccion - 1.96*std_error, prediccion + 1.96*std_error)
        
        return {
            'prediccion': prediccion,
            'intervalo_confianza_95': intervalo,
            'modelo_version': self.metadatos['version']
        }

# Ejemplo de uso en producci√≥n
print("\n=== EJEMPLO DE PREDICCI√ìN EN PRODUCCI√ìN ===")

# Simular datos de un nuevo cliente
nuevo_cliente = {
    'edad': 28,
    'ingresos': 45000,
    'genero': 'F',
    'ciudad': 'Buenos Aires',
    'satisfaccion': 4
}

# Crear predictor
predictor = PredictorVentas()

# Hacer predicci√≥n
resultado = predictor.predecir(nuevo_cliente)

print(f"Cliente: {nuevo_cliente}")
print(f"Predicci√≥n de ventas: ${resultado['prediccion']:.2f}")
print(f"Intervalo 95%: ${resultado['intervalo_confianza_95'][0]:.2f} - ${resultado['intervalo_confianza_95'][1]:.2f}")
```

#### **üìä 3.3 Monitoreo del Modelo**

```python
def monitorear_rendimiento_modelo(y_real, y_pred, umbral_drift=0.1):
    """Funci√≥n para monitorear el rendimiento del modelo en producci√≥n"""
    
    # Calcular m√©tricas actuales
    r2_actual = r2_score(y_real, y_pred)
    rmse_actual = np.sqrt(mean_squared_error(y_real, y_pred))
    
    # Detectar drift (cambio en la distribuci√≥n)
    # Comparar con m√©tricas de referencia (del entrenamiento)
    r2_referencia = 0.85  # Valor de referencia del entrenamiento
    drift_detectado = abs(r2_actual - r2_referencia) > umbral_drift
    
    # Generar alerta si hay drift
    if drift_detectado:
        print(f"‚ö†Ô∏è ALERTA: Drift detectado en el modelo!")
        print(f"R¬≤ actual: {r2_actual:.3f}")
        print(f"R¬≤ referencia: {r2_referencia:.3f}")
        print(f"Diferencia: {abs(r2_actual - r2_referencia):.3f}")
    
    return {
        'r2_actual': r2_actual,
        'rmse_actual': rmse_actual,
        'drift_detectado': drift_detectado,
        'necesita_retrenamiento': drift_detectado
    }

# Ejemplo de monitoreo
print("\n=== MONITOREO DEL MODELO ===")
estado_modelo = monitorear_rendimiento_modelo(y_test, predicciones_optimizado)

print(f"Estado del modelo:")
for key, value in estado_modelo.items():
    print(f"  {key}: {value}")
```

---

### üéØ **Resumen de la Implementaci√≥n Pr√°ctica**

#### **‚úÖ Checklist de Implementaci√≥n Exitosa**

1. **Preparaci√≥n de Datos** ‚úÖ
   - [ ] Limpieza de valores faltantes
   - [ ] Estandarizaci√≥n de categor√≠as
   - [ ] Manejo de outliers
   - [ ] Normalizaci√≥n/estandarizaci√≥n
   - [ ] Codificaci√≥n de variables categ√≥ricas

2. **Validaci√≥n del Modelo** ‚úÖ
   - [ ] Divisi√≥n adecuada de datos
   - [ ] Validaci√≥n cruzada
   - [ ] M√©tricas de evaluaci√≥n apropiadas
   - [ ] Ajuste de hiperpar√°metros
   - [ ] An√°lisis de residuos

3. **Despliegue** ‚úÖ
   - [ ] Serializaci√≥n del modelo
   - [ ] Funci√≥n de predicci√≥n robusta
   - [ ] Sistema de monitoreo
   - [ ] Documentaci√≥n de metadatos

#### **üö® Errores Comunes a Evitar**

1. **Data Leakage**: No usar informaci√≥n del futuro para predecir el pasado
2. **Overfitting**: Validar siempre en datos no vistos
3. **Falta de monitoreo**: Los modelos se degradan con el tiempo
4. **Ignorar el contexto de negocio**: Las m√©tricas t√©cnicas no siempre reflejan el valor real

#### **üìà Pr√≥ximos Pasos**

1. **A/B Testing**: Comparar el modelo contra m√©todos actuales
2. **Feedback Loop**: Incorporar feedback de usuarios
3. **Retrenamiento Autom√°tico**: Sistema para actualizar el modelo peri√≥dicamente
4. **Escalabilidad**: Preparar para mayor volumen de datos

---

### Apartado especial para lo que es Feature Selection

### üßπ 1. M√©todos de Filtro (*Filter Methods*)

Piensa en estos como un primer filtro r√°pido.

* **C√≥mo funcionan:** Eval√∫an cada caracter√≠stica de forma individual usando pruebas estad√≠sticas que miden su relaci√≥n con la variable objetivo (la etiqueta).
* **Velocidad:** Muy r√°pidos, ya que no requieren entrenar modelos.
* **Ejemplos:**

  * **Prueba de Chi-cuadrado** (para variables categ√≥ricas)
  * **Prueba ANOVA**
  * **Informaci√≥n mutua**
  * **Coeficiente de correlaci√≥n**

üìå **Ventajas:** R√°pidos y no dependen del modelo
üìå **Desventajas:** No consideran las interacciones entre caracter√≠sticas

---

### üß™ 2. M√©todos Envolventes (*Wrapper Methods*)

Estos son como probar diferentes combinaciones de ropa para ver cu√°l te queda mejor.

* **C√≥mo funcionan:** Prueban m√∫ltiples subconjuntos de caracter√≠sticas entrenando modelos, y eligen el subconjunto que da el mejor rendimiento.
* **Velocidad:** Lentos, porque entrenan muchos modelos.
* **Ejemplos:**

  * **Eliminaci√≥n recursiva de caracter√≠sticas (RFE)**
  * **Selecci√≥n hacia adelante**
  * **Eliminaci√≥n hacia atr√°s**

üìå **Ventajas:** Tienen en cuenta interacciones entre caracter√≠sticas
üìå **Desventajas:** Muy costosos en tiempo y recursos computacionales

---

### ‚öôÔ∏è 3. M√©todos Embebidos (*Embedded Methods*)

Estos seleccionan caracter√≠sticas **mientras entrenan** el modelo.

* **C√≥mo funcionan:** La selecci√≥n ocurre como parte del proceso de entrenamiento.
* **Ejemplos:**

  * **Lasso (regularizaci√≥n L1)** ‚Äì reduce coeficientes a cero
  * **√Årboles de decisi√≥n / Bosques aleatorios** ‚Äì calculan importancia de cada variable
  * **Elastic Net** ‚Äì combina L1 y L2

üìå **Ventajas:** M√°s eficientes y consideran interacciones
üìå **Desventajas:** Dependientes del modelo utilizado

---

### Tabla Resumen:

| Tipo de m√©todo | ¬øUsa modelo? | ¬øConsidera interacciones? | Velocidad | Ejemplos                      |
| -------------- | ------------ | ------------------------- | --------- | ----------------------------- |
| Filtro         | ‚ùå No         | ‚ùå No                      | üöÄ R√°pido | Chi-cuadrado, correlaci√≥n     |
| Envolvente     | ‚úÖ S√≠         | ‚úÖ S√≠                      | üê¢ Lento  | RFE, selecci√≥n hacia adelante |
| Embebido       | ‚úÖ S√≠         | ‚úÖ S√≠                      | ‚ö° Medio   | Lasso, √°rboles de decisi√≥n    |


## Recomendaciones para aprender:
¬°Claro! Vamos a ver cada uno de estos m√©todos de selecci√≥n de caracter√≠sticas supervisadas con **ejemplos, ventajas y desventajas**, relacion√°ndolos con los tipos que ya vimos: filtro, envolvente o embebido.

---

### üìâ 1. **Variance Threshold**

üîπ **Tipo:** Filtro
üîπ **Qu√© hace:** Elimina caracter√≠sticas con **baja varianza**, es decir, que no cambian mucho entre ejemplos (por ejemplo, una columna donde casi todos los valores son iguales).

#### ‚úÖ Ventajas:

* Muy simple y r√°pido de aplicar
* No necesita etiquetas (tambi√©n se puede usar en problemas no supervisados)

#### ‚ùå Desventajas:

* No considera la relaci√≥n con la variable objetivo
* Puede eliminar caracter√≠sticas √∫tiles si tienen poca varianza pero alta relevancia

#### üß™ Ejemplo en Python:

```python
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)
X_reduced = selector.fit_transform(X)
```

---

### ‚≠ê 2. **SelectKBest**

üîπ **Tipo:** Filtro
üîπ **Qu√© hace:** Selecciona las **k mejores caracter√≠sticas** seg√∫n una m√©trica estad√≠stica que mide la relaci√≥n con la variable objetivo (como ANOVA, chi-cuadrado, etc.).

#### ‚úÖ Ventajas:

* F√°cil de interpretar y aplicar
* R√°pido y eficiente
* Permite usar distintas m√©tricas

#### ‚ùå Desventajas:

* No considera interacciones entre variables
* Necesita que t√∫ elijas el valor de *k* (el n√∫mero de caracter√≠sticas a conservar)

#### üß™ Ejemplo en Python:

```python
from sklearn.feature_selection import SelectKBest, f_classif
selector = SelectKBest(score_func=f_classif, k=10)
X_kbest = selector.fit_transform(X, y)
```

---

### üîÑ 3. **RFE (Recursive Feature Elimination)**

üîπ **Tipo:** Envolvente
üîπ **Qu√© hace:** Usa un modelo (como una regresi√≥n o un SVM) para eliminar **recursivamente** las caracter√≠sticas menos importantes hasta quedarse con las m√°s relevantes.

#### ‚úÖ Ventajas:

* Considera interacciones entre variables
* Da muy buenos resultados si se elige bien el modelo base

#### ‚ùå Desventajas:

* Lento, especialmente con muchos datos o caracter√≠sticas
* Depende fuertemente del modelo usado

#### üß™ Ejemplo en Python:

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
rfe = RFE(estimator=model, n_features_to_select=5)
X_rfe = rfe.fit_transform(X, y)
```

---

### üå≥ 4. **Boruta**

üîπ **Tipo:** Envolvente (basado en √°rboles, como Random Forest)
üîπ **Qu√© hace:** Crea versiones "aleatorias" de las caracter√≠sticas y las compara con las reales. Solo conserva las que son mejores que las versiones aleatorias (shadow features).

#### ‚úÖ Ventajas:

* Muy robusto y preciso
* Tiene en cuenta interacciones y relaciones no lineales
* Funciona bien con datos complejos

#### ‚ùå Desventajas:

* Bastante lento (usa muchos modelos de Random Forest)
* No est√° en `scikit-learn` directamente (hay que usar una librer√≠a aparte como `boruta_py`)

#### üß™ Ejemplo con `boruta_py`:

```python
from boruta import BorutaPy
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)
boruta_selector = BorutaPy(estimator=model, n_estimators='auto', random_state=42)
boruta_selector.fit(X.values, y.values)
```

---

### üîç Resumen Comparativo

| M√©todo             | Tipo       | Interacci√≥n | Velocidad    | Modelo requerido | Ventaja principal                    |
| ------------------ | ---------- | ----------- | ------------ | ---------------- | ------------------------------------ |
| Variance Threshold | Filtro     | ‚ùå No        | üöÄ R√°pido    | ‚ùå No             | Muy simple y r√°pido                  |
| SelectKBest        | Filtro     | ‚ùå No        | üöÄ R√°pido    | ‚ùå No             | M√©tricas estad√≠sticas supervisadas   |
| RFE                | Envolvente | ‚úÖ S√≠        | üê¢ Lento     | ‚úÖ S√≠             | Preciso si el modelo es adecuado     |
| Boruta             | Envolvente | ‚úÖ S√≠        | üêå Muy lento | ‚úÖ S√≠             | Robusto frente a ruido y redundancia |

