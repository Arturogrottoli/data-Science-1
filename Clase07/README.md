## ğŸ“š Repaso Clase Anterior (Clase 6)

### TeorÃ­a Fundamental del EDA y Preprocesamiento

En la **Clase 6** establecimos las bases fundamentales para el anÃ¡lisis de datos que ahora aplicaremos en **Machine Learning**:

#### ğŸ” **AnÃ¡lisis Exploratorio de Datos (EDA)**
- **FilosofÃ­a**: Acercarse a los datos sin prejuicios para descubrir patrones inesperados
- **Objetivo**: Entender la estructura de los datos antes de aplicar modelos predictivos
- **Herramientas**: EstadÃ­stica descriptiva + visualizaciÃ³n de datos

#### ğŸ“Š **EstadÃ­stica Descriptiva**
- **Medidas de tendencia central**: Media, mediana, moda
- **Medidas de dispersiÃ³n**: Varianza, desviaciÃ³n estÃ¡ndar, IQR
- **Distribuciones**: Normal, uniforme, y su visualizaciÃ³n con histogramas
- **CorrelaciÃ³n**: RelaciÃ³n entre variables (importante: correlaciÃ³n â‰  causalidad)

#### ğŸ§¹ **Preprocesamiento de Datos**
- **Limpieza**: Manejo de valores faltantes y outliers
- **TransformaciÃ³n**: NormalizaciÃ³n, codificaciÃ³n de variables categÃ³ricas
- **IntegraciÃ³n**: Combinar datos de mÃºltiples fuentes
- **ReducciÃ³n**: PCA para simplificar la dimensionalidad

---

### ğŸ’¡ Ejemplo 1: AnÃ¡lisis EstadÃ­stico Descriptivo

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Datos de ventas mensuales de una empresa
np.random.seed(42)
ventas = np.random.normal(50000, 12000, 24)  # 2 aÃ±os de datos
ventas = np.append(ventas, [150000, -5000])  # outliers

df_ventas = pd.DataFrame({"Ventas_Mensuales": ventas})

# Medidas descriptivas
print("=== ESTADÃSTICAS DESCRIPTIVAS ===")
print(f"Media: {df_ventas['Ventas_Mensuales'].mean():.2f}")
print(f"Mediana: {df_ventas['Ventas_Mensuales'].median():.2f}")
print(f"DesviaciÃ³n estÃ¡ndar: {df_ventas['Ventas_Mensuales'].std():.2f}")

# VisualizaciÃ³n
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
sns.histplot(df_ventas["Ventas_Mensuales"], kde=True, bins=15)
plt.title("DistribuciÃ³n de Ventas Mensuales")

plt.subplot(1,2,2)
sns.boxplot(x=df_ventas["Ventas_Mensuales"])
plt.title("Boxplot - DetecciÃ³n de Outliers")
plt.show()
```

**ğŸ¯ Objetivo**: Identificar patrones en los datos de ventas y detectar valores atÃ­picos que podrÃ­an afectar modelos predictivos.

---

### ğŸ’¡ Ejemplo 2: Preprocesamiento con PCA

```python
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Dataset con mÃºltiples variables correlacionadas
np.random.seed(42)
X1 = np.random.normal(100, 20, 100)
X2 = X1 * 0.8 + np.random.normal(0, 5, 100)  # correlacionada con X1
X3 = np.random.normal(50, 10, 100)           # independiente

df_features = pd.DataFrame({
    "Ingresos": X1,
    "Gastos": X2, 
    "Ahorros": X3
})

print("=== CORRELACIONES ORIGINALES ===")
print(df_features.corr())

# EstandarizaciÃ³n (importante para PCA)
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_features)

# Aplicar PCA
pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_scaled)

print(f"\n=== VARIANZA EXPLICADA ===")
print(f"PC1: {pca.explained_variance_ratio_[0]:.2%}")
print(f"PC2: {pca.explained_variance_ratio_[1]:.2%}")

# VisualizaciÃ³n
df_pca = pd.DataFrame(pca_result, columns=["PC1", "PC2"])
plt.figure(figsize=(6,6))
sns.scatterplot(x="PC1", y="PC2", data=df_pca, s=60)
plt.title("Datos transformados con PCA")
plt.show()
```

**ğŸ¯ Objetivo**: Reducir la dimensionalidad eliminando redundancia entre variables correlacionadas, preparando datos mÃ¡s limpios para algoritmos de ML.

---

### ğŸ”— **ConexiÃ³n con Machine Learning**

Los conceptos de la **Clase 6** son **fundamentales** para el Ã©xito en ML:

- **EDA** â†’ Nos ayuda a entender quÃ© variables son relevantes para predecir
- **EstadÃ­stica descriptiva** â†’ Identifica distribuciones y relaciones que los algoritmos pueden aprovechar
- **Preprocesamiento** â†’ Asegura que los datos estÃ©n limpios y listos para entrenar modelos
- **PCA** â†’ Reduce complejidad y mejora el rendimiento de los algoritmos

---

### **Â¿QuÃ© es Machine Learning?**  
**Machine Learning (ML)** o *Aprendizaje AutomÃ¡tico* es una rama de la **Inteligencia Artificial (IA)** que se centra en desarrollar algoritmos y modelos capaces de aprender patrones a partir de datos, **sin ser programados explÃ­citamente**. En lugar de seguir reglas predefinidas, los sistemas de ML **mejoran su desempeÃ±o con la experiencia** (datos).  

Su objetivo principal es **generalizar** a partir de ejemplos para realizar predicciones o tomar decisiones en situaciones nuevas.

---

### **Particularidades y CaracterÃ­sticas Principales**  

1. **Aprendizaje basado en datos**  
   - ML requiere **datos histÃ³ricos o de entrenamiento** para identificar patrones y relaciones.  
   - A diferencia de la programaciÃ³n tradicional (donde las reglas son fijas), en ML **el modelo "aprende" de los datos**.  

2. **Capacidad predictiva y adaptativa**  
   - Los modelos de ML pueden **predecir resultados futuros** (ej.: ventas, fallos en equipos) o **clasificar informaciÃ³n** (ej.: spam/no spam).  
   - Algunos sistemas se adaptan a cambios en los datos (ej.: recomendaciones en tiempo real en Netflix o Amazon).  

3. **Tipos principales de aprendizaje**  
   - **Supervisado**: Usa datos etiquetados (ej.: predecir precios de casas basado en ejemplos pasados).  
   - **No supervisado**: Encuentra patrones en datos sin etiquetas (ej.: agrupaciÃ³n de clientes por comportamiento).  
   - **Por refuerzo**: Aprende mediante prueba/error y recompensas (ej.: robots que aprenden a caminar).  

4. **AutomatizaciÃ³n y escalabilidad**  
   - ML permite automatizar tareas complejas (ej.: diagnÃ³stico mÃ©dico, detecciÃ³n de fraudes).  
   - Escala bien con grandes volÃºmenes de datos (*Big Data*).  

5. **Ã‰nfasis en la evaluaciÃ³n**  
   - Los modelos se validan con mÃ©tricas como **precisiÃ³n, recall o error cuadrÃ¡tico medio** para garantizar su fiabilidad.  

6. **Dependencia de la calidad de los datos**  
   - El rendimiento del ML estÃ¡ ligado a la **calidad, cantidad y representatividad** de los datos.  
   - Problemas como *sesgos* o *datos incompletos* afectan los resultados.  

7. **Uso de algoritmos diversos**  
   - Desde mÃ©todos clÃ¡sicos (*regresiÃ³n lineal, Ã¡rboles de decisiÃ³n*) hasta tÃ©cnicas avanzadas (*redes neuronales, deep learning*).  

---

### **Diferencia clave con Data Science**  
Mientras **Data Science** abarca todo el ciclo de anÃ¡lisis de datos (limpieza, visualizaciÃ³n, estadÃ­stica, etc.), **ML es una herramienta dentro de DS** enfocada especÃ­ficamente en **automatizar el aprendizaje** para predicciÃ³n o toma de decisiones.  

**Ejemplo prÃ¡ctico**:  
- Un modelo de ML podrÃ­a predecir el *churn* (abandono) de clientes en una empresa, mientras que un data scientist tambiÃ©n analizarÃ­a *por quÃ©* ocurre y cÃ³mo comunicarlo.  

En resumen, **Machine Learning es la tecnologÃ­a que permite a las mÃ¡quinas "aprender" de los datos para resolver problemas complejos de manera autÃ³noma o semiautÃ³noma**. 

---


### _Si quisiÃ©ramos resolver un problema donde tenemos informaciÃ³n georeferenciada de clientes Â¿cÃ³mo podrÃ­amos utilizar el ML para incrementar las ventas de un producto?_

Para incrementar las ventas de un producto utilizando **Machine Learning (ML)** con datos georreferenciados de clientes, puedes aplicar diversas estrategias basadas en anÃ¡lisis espacial y modelos predictivos. AquÃ­ te detallo un enfoque estructurado:

---

### **1. AnÃ¡lisis Exploratorio de Datos (EDA) Geoespacial**  
- **VisualizaciÃ³n de datos**:  
  - Usar mapas de calor (*heatmaps*) para identificar zonas con alta concentraciÃ³n de clientes o ventas.  
  - Segmentar por zonas geogrÃ¡ficas (barrios, ciudades, cÃ³digos postales).  
- **DetecciÃ³n de patrones**:  
  - Correlacionar ubicaciÃ³n con variables como ingresos, edad, clima o proximidad a puntos de interÃ©s (ej.: centros comerciales).  

**Herramientas**: Python (`geopandas`, `folium`), Power BI (integrado con Mapas).  

---

### **2. SegmentaciÃ³n de Clientes por UbicaciÃ³n**  
- **Clustering no supervisado** (ej.: *K-means* o *DBSCAN*) para agrupar clientes con caracterÃ­sticas similares:  
  - Crear clusters basados en:  
    - UbicaciÃ³n (coordenadas).  
    - Comportamiento de compra + datos demogrÃ¡ficos locales.  
  - Ejemplo: Identificar "zonas de alto potencial" con clientes similares a los que ya compran el producto.  

---

### **3. Modelos Predictivos para Ventas**  
- **Aprendizaje supervisado** (ej.: *Random Forest, XGBoost*) para predecir:  
  - **PropensiÃ³n de compra**: QuÃ© clientes (o zonas) tienen mayor probabilidad de comprar el producto.  
  - **Demanda geogrÃ¡fica**: DÃ³nde habrÃ¡ mayor demanda en funciÃ³n de variables temporales (ej.: festividades locales).  
- **Variables de entrada**:  
  - Datos geogrÃ¡ficos (latitud, longitud, distancia a tiendas).  
  - Datos socioeconÃ³micos de la zona (nivel de ingresos, densidad poblacional).  
  - Historial de ventas en la regiÃ³n.  

---

### **4. Recomendaciones Geo-Personalizadas**  
- **Sistemas de recomendaciÃ³n** con filtrado colaborativo o basado en contenido:  
  - Sugerir productos populares en la zona (ej.: "En tu barrio, otros clientes compran X").  
  - Adaptar promociones segÃºn el perfil geogrÃ¡fico (ej.: descuentos en zonas con menor penetraciÃ³n).  

---

### **5. OptimizaciÃ³n LogÃ­stica y UbicaciÃ³n de Puntos de Venta**  
- **Modelos de ubicaciÃ³n Ã³ptima**:  
  - Usar *algoritmos de optimizaciÃ³n* (ej.: *p-median*) para decidir dÃ³nde abrir nuevas tiendas o colocar anuncios.  
  - Ejemplo: "Las zonas con radio de 5 km sin cobertura tienen un 20% de clientes potenciales sin atender".  

---

### **6. CampaÃ±as de Marketing Dirigido**  
- **Geo-targeting publicitario**:  
  - Entrenar modelos para identificar zonas donde campaÃ±as especÃ­ficas (ej.: SMS, redes sociales) tendrÃ¡n mayor ROI.  
  - Ejemplo: Anuncios en Facebook Ads para un radio de 10 km alrededor de tiendas con stock alto.  

---

### **7. Ejemplo PrÃ¡ctico**  
**Problema**: Una cadena de cafeterÃ­as quiere aumentar ventas en Ciudad de MÃ©xico.  
**SoluciÃ³n con ML**:  
1. Agrupa clientes por colonia usando *DBSCAN*.  
2. Entrena un modelo para predecir ventas segÃºn:  
   - Proximidad a estaciones de metro.  
   - Nivel socioeconÃ³mico (datos del INEGI).  
3. Descubre que las colonias *Roma* y *Condesa* tienen alta demanda los fines de semana.  
4. Lanza promociones "2x1" los sÃ¡bados en esas zonas via WhatsApp.  

---

### **Beneficios**  
- **ReducciÃ³n de costos**: Enfoque en zonas de alto impacto.  
- **PersonalizaciÃ³n**: Ofertas relevantes por ubicaciÃ³n.  
- **Escalabilidad**: Aplicable a mÃºltiples regiones o productos.  

# ML

*  ## [Aprendizaje Supervizado](clase_7/aprendizaje-supervisado.md)
* ## [Aprendizaje no Supervisado](clase_7/aprendizaje-no-supervisado.md)

* ## [Paso a paso para un ML funcional](clase_7/paso-a-pas.md)


---

## ğŸš€ ImplementaciÃ³n PrÃ¡ctica de Machine Learning

### ğŸ“‹ **7.6 ImplementaciÃ³n PrÃ¡ctica**

La implementaciÃ³n prÃ¡ctica de Machine Learning es el proceso que transforma la teorÃ­a en soluciones reales. Esta fase es crucial porque determina el Ã©xito o fracaso de un proyecto de ML en el mundo real.

---

### ğŸ§¹ **1. PreparaciÃ³n de Datos**

La preparaciÃ³n de datos es el **paso mÃ¡s crÃ­tico** en cualquier proyecto de ML. Se estima que el 80% del tiempo en un proyecto de ML se dedica a la preparaciÃ³n y limpieza de datos.

#### **ğŸ¯ FilosofÃ­a de la PreparaciÃ³n de Datos**
- **Principio**: "Garbage in, garbage out" - Si los datos de entrada son de mala calidad, el modelo serÃ¡ inÃºtil
- **Objetivo**: Transformar datos brutos en un formato limpio y estructurado
- **Enfoque**: Iterativo y sistemÃ¡tico

#### **ğŸ“Š Ejemplo PrÃ¡ctico: Dataset de Ventas de Tienda**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer

# Cargar datos (simulando un dataset real con problemas tÃ­picos)
np.random.seed(42)
n_samples = 1000

# Crear datos con problemas reales
data = {
    'edad': np.random.normal(35, 10, n_samples),
    'ingresos': np.random.lognormal(10, 0.5, n_samples),
    'genero': np.random.choice(['M', 'F', 'masculino', 'femenino', None], n_samples, p=[0.4, 0.3, 0.1, 0.1, 0.1]),
    'ciudad': np.random.choice(['Buenos Aires', 'CÃ³rdoba', 'Mendoza', 'BA', 'Cordoba', None], n_samples),
    'ventas_mes': np.random.gamma(2, 1000, n_samples),
    'satisfaccion': np.random.choice([1, 2, 3, 4, 5, None], n_samples, p=[0.1, 0.15, 0.2, 0.3, 0.2, 0.05])
}

df = pd.DataFrame(data)

# Agregar algunos outliers
df.loc[df.index[:50], 'ventas_mes'] *= 10
df.loc[df.index[50:60], 'edad'] = 200  # Valores imposibles

print("=== ESTADO INICIAL DE LOS DATOS ===")
print(f"Forma del dataset: {df.shape}")
print(f"\nValores faltantes por columna:")
print(df.isnull().sum())
print(f"\nTipos de datos:")
print(df.dtypes)
```

#### **ğŸ”§ 1.1 Limpieza de Datos**

```python
# 1. Manejo de Valores Faltantes
print("\n=== LIMPIEZA DE DATOS ===")

# Estrategia para valores faltantes
def limpiar_valores_faltantes(df):
    df_limpio = df.copy()
    
    # Para variables numÃ©ricas: usar la mediana
    numeric_columns = ['edad', 'ingresos', 'ventas_mes']
    for col in numeric_columns:
        if df_limpio[col].isnull().sum() > 0:
            median_value = df_limpio[col].median()
            df_limpio[col].fillna(median_value, inplace=True)
            print(f"Imputado {col} con mediana: {median_value:.2f}")
    
    # Para variables categÃ³ricas: usar la moda
    categorical_columns = ['genero', 'ciudad', 'satisfaccion']
    for col in categorical_columns:
        if df_limpio[col].isnull().sum() > 0:
            mode_value = df_limpio[col].mode()[0]
            df_limpio[col].fillna(mode_value, inplace=True)
            print(f"Imputado {col} con moda: {mode_value}")
    
    return df_limpio

df_limpio = limpiar_valores_faltantes(df)

# 2. CorrecciÃ³n de Inconsistencias
def estandarizar_categorias(df):
    df_estandarizado = df.copy()
    
    # Estandarizar gÃ©nero
    df_estandarizado['genero'] = df_estandarizado['genero'].map({
        'M': 'Masculino', 'masculino': 'Masculino',
        'F': 'Femenino', 'femenino': 'Femenino'
    })
    
    # Estandarizar ciudades
    df_estandarizado['ciudad'] = df_estandarizado['ciudad'].map({
        'BA': 'Buenos Aires', 'Cordoba': 'CÃ³rdoba'
    })
    
    return df_estandarizado

df_estandarizado = estandarizar_categorias(df_limpio)

# 3. Manejo de Outliers
def detectar_y_tratar_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    print(f"Outliers detectados en {column}: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)")
    
    # Estrategia: Capar los valores extremos
    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)
    return df

# Tratar outliers en ventas_mes y edad
df_final = detectar_y_tratar_outliers(df_estandarizado, 'ventas_mes')
df_final = detectar_y_tratar_outliers(df_final, 'edad')

print(f"\nDataset final: {df_final.shape}")
print(f"Valores faltantes restantes: {df_final.isnull().sum().sum()}")
```

#### **ğŸ”„ 1.2 TransformaciÃ³n de Datos**

```python
# TransformaciÃ³n de variables
def transformar_datos(df):
    df_transformado = df.copy()
    
    # 1. NormalizaciÃ³n/EstandarizaciÃ³n
    scaler = StandardScaler()
    numeric_columns = ['edad', 'ingresos', 'ventas_mes']
    df_transformado[numeric_columns] = scaler.fit_transform(df_transformado[numeric_columns])
    
    # 2. CodificaciÃ³n de variables categÃ³ricas
    # One-hot encoding para ciudad
    ciudad_encoded = pd.get_dummies(df_transformado['ciudad'], prefix='ciudad')
    df_transformado = pd.concat([df_transformado, ciudad_encoded], axis=1)
    
    # Label encoding para gÃ©nero y satisfacciÃ³n
    le_genero = LabelEncoder()
    df_transformado['genero_encoded'] = le_genero.fit_transform(df_transformado['genero'])
    
    # 3. TransformaciÃ³n logarÃ­tmica (si es necesario)
    # Para variables con sesgo positivo
    if df['ingresos'].skew() > 1:
        df_transformado['ingresos_log'] = np.log1p(df['ingresos'])
    
    return df_transformado

df_transformado = transformar_datos(df_final)

print("\n=== DATOS TRANSFORMADOS ===")
print(f"Forma final: {df_transformado.shape}")
print(f"Columnas: {list(df_transformado.columns)}")
```

---

### âœ… **2. ValidaciÃ³n del Modelo**

La validaciÃ³n es esencial para asegurar que nuestro modelo funcionarÃ¡ en datos reales.

#### **ğŸ“Š 2.1 DivisiÃ³n del Conjunto de Datos**

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Preparar datos para modelado
# Asumimos que queremos predecir 'ventas_mes' basado en otras variables
X = df_transformado.drop(['ventas_mes', 'genero', 'ciudad'], axis=1)
y = df['ventas_mes']  # Usar valores originales, no normalizados

# DivisiÃ³n 70-20-10: Train-Validation-Test
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.22, random_state=42)

print(f"Conjunto de entrenamiento: {X_train.shape[0]} muestras")
print(f"Conjunto de validaciÃ³n: {X_val.shape[0]} muestras")
print(f"Conjunto de prueba: {X_test.shape[0]} muestras")
```

#### **ğŸ”„ 2.2 ValidaciÃ³n Cruzada**

```python
from sklearn.model_selection import cross_val_score, KFold

def evaluar_modelo_con_cv(modelo, X, y, cv=5):
    """FunciÃ³n para evaluar modelo con validaciÃ³n cruzada"""
    
    # Configurar validaciÃ³n cruzada
    kf = KFold(n_splits=cv, shuffle=True, random_state=42)
    
    # MÃ©tricas a evaluar
    mse_scores = cross_val_score(modelo, X, y, cv=kf, scoring='neg_mean_squared_error')
    r2_scores = cross_val_score(modelo, X, y, cv=kf, scoring='r2')
    
    return {
        'MSE_mean': -mse_scores.mean(),
        'MSE_std': mse_scores.std(),
        'R2_mean': r2_scores.mean(),
        'R2_std': r2_scores.std()
    }

# Entrenar modelo
modelo = RandomForestRegressor(n_estimators=100, random_state=42)
modelo.fit(X_train, y_train)

# EvaluaciÃ³n con validaciÃ³n cruzada
cv_results = evaluar_modelo_con_cv(modelo, X_train, y_train)

print("\n=== RESULTADOS DE VALIDACIÃ“N CRUZADA ===")
print(f"MSE promedio: {cv_results['MSE_mean']:.2f} Â± {cv_results['MSE_std']:.2f}")
print(f"RÂ² promedio: {cv_results['R2_mean']:.3f} Â± {cv_results['R2_std']:.3f}")
```

#### **ğŸ“ˆ 2.3 MÃ©tricas de EvaluaciÃ³n**

```python
def evaluar_modelo_completo(modelo, X_test, y_test):
    """EvaluaciÃ³n completa del modelo"""
    
    # Predicciones
    y_pred = modelo.predict(X_test)
    
    # MÃ©tricas de regresiÃ³n
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    
    # Error absoluto medio
    mae = np.mean(np.abs(y_test - y_pred))
    
    # Error porcentual absoluto medio
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    
    resultados = {
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'RÂ²': r2,
        'MAPE': mape
    }
    
    return resultados, y_pred

# Evaluar en conjunto de prueba
resultados, predicciones = evaluar_modelo_completo(modelo, X_test, y_test)

print("\n=== MÃ‰TRICAS EN CONJUNTO DE PRUEBA ===")
for metrica, valor in resultados.items():
    print(f"{metrica}: {valor:.3f}")

# VisualizaciÃ³n de resultados
plt.figure(figsize=(15, 5))

# 1. Predicciones vs Valores Reales
plt.subplot(1, 3, 1)
plt.scatter(y_test, predicciones, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Valores Reales')
plt.ylabel('Predicciones')
plt.title(f'Predicciones vs Reales\nRÂ² = {resultados["RÂ²"]:.3f}')

# 2. Residuos
plt.subplot(1, 3, 2)
residuos = y_test - predicciones
plt.scatter(predicciones, residuos, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicciones')
plt.ylabel('Residuos')
plt.title('AnÃ¡lisis de Residuos')

# 3. DistribuciÃ³n de errores
plt.subplot(1, 3, 3)
plt.hist(residuos, bins=30, alpha=0.7, edgecolor='black')
plt.xlabel('Residuos')
plt.ylabel('Frecuencia')
plt.title('DistribuciÃ³n de Errores')

plt.tight_layout()
plt.show()
```

#### **âš™ï¸ 2.4 Ajuste de HiperparÃ¡metros**

```python
from sklearn.model_selection import GridSearchCV

def ajustar_hiperparametros(modelo, X_train, y_train, param_grid, cv=3):
    """Ajuste de hiperparÃ¡metros con GridSearch"""
    
    grid_search = GridSearchCV(
        modelo, 
        param_grid, 
        cv=cv, 
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    return grid_search

# Definir grid de parÃ¡metros para Random Forest
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

print("\n=== AJUSTE DE HIPERPARÃMETROS ===")
print("Buscando mejores parÃ¡metros...")

# Ajustar hiperparÃ¡metros
grid_search = ajustar_hiperparametros(
    RandomForestRegressor(random_state=42), 
    X_train, y_train, 
    param_grid
)

print(f"\nMejores parÃ¡metros: {grid_search.best_params_}")
print(f"Mejor score (negativo MSE): {grid_search.best_score_:.2f}")

# Evaluar modelo optimizado
modelo_optimizado = grid_search.best_estimator_
resultados_optimizado, predicciones_optimizado = evaluar_modelo_completo(modelo_optimizado, X_test, y_test)

print("\n=== COMPARACIÃ“N DE MODELOS ===")
print("Modelo Original vs Optimizado:")
print(f"RÂ² Original: {resultados['RÂ²']:.3f}")
print(f"RÂ² Optimizado: {resultados_optimizado['RÂ²']:.3f}")
print(f"Mejora: {(resultados_optimizado['RÂ²'] - resultados['RÂ²'])*100:.1f}%")
```

---

### ğŸš€ **3. Despliegue del Modelo**

El despliegue es donde el modelo pasa del laboratorio al mundo real.

#### **ğŸ“¦ 3.1 PreparaciÃ³n para ProducciÃ³n**

```python
import joblib
import json

def guardar_modelo_completo(modelo, scaler, columnas, metadatos, ruta_base='modelo_ml'):
    """Guardar modelo y metadatos para producciÃ³n"""
    
    # 1. Guardar modelo
    joblib.dump(modelo, f'{ruta_base}_modelo.pkl')
    
    # 2. Guardar scaler
    joblib.dump(scaler, f'{ruta_base}_scaler.pkl')
    
    # 3. Guardar informaciÃ³n de columnas
    with open(f'{ruta_base}_columnas.json', 'w') as f:
        json.dump(columnas, f)
    
    # 4. Guardar metadatos
    with open(f'{ruta_base}_metadatos.json', 'w') as f:
        json.dump(metadatos, f)
    
    print(f"Modelo guardado en: {ruta_base}_*")

# Preparar metadatos
metadatos = {
    'version': '1.0',
    'fecha_entrenamiento': '2024-01-15',
    'algoritmo': 'RandomForestRegressor',
    'metricas_entrenamiento': resultados_optimizado,
    'caracteristicas': list(X.columns),
    'descripcion': 'Modelo para predecir ventas mensuales basado en perfil del cliente'
}

# Guardar modelo
guardar_modelo_completo(
    modelo_optimizado, 
    scaler, 
    list(X.columns), 
    metadatos
)
```

#### **ğŸ”§ 3.2 FunciÃ³n de PredicciÃ³n**

```python
class PredictorVentas:
    """Clase para hacer predicciones en producciÃ³n"""
    
    def __init__(self, ruta_modelo='modelo_ml'):
        self.modelo = joblib.load(f'{ruta_modelo}_modelo.pkl')
        self.scaler = joblib.load(f'{ruta_modelo}_scaler.pkl')
        
        with open(f'{ruta_modelo}_columnas.json', 'r') as f:
            self.columnas_esperadas = json.load(f)
        
        with open(f'{ruta_modelo}_metadatos.json', 'r') as f:
            self.metadatos = json.load(f)
    
    def preprocesar_datos(self, datos_cliente):
        """Preprocesar datos de un cliente individual"""
        
        # Convertir a DataFrame
        df_cliente = pd.DataFrame([datos_cliente])
        
        # Aplicar mismas transformaciones que en entrenamiento
        df_cliente['genero'] = df_cliente['genero'].map({
            'M': 'Masculino', 'masculino': 'Masculino',
            'F': 'Femenino', 'femenino': 'Femenino'
        })
        
        df_cliente['ciudad'] = df_cliente['ciudad'].map({
            'BA': 'Buenos Aires', 'Cordoba': 'CÃ³rdoba'
        })
        
        # One-hot encoding para ciudad
        ciudad_encoded = pd.get_dummies(df_cliente['ciudad'], prefix='ciudad')
        df_cliente = pd.concat([df_cliente, ciudad_encoded], axis=1)
        
        # Label encoding para gÃ©nero
        le_genero = LabelEncoder()
        df_cliente['genero_encoded'] = le_genero.fit_transform(df_cliente['genero'])
        
        # Normalizar variables numÃ©ricas
        numeric_columns = ['edad', 'ingresos']
        df_cliente[numeric_columns] = self.scaler.transform(df_cliente[numeric_columns])
        
        # Asegurar que todas las columnas esperadas estÃ©n presentes
        for col in self.columnas_esperadas:
            if col not in df_cliente.columns:
                df_cliente[col] = 0
        
        # Reordenar columnas
        df_cliente = df_cliente[self.columnas_esperadas]
        
        return df_cliente
    
    def predecir(self, datos_cliente):
        """Hacer predicciÃ³n para un cliente"""
        
        # Preprocesar datos
        X_cliente = self.preprocesar_datos(datos_cliente)
        
        # Hacer predicciÃ³n
        prediccion = self.modelo.predict(X_cliente)[0]
        
        # Calcular intervalo de confianza (aproximado)
        # Nota: Para un intervalo real necesitarÃ­as mÃ¡s informaciÃ³n del modelo
        std_error = 0.1 * prediccion  # AproximaciÃ³n simple
        intervalo = (prediccion - 1.96*std_error, prediccion + 1.96*std_error)
        
        return {
            'prediccion': prediccion,
            'intervalo_confianza_95': intervalo,
            'modelo_version': self.metadatos['version']
        }

# Ejemplo de uso en producciÃ³n
print("\n=== EJEMPLO DE PREDICCIÃ“N EN PRODUCCIÃ“N ===")

# Simular datos de un nuevo cliente
nuevo_cliente = {
    'edad': 28,
    'ingresos': 45000,
    'genero': 'F',
    'ciudad': 'Buenos Aires',
    'satisfaccion': 4
}

# Crear predictor
predictor = PredictorVentas()

# Hacer predicciÃ³n
resultado = predictor.predecir(nuevo_cliente)

print(f"Cliente: {nuevo_cliente}")
print(f"PredicciÃ³n de ventas: ${resultado['prediccion']:.2f}")
print(f"Intervalo 95%: ${resultado['intervalo_confianza_95'][0]:.2f} - ${resultado['intervalo_confianza_95'][1]:.2f}")
```

#### **ğŸ“Š 3.3 Monitoreo del Modelo**

```python
def monitorear_rendimiento_modelo(y_real, y_pred, umbral_drift=0.1):
    """FunciÃ³n para monitorear el rendimiento del modelo en producciÃ³n"""
    
    # Calcular mÃ©tricas actuales
    r2_actual = r2_score(y_real, y_pred)
    rmse_actual = np.sqrt(mean_squared_error(y_real, y_pred))
    
    # Detectar drift (cambio en la distribuciÃ³n)
    # Comparar con mÃ©tricas de referencia (del entrenamiento)
    r2_referencia = 0.85  # Valor de referencia del entrenamiento
    drift_detectado = abs(r2_actual - r2_referencia) > umbral_drift
    
    # Generar alerta si hay drift
    if drift_detectado:
        print(f"âš ï¸ ALERTA: Drift detectado en el modelo!")
        print(f"RÂ² actual: {r2_actual:.3f}")
        print(f"RÂ² referencia: {r2_referencia:.3f}")
        print(f"Diferencia: {abs(r2_actual - r2_referencia):.3f}")
    
    return {
        'r2_actual': r2_actual,
        'rmse_actual': rmse_actual,
        'drift_detectado': drift_detectado,
        'necesita_retrenamiento': drift_detectado
    }

# Ejemplo de monitoreo
print("\n=== MONITOREO DEL MODELO ===")
estado_modelo = monitorear_rendimiento_modelo(y_test, predicciones_optimizado)

print(f"Estado del modelo:")
for key, value in estado_modelo.items():
    print(f"  {key}: {value}")
```

---

### ğŸ¯ **Resumen de la ImplementaciÃ³n PrÃ¡ctica**

#### **âœ… Checklist de ImplementaciÃ³n Exitosa**

1. **PreparaciÃ³n de Datos** âœ…
   - [ ] Limpieza de valores faltantes
   - [ ] EstandarizaciÃ³n de categorÃ­as
   - [ ] Manejo de outliers
   - [ ] NormalizaciÃ³n/estandarizaciÃ³n
   - [ ] CodificaciÃ³n de variables categÃ³ricas

2. **ValidaciÃ³n del Modelo** âœ…
   - [ ] DivisiÃ³n adecuada de datos
   - [ ] ValidaciÃ³n cruzada
   - [ ] MÃ©tricas de evaluaciÃ³n apropiadas
   - [ ] Ajuste de hiperparÃ¡metros
   - [ ] AnÃ¡lisis de residuos

3. **Despliegue** âœ…
   - [ ] SerializaciÃ³n del modelo
   - [ ] FunciÃ³n de predicciÃ³n robusta
   - [ ] Sistema de monitoreo
   - [ ] DocumentaciÃ³n de metadatos

#### **ğŸš¨ Errores Comunes a Evitar**

1. **Data Leakage**: No usar informaciÃ³n del futuro para predecir el pasado
2. **Overfitting**: Validar siempre en datos no vistos
3. **Falta de monitoreo**: Los modelos se degradan con el tiempo
4. **Ignorar el contexto de negocio**: Las mÃ©tricas tÃ©cnicas no siempre reflejan el valor real

#### **ğŸ“ˆ PrÃ³ximos Pasos**

1. **A/B Testing**: Comparar el modelo contra mÃ©todos actuales
2. **Feedback Loop**: Incorporar feedback de usuarios
3. **Retrenamiento AutomÃ¡tico**: Sistema para actualizar el modelo periÃ³dicamente
4. **Escalabilidad**: Preparar para mayor volumen de datos

---

### Apartado especial para lo que es Feature Selection

### ğŸ§¹ 1. MÃ©todos de Filtro (*Filter Methods*)

Piensa en estos como un primer filtro rÃ¡pido.

* **CÃ³mo funcionan:** EvalÃºan cada caracterÃ­stica de forma individual usando pruebas estadÃ­sticas que miden su relaciÃ³n con la variable objetivo (la etiqueta).
* **Velocidad:** Muy rÃ¡pidos, ya que no requieren entrenar modelos.
* **Ejemplos:**

  * **Prueba de Chi-cuadrado** (para variables categÃ³ricas)
  * **Prueba ANOVA**
  * **InformaciÃ³n mutua**
  * **Coeficiente de correlaciÃ³n**

ğŸ“Œ **Ventajas:** RÃ¡pidos y no dependen del modelo
ğŸ“Œ **Desventajas:** No consideran las interacciones entre caracterÃ­sticas

---

### ğŸ§ª 2. MÃ©todos Envolventes (*Wrapper Methods*)

Estos son como probar diferentes combinaciones de ropa para ver cuÃ¡l te queda mejor.

* **CÃ³mo funcionan:** Prueban mÃºltiples subconjuntos de caracterÃ­sticas entrenando modelos, y eligen el subconjunto que da el mejor rendimiento.
* **Velocidad:** Lentos, porque entrenan muchos modelos.
* **Ejemplos:**

  * **EliminaciÃ³n recursiva de caracterÃ­sticas (RFE)**
  * **SelecciÃ³n hacia adelante**
  * **EliminaciÃ³n hacia atrÃ¡s**

ğŸ“Œ **Ventajas:** Tienen en cuenta interacciones entre caracterÃ­sticas
ğŸ“Œ **Desventajas:** Muy costosos en tiempo y recursos computacionales

---

### âš™ï¸ 3. MÃ©todos Embebidos (*Embedded Methods*)

Estos seleccionan caracterÃ­sticas **mientras entrenan** el modelo.

* **CÃ³mo funcionan:** La selecciÃ³n ocurre como parte del proceso de entrenamiento.
* **Ejemplos:**

  * **Lasso (regularizaciÃ³n L1)** â€“ reduce coeficientes a cero
  * **Ãrboles de decisiÃ³n / Bosques aleatorios** â€“ calculan importancia de cada variable
  * **Elastic Net** â€“ combina L1 y L2

ğŸ“Œ **Ventajas:** MÃ¡s eficientes y consideran interacciones
ğŸ“Œ **Desventajas:** Dependientes del modelo utilizado

---

### Tabla Resumen:

| Tipo de mÃ©todo | Â¿Usa modelo? | Â¿Considera interacciones? | Velocidad | Ejemplos                      |
| -------------- | ------------ | ------------------------- | --------- | ----------------------------- |
| Filtro         | âŒ No         | âŒ No                      | ğŸš€ RÃ¡pido | Chi-cuadrado, correlaciÃ³n     |
| Envolvente     | âœ… SÃ­         | âœ… SÃ­                      | ğŸ¢ Lento  | RFE, selecciÃ³n hacia adelante |
| Embebido       | âœ… SÃ­         | âœ… SÃ­                      | âš¡ Medio   | Lasso, Ã¡rboles de decisiÃ³n    |


## Recomendaciones para aprender:
Â¡Claro! Vamos a ver cada uno de estos mÃ©todos de selecciÃ³n de caracterÃ­sticas supervisadas con **ejemplos, ventajas y desventajas**, relacionÃ¡ndolos con los tipos que ya vimos: filtro, envolvente o embebido.

---

### ğŸ“‰ 1. **Variance Threshold**

ğŸ”¹ **Tipo:** Filtro
ğŸ”¹ **QuÃ© hace:** Elimina caracterÃ­sticas con **baja varianza**, es decir, que no cambian mucho entre ejemplos (por ejemplo, una columna donde casi todos los valores son iguales).

#### âœ… Ventajas:

* Muy simple y rÃ¡pido de aplicar
* No necesita etiquetas (tambiÃ©n se puede usar en problemas no supervisados)

#### âŒ Desventajas:

* No considera la relaciÃ³n con la variable objetivo
* Puede eliminar caracterÃ­sticas Ãºtiles si tienen poca varianza pero alta relevancia

#### ğŸ§ª Ejemplo en Python:

```python
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)
X_reduced = selector.fit_transform(X)
```

---

### â­ 2. **SelectKBest**

ğŸ”¹ **Tipo:** Filtro
ğŸ”¹ **QuÃ© hace:** Selecciona las **k mejores caracterÃ­sticas** segÃºn una mÃ©trica estadÃ­stica que mide la relaciÃ³n con la variable objetivo (como ANOVA, chi-cuadrado, etc.).

#### âœ… Ventajas:

* FÃ¡cil de interpretar y aplicar
* RÃ¡pido y eficiente
* Permite usar distintas mÃ©tricas

#### âŒ Desventajas:

* No considera interacciones entre variables
* Necesita que tÃº elijas el valor de *k* (el nÃºmero de caracterÃ­sticas a conservar)

#### ğŸ§ª Ejemplo en Python:

```python
from sklearn.feature_selection import SelectKBest, f_classif
selector = SelectKBest(score_func=f_classif, k=10)
X_kbest = selector.fit_transform(X, y)
```

---

### ğŸ”„ 3. **RFE (Recursive Feature Elimination)**

ğŸ”¹ **Tipo:** Envolvente
ğŸ”¹ **QuÃ© hace:** Usa un modelo (como una regresiÃ³n o un SVM) para eliminar **recursivamente** las caracterÃ­sticas menos importantes hasta quedarse con las mÃ¡s relevantes.

#### âœ… Ventajas:

* Considera interacciones entre variables
* Da muy buenos resultados si se elige bien el modelo base

#### âŒ Desventajas:

* Lento, especialmente con muchos datos o caracterÃ­sticas
* Depende fuertemente del modelo usado

#### ğŸ§ª Ejemplo en Python:

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
rfe = RFE(estimator=model, n_features_to_select=5)
X_rfe = rfe.fit_transform(X, y)
```

---

### ğŸŒ³ 4. **Boruta**

ğŸ”¹ **Tipo:** Envolvente (basado en Ã¡rboles, como Random Forest)
ğŸ”¹ **QuÃ© hace:** Crea versiones "aleatorias" de las caracterÃ­sticas y las compara con las reales. Solo conserva las que son mejores que las versiones aleatorias (shadow features).

#### âœ… Ventajas:

* Muy robusto y preciso
* Tiene en cuenta interacciones y relaciones no lineales
* Funciona bien con datos complejos

#### âŒ Desventajas:

* Bastante lento (usa muchos modelos de Random Forest)
* No estÃ¡ en `scikit-learn` directamente (hay que usar una librerÃ­a aparte como `boruta_py`)

#### ğŸ§ª Ejemplo con `boruta_py`:

```python
from boruta import BorutaPy
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)
boruta_selector = BorutaPy(estimator=model, n_estimators='auto', random_state=42)
boruta_selector.fit(X.values, y.values)
```

---

### ğŸ” Resumen Comparativo

| MÃ©todo             | Tipo       | InteracciÃ³n | Velocidad    | Modelo requerido | Ventaja principal                    |
| ------------------ | ---------- | ----------- | ------------ | ---------------- | ------------------------------------ |
| Variance Threshold | Filtro     | âŒ No        | ğŸš€ RÃ¡pido    | âŒ No             | Muy simple y rÃ¡pido                  |
| SelectKBest        | Filtro     | âŒ No        | ğŸš€ RÃ¡pido    | âŒ No             | MÃ©tricas estadÃ­sticas supervisadas   |
| RFE                | Envolvente | âœ… SÃ­        | ğŸ¢ Lento     | âœ… SÃ­             | Preciso si el modelo es adecuado     |
| Boruta             | Envolvente | âœ… SÃ­        | ğŸŒ Muy lento | âœ… SÃ­             | Robusto frente a ruido y redundancia |

