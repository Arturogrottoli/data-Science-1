#Recordatorio Clase 06 se hace la primer pre entrega.

# ============================================================
# REPASO CLASE 05CLASE DE VISUALIZACIÃ“N AVANZADA CON SEABORN Y MATPLOTLIB
# ============================================================
# Tema: Herramientas avanzadas de visualizaciÃ³n
#
# Objetivo de la clase:
# - Entender quÃ© es Seaborn y por quÃ© usarlo encima de Matplotlib.
# - Diferenciar subplots clÃ¡sicos de Matplotlib vs. FacetGrid de Seaborn.
# - Practicar con ejemplos reales de grÃ¡ficos.
#
# ------------------------------------------------------------
# TEORÃA:
# ------------------------------------------------------------
# Â¿QuÃ© es Seaborn?
# - Es una librerÃ­a de Python construida sobre Matplotlib.
# - Se integra directamente con Pandas, lo que facilita graficar DataFrames.
# - Permite hacer grÃ¡ficos estadÃ­sticos de forma rÃ¡pida, estÃ©tica y sencilla.
#
# Ventajas principales:
# 1. Sintaxis mÃ¡s simple que Matplotlib.
# 2. Temas visuales predefinidos (mejor estÃ©tica por defecto).
# 3. Funciona directamente con DataFrames y columnas.
# 4. Tiene funciones para grÃ¡ficos complejos en pocas lÃ­neas.
#
# ------------------------------------------------------------
# Diferencia entre funciones "Axes-level" y "Figure-level":
# - Axes-level (ej: sns.scatterplot, sns.boxplot):
#   -> Se dibujan en un objeto de ejes especÃ­fico (matplotlib.pyplot.Axes).
#   -> Ãštiles cuando quiero controlar UN grÃ¡fico dentro de subplots.
#
# - Figure-level (ej: sns.relplot, sns.catplot, sns.FacetGrid):
#   -> Controlan toda la figura completa, creando automÃ¡ticamente subplots.
#   -> Ãštiles para comparar subgrupos o mÃºltiples distribuciones de datos.
#
# ------------------------------------------------------------
# EJEMPLO 1: SUBPLOTS CLÃSICOS CON MATPLOTLIB + SEABORN
# ------------------------------------------------------------
# Idea: crear una figura con 4 grÃ¡ficos diferentes para mostrar:
# - Histograma
# - Boxplot
# - GrÃ¡fico de dispersiÃ³n
# - GrÃ¡fico de lÃ­neas
#
# Objetivo: practicar cÃ³mo ubicar diferentes grÃ¡ficos en una grilla
# (2 filas x 2 columnas) y cÃ³mo elegir quÃ© grÃ¡fico va en cada lugar.
# ============================================================

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Generamos datos de ejemplo usando NumPy
# np.random.seed(42) -> fijamos semilla para reproducibilidad (los datos serÃ¡n siempre iguales)
np.random.seed(42)
df = pd.DataFrame({
    "variable1": np.random.normal(0, 1, 100),   # distribuciÃ³n normal media=0, sd=1
    "variable2": np.random.exponential(1, 100), # distribuciÃ³n exponencial
    "variable3": np.random.uniform(0, 10, 100), # distribuciÃ³n uniforme
    "variable4": np.random.normal(5, 2, 100),   # normal con media=5, sd=2
    "variable5": range(100),                    # valores consecutivos 0 a 99
    "variable6": np.random.normal(50, 10, 100)  # normal con media=50, sd=10
})

# Creamos la figura con 2 filas y 2 columnas de subplots
fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))

# Subplot 1: Histograma de variable1
# Histograma = distribuciÃ³n de frecuencias
sns.histplot(data=df, x='variable1', ax=axs[0, 0], color="skyblue")
axs[0, 0].set_title("Histograma de variable1")

# Subplot 2: Boxplot de variable2
# Boxplot = detecta la dispersiÃ³n de los datos y posibles outliers
sns.boxplot(data=df, x='variable2', ax=axs[0, 1], color="lightgreen")
axs[0, 1].set_title("Boxplot de variable2")

# Subplot 3: GrÃ¡fico de dispersiÃ³n (scatterplot)
# Muestra relaciÃ³n entre dos variables
sns.scatterplot(data=df, x='variable3', y='variable4', ax=axs[1, 0], color="salmon")
axs[1, 0].set_title("DispersiÃ³n variable3 vs variable4")

# Subplot 4: GrÃ¡fico de lÃ­neas
# Ãštil para series temporales o valores consecutivos
sns.lineplot(data=df, x='variable5', y='variable6', ax=axs[1, 1], color="purple")
axs[1, 1].set_title("LÃ­nea variable5 vs variable6")

# Ajustamos para que no se encimen tÃ­tulos y grÃ¡ficos
plt.tight_layout()
plt.show()

# ============================================================
# EJEMPLO 2: FACETGRID EN SEABORN
# ============================================================
# Objetivo:
# - Mostrar cÃ³mo dividir automÃ¡ticamente un dataset en mÃºltiples grÃ¡ficos
#   segÃºn categorÃ­as.
# - Usaremos el dataset "tips" de Seaborn (propinas en un restaurante).
#
# Casos de uso tÃ­picos:
# - Comparar distribuciones entre grupos (ej: hombres vs mujeres).
# - Ver cÃ³mo cambia una variable segÃºn otra categorÃ­a (ej: Lunch vs Dinner).
# ============================================================

# Cargamos dataset "tips"
df_tips = sns.load_dataset("tips")

# Mostramos primeras filas para conocer estructura
print(df_tips.head())

# Creamos un FacetGrid
# col="sex"  -> divide columnas por sexo
# row="time" -> divide filas por tiempo (Lunch/Dinner)
g = sns.FacetGrid(df_tips, col="sex", row="time", margin_titles=True)

# Mapear grÃ¡fico: en cada subplot mostrar histograma de "total_bill"
g.map(sns.histplot, "total_bill", bins=15, color="skyblue")

# Agregar leyenda para categorÃ­as
g.add_legend()

# Mostrar figura
plt.show()

# ------------------------------------------------------------
# CONCLUSIONES DE LA CLASE:
# - Matplotlib permite un control detallado de grÃ¡ficos y subplots.
# - Seaborn simplifica la sintaxis y mejora la estÃ©tica.
# - Subplots clÃ¡sicos: control manual de cada grÃ¡fico.
# - FacetGrid: automatizaciÃ³n para comparar categorÃ­as.
# ------------------------------------------------------------


#inicio CLase 06
# Tipos de variables en un analisis del tipo EDA
[DIAPOSITIVAS](https://docs.google.com/presentation/d/1bTgblneO_G2WteTeku40AN-3yKmhiVvpcudqcGo0cco/edit?slide=id.p2#slide=id.p2)


#Teoria 6.1

# ============================================================
# 6.1 FUNDAMENTOS DE ESTADÃSTICA
# ============================================================

# ğŸ‘‰ EstadÃ­stica Descriptiva: Concepto y Relevancia
# - Rama de la estadÃ­stica que recopila, organiza y resume datos.
# - Su objetivo principal es ofrecer una visiÃ³n general y comprensible.
# - Permite identificar patrones, tendencias y relaciones.
# - Herramienta clave para la toma de decisiones informadas.
# - Usa medidas de resumen como:
#   -> Media (promedio)
#   -> Mediana (valor central)
#   -> Moda (valor mÃ¡s frecuente)
#   -> Varianza y DesviaciÃ³n EstÃ¡ndar (medidas de dispersiÃ³n).

# ğŸ‘‰ Importancia prÃ¡ctica:
# Nos ayuda a responder preguntas como:
# - Â¿CuÃ¡l es el valor tÃ­pico de los datos?
# - Â¿QuÃ© tan dispersos o concentrados estÃ¡n?
# - Â¿Existen valores atÃ­picos (outliers)?

# ------------------------------------------------------------
# IntroducciÃ³n al AnÃ¡lisis Exploratorio de Datos (EDA)
# ------------------------------------------------------------
# - Fase inicial del anÃ¡lisis de datos.
# - Objetivo: descubrir patrones, detectar anomalÃ­as,
#   probar hipÃ³tesis y verificar supuestos.
# - Se realiza combinando:
#   -> EstadÃ­stica descriptiva
#   -> VisualizaciÃ³n de datos
#
# FilosofÃ­a del EDA:
# - Acercarse a los datos sin prejuicios.
# - Explorar abiertamente para encontrar caracterÃ­sticas inesperadas.
#
# Beneficios del EDA:
# - Entender la estructura de los datos.
# - Preparar datos para modelos mÃ¡s complejos.
# - Tomar mejores decisiones sobre limpieza y preprocesamiento.

# ------------------------------------------------------------
# RESUMEN PARA LA CLASE:
# ------------------------------------------------------------
# - La estadÃ­stica descriptiva resume y organiza los datos.
# - El EDA es el paso inicial para explorar datos a fondo.
# - Juntos permiten comprender los datos antes de aplicar tÃ©cnicas
#   de predicciÃ³n o machine learning.
#
# EJEMPLOS EN CLASE:
# - Usar Python para calcular media, mediana y moda de un dataset.
# - Hacer histogramas y boxplots para visualizar distribuciones.


Parte 6.2

# ============================================================
# 6.2 MEDIDAS DE RESUMEN Y DISTRIBUCIONES
# ============================================================

# ğŸ‘‰ Medidas Cuantitativas
# ------------------------------------------------------------
# MEDIA:
# - Promedio de todos los valores.
# - FÃ³rmula: Media = (Î£ xi) / n
# - Representa el valor tÃ­pico, pero es sensible a outliers.
#
# MEDIANA:
# - Valor central de los datos ordenados.
# - Robusta frente a valores atÃ­picos.
#
# MODA:
# - Valor que mÃ¡s se repite.
# - Puede ser unimodal (1), bimodal (2) o multimodal (+2).
# - Ãštil especialmente en datos categÃ³ricos.
#
# VARIANZA:
# - Mide la dispersiÃ³n de los datos respecto a la media.
# - FÃ³rmula: Var = Î£(xi - media)^2 / (n-1)
#
# DESVIACIÃ“N ESTÃNDAR:
# - RaÃ­z cuadrada de la varianza.
# - Se expresa en las mismas unidades que los datos.
# - Indica cuÃ¡nto se alejan, en promedio, los datos de la media.
#
# CUARTILES Y PERCENTILES:
# - Dividen los datos en partes iguales.
# - Q1: 25% inferior, Q2: mediana, Q3: 75% inferior.
# - Percentiles: dividen en 100 partes.
# - Ãštiles para ver posiciones relativas (ej: percentil 90).

# ğŸ‘‰ Medidas Cualitativas
# ------------------------------------------------------------
# - Variables categÃ³ricas (ej: gÃ©nero, estado civil, color).
# - Se resumen con:
#   -> Conteo de observaciones (frecuencias).
#   -> Moda (categorÃ­a mÃ¡s comÃºn).
#
# Ejemplo:
#   GÃ©nero: {Hombres: 60, Mujeres: 40}  -> Conteo
#   Moda = "Hombres"  -> CategorÃ­a mÃ¡s frecuente

# ğŸ‘‰ Distribuciones de Variables
# ------------------------------------------------------------
# DISTRIBUCIÃ“N UNIFORME:
# - Todos los valores tienen la misma probabilidad.
# - Ejemplo: lanzar un dado justo.
# - GrÃ¡fico: barras del histograma con alturas similares.
#
# DISTRIBUCIÃ“N NORMAL (campana de Gauss):
# - SimÃ©trica respecto a la media.
# - La mayorÃ­a de datos se concentra en torno a la media.
# - Media = Mediana = Moda.
# - Ejemplo: altura de personas.
#
# DIFERENCIAS:
# - Uniforme = todos los resultados igual de probables.
# - Normal = los valores centrales son mÃ¡s probables.

# ğŸ‘‰ Histogramas
# ------------------------------------------------------------
# - Muestran cÃ³mo se distribuyen los datos.
# - Uniforme: barras todas del mismo nivel.
# - Normal: forma de campana (barras altas en el centro).
#
# EJEMPLO VISUAL:
# - Uniforme: â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ
# - Normal:     â–‚â–„â–ˆâ–„â–‚

# ğŸ‘‰ CorrelaciÃ³n Lineal y Coeficiente de CorrelaciÃ³n
# ------------------------------------------------------------
# - RelaciÃ³n entre dos variables (fuerza y direcciÃ³n).
# - Coeficiente r âˆˆ [-1, 1]:
#   r â‰ˆ 1  -> correlaciÃ³n positiva fuerte
#   r â‰ˆ -1 -> correlaciÃ³n negativa fuerte
#   r â‰ˆ 0  -> no hay relaciÃ³n lineal
#
# IMPORTANTE: CorrelaciÃ³n â‰  causalidad.
# - Que dos variables estÃ©n relacionadas no significa
#   que una cause a la otra.
#
# Herramienta clave: diagrama de dispersiÃ³n (scatter plot).
# - Permite visualizar la relaciÃ³n entre dos variables.
#
# Ejemplo:
# - r = 0.9 -> a mayor estudio, mayor nota (fuerte positiva).
# - r = -0.8 -> a mÃ¡s horas de TV, menor nota (fuerte negativa).
# - r â‰ˆ 0.0 -> variables independientes.

Punto 6.3

# ============================================
# ğŸ“Œ 6.3 IntroducciÃ³n al Preprocesamiento
# ============================================
# El preprocesamiento de datos es una etapa FUNDAMENTAL en cualquier proyecto de Ciencia de Datos.
# Consiste en transformar datos crudos (raw data) en datos limpios y listos para el anÃ¡lisis.
# 
# ğŸš¨ Problema: Los datos reales suelen venir incompletos, ruidosos, inconsistentes.
# âœ… SoluciÃ³n: Aplicar tÃ©cnicas de preprocesamiento para mejorar su calidad y obtener mejores modelos.
#
# Importancia:
# - Mejora la calidad de los datos.
# - Aumenta el rendimiento de los algoritmos de Machine Learning.
# - Evita errores y conclusiones incorrectas.
#
# Tareas comunes:
# 1. Limpieza de datos
# 2. IntegraciÃ³n de datos
# 3. TransformaciÃ³n de datos
# 4. ReducciÃ³n de datos
# 5. DetecciÃ³n y tratamiento de outliers

# ============================================
# ğŸ”¹ EJEMPLOS PRÃCTICOS CON PYTHON
# ============================================

import pandas as pd
import numpy as np

# Creamos un DataFrame de ejemplo
data = {
    "Nombre": ["Ana", "Luis", "Pedro", "MarÃ­a", None],
    "Edad": [23, np.nan, 35, 29, 40],
    "Ciudad": ["Madrid", "Madrid", "Barcelona", "Madrid", "Barcelona"],
    "Salario": [30000, 40000, None, 50000, 1000000]  # <--- hay un valor outlier
}

df = pd.DataFrame(data)

print("ğŸ“Š Datos originales:")
print(df)

# ============================================
# 1. Limpieza de Datos (Data Cleaning)
# ============================================
# - Manejo de valores faltantes
# - CorrecciÃ³n/eliminaciÃ³n de outliers
# - ReducciÃ³n del ruido en los datos

# Rellenar valores faltantes en Edad con la media
df["Edad"].fillna(df["Edad"].mean(), inplace=True)

# Rellenar valores faltantes en Salario con la mediana
df["Salario"].fillna(df["Salario"].median(), inplace=True)

# Eliminar filas con Nombre nulo
df.dropna(subset=["Nombre"], inplace=True)

print("\nâœ… Datos despuÃ©s de limpieza:")
print(df)

# ============================================
# 2. IntegraciÃ³n de Datos (Data Integration)
# ============================================
# - Combinar datos de distintas fuentes.
# (Ejemplo simple con un dataset adicional de bonus)

bonus = pd.DataFrame({
    "Nombre": ["Ana", "Luis", "Pedro", "MarÃ­a"],
    "Bonus": [1000, 1500, 1200, 2000]
})

df = pd.merge(df, bonus, on="Nombre", how="left")

print("\nğŸ”— Datos despuÃ©s de integraciÃ³n:")
print(df)

# ============================================
# 3. TransformaciÃ³n de Datos (Data Transformation)
# ============================================
# - NormalizaciÃ³n / Escalado
# - CodificaciÃ³n de variables categÃ³ricas

from sklearn.preprocessing import MinMaxScaler, LabelEncoder

# Normalizar Salario en rango [0,1]
scaler = MinMaxScaler()
df["Salario_Normalizado"] = scaler.fit_transform(df[["Salario"]])

# Codificar variable categÃ³rica Ciudad
encoder = LabelEncoder()
df["Ciudad_Codificada"] = encoder.fit_transform(df["Ciudad"])

print("\nğŸ”„ Datos despuÃ©s de transformaciÃ³n:")
print(df)

# ============================================
# 4. ReducciÃ³n de Datos (Data Reduction)
# ============================================
# - SelecciÃ³n de caracterÃ­sticas
# - ReducciÃ³n de dimensionalidad (ej. PCA)
# - Muestreo de datos

# Ejemplo: Nos quedamos solo con las variables mÃ¡s relevantes
df_reducido = df[["Edad", "Salario_Normalizado", "Ciudad_Codificada"]]

print("\nğŸ“‰ Datos reducidos:")
print(df_reducido)

# ============================================
# 5. DetecciÃ³n y Tratamiento de Outliers
# ============================================
# - Identificar valores atÃ­picos que distorsionan el anÃ¡lisis

Q1 = df["Salario"].quantile(0.25)
Q3 = df["Salario"].quantile(0.75)
IQR = Q3 - Q1

outliers = df[(df["Salario"] < (Q1 - 1.5 * IQR)) | (df["Salario"] > (Q3 + 1.5 * IQR))]

print("\nğŸš¨ Outliers detectados:")
print(outliers)

# ============================================
# ğŸ“Œ ConclusiÃ³n:
# ============================================
# El preprocesamiento es clave en Data Science porque:
# - Mejora la calidad de los datos.
# - Prepara los datos para modelos de ML.
# - Permite descubrir patrones mÃ¡s claros.
# 
# Sin un buen preprocesamiento, el anÃ¡lisis o modelado puede ser incorrecto.


# ============================================
# ğŸ“Œ 6.4 DetecciÃ³n y Manejo de Outliers
# ============================================
# Los OUTLIERS (valores atÃ­picos) son observaciones que estÃ¡n muy alejadas
# del resto de los datos en un conjunto.
#
# ğŸš¨ Problema: Pueden distorsionar anÃ¡lisis estadÃ­sticos y modelos predictivos.
# âœ… SoluciÃ³n: Detectarlos y tratarlos de forma adecuada segÃºn el contexto.
#
# --------------------------------------------
# ğŸ” IdentificaciÃ³n con Diagramas de Caja y Bigotes (Boxplots)
# --------------------------------------------
# - Caja (Box): representa el rango intercuartil (IQR = Q3 - Q1).
# - Bigotes (Whiskers): se extienden desde Q1 y Q3 hasta 1.5 * IQR.
# - Outliers: cualquier punto fuera de los bigotes.
#
# --------------------------------------------
# ğŸ¯ Opciones de Tratamiento de Outliers
# --------------------------------------------
# 1. EliminaciÃ³n â†’ si son errores de mediciÃ³n o ingreso de datos.
# 2. TransformaciÃ³n â†’ suavizar su efecto (ej. logaritmo).
# 3. ImputaciÃ³n â†’ reemplazarlos con valores como cuartiles o medianas.
# 4. AnÃ¡lisis de sensibilidad â†’ comparar resultados con y sin outliers.
#
# --------------------------------------------
# ğŸ“Œ ConclusiÃ³n:
# Detectar y manejar outliers es crucial para obtener modelos mÃ¡s precisos.
# La decisiÃ³n de cÃ³mo tratarlos depende del contexto y de su origen.

# ============================================
# ğŸ”¹ EJEMPLO PRÃCTICO CON PYTHON
# ============================================

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Generamos un dataset con outliers
np.random.seed(42)
data = np.random.normal(50, 10, 100)  # distribuciÃ³n normal
data = np.append(data, [150, 200, -50])  # agregamos valores extremos (outliers)

df = pd.DataFrame({"Valor": data})

print("ğŸ“Š Datos originales (con outliers):")
print(df.describe())

# ============================================
# 1. IdentificaciÃ³n de Outliers con Boxplot
# ============================================
plt.figure(figsize=(6,4))
sns.boxplot(x=df["Valor"])
plt.title("Diagrama de Caja y Bigotes - Outliers visibles")
plt.show()

# ============================================
# 2. DetecciÃ³n numÃ©rica de Outliers con IQR
# ============================================
Q1 = df["Valor"].quantile(0.25)
Q3 = df["Valor"].quantile(0.75)
IQR = Q3 - Q1

# Definimos los lÃ­mites
limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR

outliers = df[(df["Valor"] < limite_inferior) | (df["Valor"] > limite_superior)]

print("\nğŸš¨ Outliers detectados:")
print(outliers)

# ============================================
# 3. Opciones de Tratamiento
# ============================================

# ğŸ”¹ OpciÃ³n A: EliminaciÃ³n de outliers
df_sin_outliers = df[(df["Valor"] >= limite_inferior) & (df["Valor"] <= limite_superior)]
print("\nâœ… Datos sin outliers (eliminaciÃ³n):")
print(df_sin_outliers.describe())

# ğŸ”¹ OpciÃ³n B: TransformaciÃ³n (logarÃ­tmica)
# (Solo funciona con valores positivos)
df["Log_Valor"] = np.log(df["Valor"] + abs(df["Valor"].min()) + 1)
print("\nğŸ”„ Datos transformados con logaritmo:")
print(df[["Valor", "Log_Valor"]].head())

# ğŸ”¹ OpciÃ³n C: ImputaciÃ³n (reemplazar por mediana)
mediana = df["Valor"].median()
df_imputado = df.copy()
df_imputado.loc[df_imputado["Valor"] > limite_superior, "Valor"] = mediana
df_imputado.loc[df_imputado["Valor"] < limite_inferior, "Valor"] = mediana

print("\nâ™»ï¸ Datos con imputaciÃ³n (reemplazo por mediana):")
print(df_imputado.describe())

# ============================================
# ğŸ“Œ ConclusiÃ³n:
# - Detectamos outliers con boxplots y con IQR.
# - Mostramos varias formas de tratarlos (eliminaciÃ³n, transformaciÃ³n, imputaciÃ³n).
# - La elecciÃ³n depende del contexto y de la naturaleza de los datos.

"""
6.5 AnÃ¡lisis de Componentes Principales (PCA)

ğŸ“Œ El PCA (Principal Component Analysis) es una tÃ©cnica estadÃ­stica utilizada en Data Science
para reducir la dimensionalidad de un conjunto de datos mientras se conserva la mayor parte
de la variabilidad (informaciÃ³n).

ğŸ‘‰ Â¿Por quÃ© usamos PCA?
- Muchos datasets tienen variables altamente correlacionadas â†’ redundancia.
- MÃ¡s variables = modelos mÃ¡s complejos y lentos.
- Con PCA reducimos variables â†’ hacemos los modelos mÃ¡s simples, rÃ¡pidos y generalizables.
"""

# =====================================================
# 1. Importamos librerÃ­as necesarias
# =====================================================
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# =====================================================
# 2. Creamos un dataset de ejemplo
# =====================================================
# Para entender PCA, generamos datos artificiales con correlaciones
np.random.seed(42)
X1 = np.random.normal(5, 2, 100)      # Variable 1
X2 = X1 * 0.8 + np.random.normal(0,1,100)   # Variable 2 correlacionada con X1
X3 = np.random.normal(10, 5, 100)     # Variable 3 independiente

df = pd.DataFrame({
    "Var1": X1,
    "Var2": X2,
    "Var3": X3
})

print("\n=== Dataset original ===")
print(df.head())

# =====================================================
# 3. EstandarizaciÃ³n de los datos
# =====================================================
"""
ğŸ“Œ IMPORTANTE:
PCA es sensible a la escala de las variables. 
Antes de aplicar PCA, se recomienda escalar los datos 
(media = 0, desviaciÃ³n estÃ¡ndar = 1).
"""
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)

print("\n=== Datos estandarizados (primeras filas) ===")
print(scaled_data[:5])

# =====================================================
# 4. AplicaciÃ³n de PCA
# =====================================================
"""
ğŸ“Œ Proceso de PCA:
1. Calcular matriz de covarianza.
2. Obtener valores propios y vectores propios.
3. Seleccionar componentes principales (PCs).
4. Proyectar los datos en estas PCs.

Con sklearn esto se simplifica enormemente.
"""
pca = PCA(n_components=2)   # Queremos reducir a 2 dimensiones
pca_result = pca.fit_transform(scaled_data)

# Convertimos resultado en DataFrame para analizar
df_pca = pd.DataFrame(data=pca_result, columns=["PC1", "PC2"])
print("\n=== Dataset transformado con PCA ===")
print(df_pca.head())

# =====================================================
# 5. Varianza explicada por cada componente
# =====================================================
"""
ğŸ“Œ InterpretaciÃ³n:
- Cada componente explica un % de la varianza total.
- PC1 explica la mayor parte.
- PC2 explica la siguiente parte, y asÃ­ sucesivamente.
"""
print("\n=== Varianza explicada por cada componente ===")
print(pca.explained_variance_ratio_)

# Visualizamos en grÃ¡fico
plt.figure(figsize=(6,4))
plt.bar(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_, alpha=0.7)
plt.xlabel("Componentes Principales")
plt.ylabel("Varianza Explicada")
plt.title("Varianza explicada por PCA")
plt.show()

# =====================================================
# 6. VisualizaciÃ³n de los datos en el nuevo espacio PCA
# =====================================================
"""
ğŸ“Œ Ahora los datos estÃ¡n representados en un espacio de menor dimensiÃ³n,
pero reteniendo la mayor parte de la informaciÃ³n original.
"""
plt.figure(figsize=(6,6))
sns.scatterplot(x="PC1", y="PC2", data=df_pca, s=60, color="purple")
plt.title("ProyecciÃ³n de los datos en los 2 primeros Componentes Principales")
plt.show()

# =====================================================
# 7. ConclusiÃ³n
# =====================================================
"""
âœ… PCA es Ãºtil para:
- Reducir dimensionalidad.
- Eliminar redundancia de variables.
- Simplificar modelos.
- Mitigar riesgo de overfitting.
- Mejorar interpretabilidad.

En este ejemplo:
- Pasamos de 3 variables a 2 componentes principales.
- Aun asÃ­, mantenemos gran parte de la varianza (informaciÃ³n).

Esto mismo se puede aplicar en datasets de alta dimensiÃ³n
(imÃ¡genes, genÃ©tica, sensores, finanzas, etc).
"""
"""
6.6 Ejemplos PrÃ¡cticos

ğŸ“Œ En esta secciÃ³n veremos ejemplos de:
1. EstadÃ­stica descriptiva aplicada a datos de ingresos.
2. Preprocesamiento de datos (limpieza, transformaciÃ³n, integraciÃ³n, reducciÃ³n).

ğŸ‘‰ Importante:
La estadÃ­stica descriptiva permite entender las caracterÃ­sticas de un dataset
(medidas de tendencia central, dispersiÃ³n, distribuciÃ³n).
El preprocesamiento asegura que los datos estÃ©n listos para aplicar modelos predictivos.
"""

# =====================================================
# 1. Importamos librerÃ­as necesarias
# =====================================================
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA

# =====================================================
# 2. Ejemplo de EstadÃ­stica Descriptiva
# =====================================================
"""
ğŸ¯ Contexto:
Tenemos un conjunto de datos con informaciÃ³n de ingresos anuales de una poblaciÃ³n.
Queremos entender su distribuciÃ³n con estadÃ­sticas descriptivas.
"""

# Generamos un dataset simulado de ingresos
np.random.seed(42)
ingresos = np.random.normal(50000, 15000, 200)  # media=50k, std=15k
ingresos = np.append(ingresos, [120000, 150000])  # agregamos outliers

df_ingresos = pd.DataFrame({"Ingreso_Anual": ingresos})

print("\n=== Primeras filas de los ingresos ===")
print(df_ingresos.head())

# Medidas de tendencia central
media = df_ingresos["Ingreso_Anual"].mean()
mediana = df_ingresos["Ingreso_Anual"].median()
moda = df_ingresos["Ingreso_Anual"].mode()[0]

print("\n=== Medidas de Tendencia Central ===")
print(f"Media: {media:.2f}, Mediana: {mediana:.2f}, Moda: {moda:.2f}")

# Medidas de dispersiÃ³n
desviacion = df_ingresos["Ingreso_Anual"].std()
q1 = df_ingresos["Ingreso_Anual"].quantile(0.25)
q3 = df_ingresos["Ingreso_Anual"].quantile(0.75)
iqr = q3 - q1

print("\n=== Medidas de DispersiÃ³n ===")
print(f"DesviaciÃ³n EstÃ¡ndar: {desviacion:.2f}")
print(f"IQR (Rango IntercuartÃ­lico): {iqr:.2f}")

# VisualizaciÃ³n: histograma y boxplot
plt.figure(figsize=(12,4))

plt.subplot(1,2,1)
sns.histplot(df_ingresos["Ingreso_Anual"], kde=True, bins=30, color="skyblue")
plt.axvline(media, color="red", linestyle="--", label="Media")
plt.axvline(mediana, color="green", linestyle="--", label="Mediana")
plt.legend()
plt.title("DistribuciÃ³n de Ingresos Anuales")

plt.subplot(1,2,2)
sns.boxplot(x=df_ingresos["Ingreso_Anual"], color="lightcoral")
plt.title("Boxplot de Ingresos Anuales")

plt.show()

# =====================================================
# 3. Ejemplo de Preprocesamiento de Datos
# =====================================================
"""
ğŸ¯ Contexto:
Dataset de clientes de un banco con:
- Edad
- Saldo de cuenta (con valores faltantes y outliers)
- Historial crediticio (texto)
- Tipo de cuenta

Queremos limpiar, transformar y preparar los datos.
"""

# Creamos dataset simulado
df_clientes = pd.DataFrame({
    "Edad": [25, 40, 35, 50, 29, np.nan, 60],
    "Saldo": [1000, 2000, -500, 999999, 3000, np.nan, 2500],
    "Historial_Credito": ["Bueno", "Malo", "Bueno", "Malo", "Bueno", "Regular", "Malo"],
    "Tipo_Cuenta": ["Ahorro", "Corriente", "Ahorro", "Ahorro", "Corriente", "Corriente", "Ahorro"]
})

print("\n=== Dataset original de clientes ===")
print(df_clientes)

# --- Limpieza de datos ---
# 1. Rellenamos valores faltantes de Edad con la media
df_clientes["Edad"].fillna(df_clientes["Edad"].mean(), inplace=True)

# 2. Rellenamos valores faltantes de Saldo con la mediana
df_clientes["Saldo"].fillna(df_clientes["Saldo"].median(), inplace=True)

# 3. Eliminamos outliers extremos en Saldo
q1 = df_clientes["Saldo"].quantile(0.25)
q3 = df_clientes["Saldo"].quantile(0.75)
iqr = q3 - q1
limite_inferior = q1 - 1.5 * iqr
limite_superior = q3 + 1.5 * iqr
df_clientes = df_clientes[(df_clientes["Saldo"] >= limite_inferior) & (df_clientes["Saldo"] <= limite_superior)]

print("\n=== Dataset despuÃ©s de limpieza ===")
print(df_clientes)

# --- TransformaciÃ³n de datos ---
"""
ğŸ“Œ One Hot Encoding:
Convertimos variables categÃ³ricas (texto) en binarias (0/1).
"""
encoder = OneHotEncoder(sparse_output=False)
encoded_vars = encoder.fit_transform(df_clientes[["Historial_Credito", "Tipo_Cuenta"]])

encoded_df = pd.DataFrame(encoded_vars, columns=encoder.get_feature_names_out())
df_final = pd.concat([df_clientes.reset_index(drop=True), encoded_df], axis=1)

print("\n=== Dataset transformado con One Hot Encoding ===")
print(df_final)

# --- ReducciÃ³n de dimensionalidad con PCA ---
"""
ğŸ“Œ Aplicamos PCA en las variables numÃ©ricas (Edad, Saldo, variables binarias).
"""
pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_final.drop(columns=["Historial_Credito", "Tipo_Cuenta"]))

df_pca = pd.DataFrame(pca_result, columns=["PC1", "PC2"])

print("\n=== Dataset reducido con PCA ===")
print(df_pca)

# VisualizaciÃ³n de clientes en espacio PCA
plt.figure(figsize=(6,6))
sns.scatterplot(x="PC1", y="PC2", data=df_pca, s=80, color="purple")
plt.title("Clientes proyectados en 2 Componentes Principales (PCA)")
plt.show()

# =====================================================
# 4. ConclusiÃ³n
# =====================================================
"""
âœ… EstadÃ­stica descriptiva â†’ nos permitiÃ³ entender distribuciÃ³n de ingresos, identificar outliers y medir dispersiÃ³n.
âœ… Preprocesamiento â†’ limpiamos valores faltantes y outliers, transformamos variables categÃ³ricas, y reducimos dimensionalidad con PCA.

Esto refleja el flujo real de trabajo en Ciencia de Datos:
- Explorar
- Limpiar
- Transformar
- Preparar para modelar
"""


### ğŸ§  1. **SegÃºn su naturaleza:**

#### a. **Cuantitativas (NumÃ©ricas)**

* Representan cantidades o valores numÃ©ricos.
* Se subdividen en:

##### ğŸ”¹ *Continuas*

* Pueden tomar **infinitos valores dentro de un rango**.
* Se miden, no se cuentan.
* **Ejemplos**:

  * Altura (1.75 m, 1.76 m, etc.)
  * Peso (65.5 kg, 66.2 kg)
  * Tiempo (2.5 horas)

##### ğŸ”¹ *Discretas*

* Solo pueden tomar **valores enteros o finitos**.
* Se cuentan.
* **Ejemplos**:

  * NÃºmero de hijos (0, 1, 2â€¦)
  * NÃºmero de autos en una familia
  * Cantidad de errores en una prueba

---

#### b. **Cualitativas (Categoricas)**

* Representan **categorÃ­as o cualidades**.
* No implican operaciones matemÃ¡ticas.

##### ğŸ”¹ *Nominales*

* No tienen orden lÃ³gico.
* **Ejemplos**:

  * Color de ojos (azul, verde, marrÃ³n)
  * GÃ©nero (masculino, femenino, otro)
  * Nacionalidad (argentina, chilena)

##### ğŸ”¹ *Ordinales*

* Tienen **orden o jerarquÃ­a**, pero no hay distancia precisa entre categorÃ­as.
* **Ejemplos**:

  * Nivel educativo (primario, secundario, universitario)
  * Grado de satisfacciÃ³n (bajo, medio, alto)
  * Rango militar (soldado, cabo, sargento)

---

### ğŸ” 2. **SegÃºn su variabilidad:**

#### ğŸ”¹ *Dependientes*

* Son afectadas por otras variables.
* **Ejemplo**: La presiÃ³n arterial depende del nivel de actividad fÃ­sica.

#### ğŸ”¹ *Independientes*

* Se manipulan para ver su efecto.
* **Ejemplo**: Horas de estudio como variable que afecta al rendimiento acadÃ©mico.

---

### ğŸ“Š 3. **Otras clasificaciones Ãºtiles:**

#### ğŸ”¹ *Binarias o dicotÃ³micas*

* Solo tienen **dos categorÃ­as posibles**.
* **Ejemplo**: SÃ­ / No, Aprobado / Reprobado

#### ğŸ”¹ *PolinÃ³micas*

* Tienen mÃ¡s de dos categorÃ­as.
* **Ejemplo**: Tipo de sangre (A, B, AB, O)
---
Las **series temporales** son un tipo especial de conjunto de datos en el que **las observaciones estÃ¡n ordenadas en el tiempo**. Se utilizan para analizar y predecir cÃ³mo cambian los valores de una variable a lo largo del tiempo.

---


## Consideracion de otras variables sobre Temporalidades

### ğŸ•’ Â¿QuÃ© es una serie temporal?

Una **serie temporal** es una **secuencia de datos** registrados a lo largo del tiempo, **en intervalos regulares** (diarios, mensuales, anuales, etc.).

* **Ejemplos**:

  * Temperatura diaria de una ciudad
  * Precio del dÃ³lar por mes
  * Ventas de una empresa por trimestre
  * Cantidad de pasajeros por hora en un subte

---

### ğŸ§© Cualidades distintivas de las series temporales:

1. ### ğŸ“ **Secuencialidad**

   * El orden cronolÃ³gico **es fundamental**.
   * A diferencia de otros conjuntos de datos, **reordenar los datos rompe la estructura** de la serie.

2. ### ğŸ” **Dependencia temporal**

   * Los valores sucesivos estÃ¡n **relacionados entre sÃ­**.
   * El valor en el tiempo `t` suele depender del valor en `t-1`, `t-2`, etc.
   * Esto permite usar modelos que capturan esa dependencia, como **ARIMA, LSTM, Prophet**, etc.

3. ### â³ **Estructura del tiempo**

   * El tiempo tiene **caracterÃ­sticas particulares** que afectan los datos:

     * **Estacionalidad**: patrones que se repiten en intervalos regulares (ej. mÃ¡s helado en verano).
     * **Tendencia**: crecimiento o disminuciÃ³n a lo largo del tiempo.
     * **Ciclos**: fluctuaciones mÃ¡s irregulares que pueden durar aÃ±os (ej. economÃ­a).
     * **Eventos especiales**: feriados, aÃ±os bisiestos, pandemias, etc.
     * **Cambio de estaciones del aÃ±o**: especialmente relevante en fenÃ³menos naturales o de consumo.

4. ### ğŸ“ **Frecuencia**

   * Puede ser **horaria, diaria, semanal, mensual, trimestral, anual**, etc.
   * La elecciÃ³n de la frecuencia afecta el tipo de anÃ¡lisis (por ejemplo, anÃ¡lisis de estacionalidad mensual vs diaria).

---

### ğŸ¯ Aplicaciones tÃ­picas:

* PredicciÃ³n de ventas o demanda
* AnÃ¡lisis financiero (acciones, divisas)
* MeteorologÃ­a
* Series biomÃ©dicas (ritmo cardÃ­aco, glucosa)
* Sensores IoT (temperatura, vibraciones)

---

### ğŸ“˜ **Resumen General: Medidas de Resumen y Distribuciones**

Las medidas de resumen se dividen en dos grandes grupos:

1. **Cuantitativas** (valores numÃ©ricos):

   * **Media**: Promedio aritmÃ©tico, sensible a valores extremos.
   * **Mediana**: Valor central, resistente a outliers.
   * **Moda**: Valor mÃ¡s frecuente.
   * **Varianza**: Mide cuÃ¡n dispersos estÃ¡n los datos respecto a la media.
   * **DesviaciÃ³n estÃ¡ndar**: DispersiÃ³n en las mismas unidades que los datos.
   * **Cuartiles y percentiles**: Dividen el conjunto de datos en segmentos para analizar la distribuciÃ³n.

2. **Cualitativas** (valores categÃ³ricos):

   * **Conteo de observaciones**: Frecuencia de cada categorÃ­a.
   * **Moda**: CategorÃ­a mÃ¡s frecuente.

TambiÃ©n se analizan **distribuciones**:

* **DistribuciÃ³n uniforme**: Todos los valores tienen igual probabilidad.
* **DistribuciÃ³n normal**: Forma de campana, simÃ©trica, con mayor densidad de datos en torno a la media.

Y se interpretan grÃ¡ficamente mediante:

* **Histogramas**: Muestran frecuencias por intervalos.
* **CorrelaciÃ³n lineal**: Mide fuerza y direcciÃ³n de la relaciÃ³n entre dos variables (coeficiente de correlaciÃ³n de âˆ’1 a 1).

---

### ğŸ“Š **Tabla Resumen: Medidas y Distribuciones**

| **Concepto**                | **Tipo de Variable**     | **DescripciÃ³n**                                                   | **Ventajas**                               | **Limitaciones**                                      |
| --------------------------- | ------------------------ | ----------------------------------------------------------------- | ------------------------------------------ | ----------------------------------------------------- |
| **Media**                   | Cuantitativa             | Promedio de todos los valores                                     | FÃ¡cil de calcular e interpretar            | Afectada por valores extremos (outliers)              |
| **Mediana**                 | Cuantitativa             | Valor central de los datos ordenados                              | Robusta frente a outliers                  | No refleja toda la informaciÃ³n del conjunto           |
| **Moda**                    | Cualitativa/Cuantitativa | Valor o categorÃ­a mÃ¡s frecuente                                   | Intuitiva y Ãºtil en cualitativas           | Puede haber mÃ¡s de una o ninguna moda                 |
| **Varianza**                | Cuantitativa             | Promedio de los cuadrados de las desviaciones respecto a la media | EvalÃºa dispersiÃ³n con detalle              | DifÃ­cil interpretaciÃ³n directa (unidades al cuadrado) |
| **DesviaciÃ³n estÃ¡ndar**     | Cuantitativa             | RaÃ­z cuadrada de la varianza                                      | MÃ¡s intuitiva que la varianza              | Sensible a valores extremos                           |
| **Cuartiles / Percentiles** | Cuantitativa             | Dividen los datos ordenados en partes iguales                     | Permiten comparar posiciones relativas     | No indican dispersiÃ³n general completa                |
| **Conteo de observaciones** | Cualitativa              | NÃºmero de veces que aparece cada categorÃ­a                        | Simple y clara                             | No indica relaciÃ³n entre categorÃ­as                   |
| **DistribuciÃ³n uniforme**   | Cualitativa/Cuantitativa | Todos los valores tienen igual probabilidad                       | Ideal para procesos aleatorios controlados | Poco realista en fenÃ³menos naturales                  |
| **DistribuciÃ³n normal**     | Cuantitativa             | Datos concentrados alrededor de la media con forma de campana     | Base de muchos mÃ©todos estadÃ­sticos        | No todos los fenÃ³menos la siguen                      |
| **Histograma**              | Cualitativa/Cuantitativa | Representa frecuencias por intervalos                             | Visualiza forma de la distribuciÃ³n         | Depende del nÃºmero de intervalos                      |
| **CorrelaciÃ³n lineal**      | Cuantitativa             | RelaciÃ³n entre dos variables (de âˆ’1 a 1)                          | Detecta patrones lineales                  | No implica causalidad                                 |

---

### RESPUESTA DE LA FILMINA pag.8 

**Ã­ndice trimestral de inflaciÃ³n se considera una serie de tiempo**?

### Â¿Por quÃ©?

Una **serie de tiempo** es un conjunto de observaciones recogidas **en momentos sucesivos del tiempo**, generalmente a intervalos regulares (diarios, mensuales, trimestrales, etc.). El **Ã­ndice trimestral de inflaciÃ³n** cumple con estas caracterÃ­sticas:

* **Observaciones ordenadas en el tiempo:** cada dato representa la inflaciÃ³n de un trimestre especÃ­fico (por ejemplo, Q1 2022, Q2 2022, etc.).
* **Intervalos regulares:** los datos se recopilan **cada tres meses**, lo que constituye un intervalo de tiempo constante.
* **AnÃ¡lisis temporal posible:** se puede analizar su tendencia, estacionalidad, ciclos econÃ³micos y variabilidad a lo largo del tiempo.

### En pocas palabras:

âœ… **SÃ­, el Ã­ndice trimestral de inflaciÃ³n es una serie de tiempo**, y puede ser analizado mediante mÃ©todos estadÃ­sticos y economÃ©tricos propios del anÃ¡lisis de series temporales.
---


